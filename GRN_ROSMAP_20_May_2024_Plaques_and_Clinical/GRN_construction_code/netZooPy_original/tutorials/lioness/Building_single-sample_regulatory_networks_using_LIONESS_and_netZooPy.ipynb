{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building single-sample regulatory networks using LIONESS and netZooPy\n",
    "### Author: \n",
    "Qi (Alex) Song*.\n",
    "\n",
    "*Channing division of network medicine, Brigham's and Women hospital and Harvard Medical School, Boston, MA. (qi.song@channing.harvard.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "In this tutorial, we will briefly walk through the steps to perform analysis with Lioness algorithm using netZooPy package. Lioness is an algorithm for estimating sample-specific gene regualtory networks in a population.  LIONESS infers individual sample networks by applying linear interpolation to the predictions made by existing aggregate network inference approaches [1]. In this tutorial, we will use Panda as our basic network inference apporach to build sample-specific networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installation of netZooPy.\n",
    "netZooPy comes with full support for Lioness algorithm. netZooPy can be installed through `pip` command. For more details, please refer to the installation guide at netZooPy documentation site [here](https://netzoopy.readthedocs.io/en/latest/install/index.html).    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load required modules\n",
    "We will need `Panda` and `Lioness` python classes from netZooPy package. We will also need `read_csv()` function from `pandas` package for demonstrating the input data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netZooPy.panda import Panda\n",
    "from netZooPy.lioness import Lioness\n",
    "from netZooPy.lioness.analyze_lioness import AnalyzeLioness\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the three data sets to get a sense about what the inputs look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_data = pd.read_csv('~/netZooPy/tests/puma/ToyData/ToyExpressionData.txt',header=None, index_col = 0, sep = \"\\t\")\n",
    "motif_data = pd.read_csv('~/netZooPy/tests/puma/ToyData/ToyMotifData.txt',header=None, sep = \"\\t\")\n",
    "ppi_data = pd.read_csv('~/netZooPy/tests/puma/ToyData/ToyPPIData.txt',header=None, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expression data is a matrix where rows are genes and columns are samples.There are 1000 genes and 50 samples in this expression dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AACSL</th>\n",
       "      <td>0.141431</td>\n",
       "      <td>-4.153056</td>\n",
       "      <td>2.854971</td>\n",
       "      <td>0.413670</td>\n",
       "      <td>1.082093</td>\n",
       "      <td>1.882361</td>\n",
       "      <td>1.450223</td>\n",
       "      <td>2.130209</td>\n",
       "      <td>0.548923</td>\n",
       "      <td>0.583043</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.551870</td>\n",
       "      <td>-6.645621</td>\n",
       "      <td>-3.970460</td>\n",
       "      <td>-2.041915</td>\n",
       "      <td>0.811989</td>\n",
       "      <td>0.979641</td>\n",
       "      <td>0.063161</td>\n",
       "      <td>3.652624</td>\n",
       "      <td>-2.387639</td>\n",
       "      <td>0.929521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAK1</th>\n",
       "      <td>3.528478</td>\n",
       "      <td>-0.949701</td>\n",
       "      <td>1.039986</td>\n",
       "      <td>-1.618816</td>\n",
       "      <td>-1.228012</td>\n",
       "      <td>-0.171763</td>\n",
       "      <td>-2.822020</td>\n",
       "      <td>0.047464</td>\n",
       "      <td>-0.456019</td>\n",
       "      <td>1.134087</td>\n",
       "      <td>...</td>\n",
       "      <td>2.808942</td>\n",
       "      <td>-0.035529</td>\n",
       "      <td>0.473384</td>\n",
       "      <td>-1.971053</td>\n",
       "      <td>1.759803</td>\n",
       "      <td>3.151289</td>\n",
       "      <td>-5.189503</td>\n",
       "      <td>-0.233187</td>\n",
       "      <td>0.349614</td>\n",
       "      <td>0.704183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABCA17P</th>\n",
       "      <td>-2.597842</td>\n",
       "      <td>3.970710</td>\n",
       "      <td>-2.809212</td>\n",
       "      <td>0.474679</td>\n",
       "      <td>-2.714377</td>\n",
       "      <td>-0.474146</td>\n",
       "      <td>-6.738092</td>\n",
       "      <td>-2.811364</td>\n",
       "      <td>-1.017466</td>\n",
       "      <td>-1.646993</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.324379</td>\n",
       "      <td>-0.999586</td>\n",
       "      <td>0.987566</td>\n",
       "      <td>2.591347</td>\n",
       "      <td>-1.445705</td>\n",
       "      <td>-2.788339</td>\n",
       "      <td>2.295727</td>\n",
       "      <td>0.953828</td>\n",
       "      <td>-1.094031</td>\n",
       "      <td>-2.104951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABCB8</th>\n",
       "      <td>0.352052</td>\n",
       "      <td>-1.866545</td>\n",
       "      <td>-0.007765</td>\n",
       "      <td>3.289632</td>\n",
       "      <td>2.675149</td>\n",
       "      <td>3.819294</td>\n",
       "      <td>0.668285</td>\n",
       "      <td>2.608310</td>\n",
       "      <td>3.342104</td>\n",
       "      <td>-2.792534</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.666569</td>\n",
       "      <td>-0.074404</td>\n",
       "      <td>4.630231</td>\n",
       "      <td>0.883074</td>\n",
       "      <td>-1.573444</td>\n",
       "      <td>4.909868</td>\n",
       "      <td>0.866853</td>\n",
       "      <td>2.374492</td>\n",
       "      <td>1.410069</td>\n",
       "      <td>-3.828003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABCC1</th>\n",
       "      <td>-4.638927</td>\n",
       "      <td>2.440799</td>\n",
       "      <td>-1.655580</td>\n",
       "      <td>0.506424</td>\n",
       "      <td>3.289914</td>\n",
       "      <td>2.460479</td>\n",
       "      <td>-1.003678</td>\n",
       "      <td>1.537393</td>\n",
       "      <td>-1.342323</td>\n",
       "      <td>-1.003316</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.375289</td>\n",
       "      <td>-3.214583</td>\n",
       "      <td>5.531917</td>\n",
       "      <td>-1.693335</td>\n",
       "      <td>1.506472</td>\n",
       "      <td>1.020980</td>\n",
       "      <td>4.933972</td>\n",
       "      <td>2.268159</td>\n",
       "      <td>-0.734398</td>\n",
       "      <td>-2.618825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABCF3</th>\n",
       "      <td>3.822458</td>\n",
       "      <td>0.241117</td>\n",
       "      <td>-0.629730</td>\n",
       "      <td>-6.448074</td>\n",
       "      <td>-2.221022</td>\n",
       "      <td>0.559189</td>\n",
       "      <td>-0.817507</td>\n",
       "      <td>0.404023</td>\n",
       "      <td>-2.105254</td>\n",
       "      <td>6.397036</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416917</td>\n",
       "      <td>-1.061398</td>\n",
       "      <td>4.559200</td>\n",
       "      <td>-1.014232</td>\n",
       "      <td>0.372768</td>\n",
       "      <td>1.563113</td>\n",
       "      <td>2.331843</td>\n",
       "      <td>7.556858</td>\n",
       "      <td>1.705383</td>\n",
       "      <td>-1.658708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABCG1</th>\n",
       "      <td>0.562681</td>\n",
       "      <td>0.348409</td>\n",
       "      <td>-1.800319</td>\n",
       "      <td>-3.869393</td>\n",
       "      <td>6.949607</td>\n",
       "      <td>-0.700023</td>\n",
       "      <td>0.613762</td>\n",
       "      <td>-0.194774</td>\n",
       "      <td>0.477825</td>\n",
       "      <td>1.010311</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.687904</td>\n",
       "      <td>-2.232982</td>\n",
       "      <td>-3.412368</td>\n",
       "      <td>-2.943314</td>\n",
       "      <td>-2.985734</td>\n",
       "      <td>1.551215</td>\n",
       "      <td>1.133610</td>\n",
       "      <td>0.454167</td>\n",
       "      <td>-2.928591</td>\n",
       "      <td>2.399636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACOT2</th>\n",
       "      <td>2.411224</td>\n",
       "      <td>-4.461962</td>\n",
       "      <td>2.932685</td>\n",
       "      <td>-3.482673</td>\n",
       "      <td>1.622149</td>\n",
       "      <td>1.970574</td>\n",
       "      <td>3.783328</td>\n",
       "      <td>2.018564</td>\n",
       "      <td>-4.157278</td>\n",
       "      <td>-0.454352</td>\n",
       "      <td>...</td>\n",
       "      <td>3.496161</td>\n",
       "      <td>-4.542483</td>\n",
       "      <td>1.584941</td>\n",
       "      <td>-0.167026</td>\n",
       "      <td>-1.091618</td>\n",
       "      <td>0.266757</td>\n",
       "      <td>0.242896</td>\n",
       "      <td>-1.986338</td>\n",
       "      <td>1.453353</td>\n",
       "      <td>-3.237589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACSF2</th>\n",
       "      <td>0.452929</td>\n",
       "      <td>2.932288</td>\n",
       "      <td>2.580745</td>\n",
       "      <td>0.576073</td>\n",
       "      <td>1.568796</td>\n",
       "      <td>-3.090774</td>\n",
       "      <td>-2.057883</td>\n",
       "      <td>-5.726421</td>\n",
       "      <td>-5.013427</td>\n",
       "      <td>1.913956</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.264008</td>\n",
       "      <td>0.828671</td>\n",
       "      <td>0.139710</td>\n",
       "      <td>-0.038387</td>\n",
       "      <td>1.705630</td>\n",
       "      <td>5.832881</td>\n",
       "      <td>-0.075640</td>\n",
       "      <td>-3.998545</td>\n",
       "      <td>-0.008504</td>\n",
       "      <td>-0.974825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADAM2</th>\n",
       "      <td>0.355641</td>\n",
       "      <td>-0.011333</td>\n",
       "      <td>3.167627</td>\n",
       "      <td>-2.292000</td>\n",
       "      <td>0.309439</td>\n",
       "      <td>1.242176</td>\n",
       "      <td>-0.447120</td>\n",
       "      <td>0.028113</td>\n",
       "      <td>-3.598405</td>\n",
       "      <td>1.442208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.352784</td>\n",
       "      <td>2.091205</td>\n",
       "      <td>-1.578376</td>\n",
       "      <td>5.417486</td>\n",
       "      <td>1.012646</td>\n",
       "      <td>-2.214839</td>\n",
       "      <td>-2.154928</td>\n",
       "      <td>2.582198</td>\n",
       "      <td>-0.176870</td>\n",
       "      <td>1.551844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADAMTS10</th>\n",
       "      <td>-6.435335</td>\n",
       "      <td>-3.484197</td>\n",
       "      <td>-1.334098</td>\n",
       "      <td>-6.257098</td>\n",
       "      <td>1.414799</td>\n",
       "      <td>1.847578</td>\n",
       "      <td>-0.854010</td>\n",
       "      <td>-1.351184</td>\n",
       "      <td>-5.122214</td>\n",
       "      <td>2.551296</td>\n",
       "      <td>...</td>\n",
       "      <td>4.033988</td>\n",
       "      <td>3.940538</td>\n",
       "      <td>3.105636</td>\n",
       "      <td>-0.830551</td>\n",
       "      <td>0.912196</td>\n",
       "      <td>-0.126224</td>\n",
       "      <td>-2.345661</td>\n",
       "      <td>7.605495</td>\n",
       "      <td>1.028245</td>\n",
       "      <td>-3.296487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADAMTS8</th>\n",
       "      <td>-1.500518</td>\n",
       "      <td>-1.781252</td>\n",
       "      <td>0.390679</td>\n",
       "      <td>-4.498557</td>\n",
       "      <td>-2.950740</td>\n",
       "      <td>2.187399</td>\n",
       "      <td>-4.911543</td>\n",
       "      <td>-2.987357</td>\n",
       "      <td>1.188654</td>\n",
       "      <td>-4.559955</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.554750</td>\n",
       "      <td>-1.501601</td>\n",
       "      <td>-4.605737</td>\n",
       "      <td>-2.175497</td>\n",
       "      <td>3.599596</td>\n",
       "      <td>2.921266</td>\n",
       "      <td>3.366518</td>\n",
       "      <td>-3.623441</td>\n",
       "      <td>6.498009</td>\n",
       "      <td>0.601931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADCY6</th>\n",
       "      <td>0.239552</td>\n",
       "      <td>3.663878</td>\n",
       "      <td>-2.478119</td>\n",
       "      <td>-1.249731</td>\n",
       "      <td>-4.508629</td>\n",
       "      <td>-1.559120</td>\n",
       "      <td>-1.018709</td>\n",
       "      <td>0.820547</td>\n",
       "      <td>-0.839912</td>\n",
       "      <td>-2.340473</td>\n",
       "      <td>...</td>\n",
       "      <td>4.726817</td>\n",
       "      <td>-2.105939</td>\n",
       "      <td>2.066263</td>\n",
       "      <td>1.598242</td>\n",
       "      <td>-4.352578</td>\n",
       "      <td>2.937616</td>\n",
       "      <td>0.838953</td>\n",
       "      <td>0.873717</td>\n",
       "      <td>2.164306</td>\n",
       "      <td>-7.228367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADCYAP1R1</th>\n",
       "      <td>0.458076</td>\n",
       "      <td>5.646495</td>\n",
       "      <td>1.054934</td>\n",
       "      <td>2.420345</td>\n",
       "      <td>1.272796</td>\n",
       "      <td>2.603719</td>\n",
       "      <td>1.006353</td>\n",
       "      <td>5.376351</td>\n",
       "      <td>-0.302532</td>\n",
       "      <td>6.419028</td>\n",
       "      <td>...</td>\n",
       "      <td>4.236462</td>\n",
       "      <td>-1.691084</td>\n",
       "      <td>-0.661785</td>\n",
       "      <td>0.803437</td>\n",
       "      <td>-0.274287</td>\n",
       "      <td>-0.872806</td>\n",
       "      <td>1.080127</td>\n",
       "      <td>-1.647711</td>\n",
       "      <td>-7.314351</td>\n",
       "      <td>-0.454197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADH1B</th>\n",
       "      <td>-2.498004</td>\n",
       "      <td>4.363269</td>\n",
       "      <td>3.047640</td>\n",
       "      <td>-2.887588</td>\n",
       "      <td>-0.382138</td>\n",
       "      <td>-2.791821</td>\n",
       "      <td>-3.554774</td>\n",
       "      <td>-4.072156</td>\n",
       "      <td>0.171069</td>\n",
       "      <td>-4.810303</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.070164</td>\n",
       "      <td>-2.778129</td>\n",
       "      <td>-0.110080</td>\n",
       "      <td>-3.957719</td>\n",
       "      <td>-1.410711</td>\n",
       "      <td>0.695792</td>\n",
       "      <td>2.330447</td>\n",
       "      <td>-3.053955</td>\n",
       "      <td>-4.306005</td>\n",
       "      <td>1.839237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADH6</th>\n",
       "      <td>-0.224751</td>\n",
       "      <td>-4.080510</td>\n",
       "      <td>3.996458</td>\n",
       "      <td>2.259859</td>\n",
       "      <td>-0.291146</td>\n",
       "      <td>0.586628</td>\n",
       "      <td>3.458583</td>\n",
       "      <td>3.730949</td>\n",
       "      <td>5.144894</td>\n",
       "      <td>-0.546617</td>\n",
       "      <td>...</td>\n",
       "      <td>3.345719</td>\n",
       "      <td>1.916228</td>\n",
       "      <td>4.300311</td>\n",
       "      <td>-4.651193</td>\n",
       "      <td>0.775120</td>\n",
       "      <td>-3.972906</td>\n",
       "      <td>-0.698086</td>\n",
       "      <td>-0.430629</td>\n",
       "      <td>-0.816764</td>\n",
       "      <td>0.763499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFF1</th>\n",
       "      <td>1.267301</td>\n",
       "      <td>-1.743443</td>\n",
       "      <td>-0.558082</td>\n",
       "      <td>0.711610</td>\n",
       "      <td>-1.599487</td>\n",
       "      <td>-4.036190</td>\n",
       "      <td>1.768361</td>\n",
       "      <td>3.132970</td>\n",
       "      <td>0.851968</td>\n",
       "      <td>-0.930023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079065</td>\n",
       "      <td>-7.419378</td>\n",
       "      <td>3.747206</td>\n",
       "      <td>-2.912467</td>\n",
       "      <td>-3.456261</td>\n",
       "      <td>2.962799</td>\n",
       "      <td>-3.951777</td>\n",
       "      <td>-4.874321</td>\n",
       "      <td>-0.339520</td>\n",
       "      <td>3.577241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGR3</th>\n",
       "      <td>2.689198</td>\n",
       "      <td>0.551704</td>\n",
       "      <td>4.595840</td>\n",
       "      <td>5.758756</td>\n",
       "      <td>2.143647</td>\n",
       "      <td>6.210038</td>\n",
       "      <td>3.802123</td>\n",
       "      <td>4.112670</td>\n",
       "      <td>3.055105</td>\n",
       "      <td>-2.853347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.480619</td>\n",
       "      <td>-1.733525</td>\n",
       "      <td>-0.008265</td>\n",
       "      <td>4.462258</td>\n",
       "      <td>-8.505583</td>\n",
       "      <td>-0.109861</td>\n",
       "      <td>-1.752762</td>\n",
       "      <td>-0.309343</td>\n",
       "      <td>-0.280030</td>\n",
       "      <td>-3.342774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGRN</th>\n",
       "      <td>-3.803666</td>\n",
       "      <td>3.013388</td>\n",
       "      <td>0.094045</td>\n",
       "      <td>-3.100119</td>\n",
       "      <td>3.327980</td>\n",
       "      <td>-3.495524</td>\n",
       "      <td>3.172164</td>\n",
       "      <td>0.686780</td>\n",
       "      <td>1.848163</td>\n",
       "      <td>-4.764568</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059656</td>\n",
       "      <td>1.874475</td>\n",
       "      <td>2.730239</td>\n",
       "      <td>2.368229</td>\n",
       "      <td>0.895757</td>\n",
       "      <td>1.090120</td>\n",
       "      <td>3.247198</td>\n",
       "      <td>3.194017</td>\n",
       "      <td>1.101338</td>\n",
       "      <td>2.587900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AHR</th>\n",
       "      <td>-0.206353</td>\n",
       "      <td>5.031601</td>\n",
       "      <td>-6.772378</td>\n",
       "      <td>0.658805</td>\n",
       "      <td>-4.815799</td>\n",
       "      <td>2.131688</td>\n",
       "      <td>-0.616780</td>\n",
       "      <td>2.924277</td>\n",
       "      <td>-0.459491</td>\n",
       "      <td>1.096779</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.079071</td>\n",
       "      <td>7.787621</td>\n",
       "      <td>4.173739</td>\n",
       "      <td>-4.853682</td>\n",
       "      <td>1.384796</td>\n",
       "      <td>-2.567371</td>\n",
       "      <td>-4.304014</td>\n",
       "      <td>-2.440764</td>\n",
       "      <td>2.933068</td>\n",
       "      <td>3.258371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AIM1</th>\n",
       "      <td>-3.253885</td>\n",
       "      <td>-1.344785</td>\n",
       "      <td>0.778770</td>\n",
       "      <td>-3.178421</td>\n",
       "      <td>-2.241247</td>\n",
       "      <td>0.076789</td>\n",
       "      <td>0.705845</td>\n",
       "      <td>-3.178349</td>\n",
       "      <td>-3.290212</td>\n",
       "      <td>-4.055884</td>\n",
       "      <td>...</td>\n",
       "      <td>2.506882</td>\n",
       "      <td>0.238419</td>\n",
       "      <td>0.795984</td>\n",
       "      <td>0.555804</td>\n",
       "      <td>3.693480</td>\n",
       "      <td>3.648031</td>\n",
       "      <td>-2.599992</td>\n",
       "      <td>-1.235266</td>\n",
       "      <td>-1.932245</td>\n",
       "      <td>1.393250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AJAP1</th>\n",
       "      <td>-0.671359</td>\n",
       "      <td>-1.453216</td>\n",
       "      <td>1.583014</td>\n",
       "      <td>2.223739</td>\n",
       "      <td>6.980131</td>\n",
       "      <td>-3.211525</td>\n",
       "      <td>-2.033678</td>\n",
       "      <td>-3.638395</td>\n",
       "      <td>1.529547</td>\n",
       "      <td>0.024004</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.760967</td>\n",
       "      <td>0.402947</td>\n",
       "      <td>2.047986</td>\n",
       "      <td>9.711038</td>\n",
       "      <td>-5.583309</td>\n",
       "      <td>-1.635951</td>\n",
       "      <td>-2.851895</td>\n",
       "      <td>-4.855163</td>\n",
       "      <td>-1.584887</td>\n",
       "      <td>1.328536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AKAP10</th>\n",
       "      <td>2.111122</td>\n",
       "      <td>1.642002</td>\n",
       "      <td>1.268021</td>\n",
       "      <td>0.915710</td>\n",
       "      <td>-2.686503</td>\n",
       "      <td>0.648780</td>\n",
       "      <td>3.363863</td>\n",
       "      <td>-1.063846</td>\n",
       "      <td>-1.164144</td>\n",
       "      <td>2.371818</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.532954</td>\n",
       "      <td>0.104894</td>\n",
       "      <td>2.236829</td>\n",
       "      <td>1.582274</td>\n",
       "      <td>1.728103</td>\n",
       "      <td>2.456019</td>\n",
       "      <td>-2.895845</td>\n",
       "      <td>-3.776638</td>\n",
       "      <td>-2.602940</td>\n",
       "      <td>1.132555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AKAP8L</th>\n",
       "      <td>-2.076503</td>\n",
       "      <td>5.121783</td>\n",
       "      <td>0.120849</td>\n",
       "      <td>4.835900</td>\n",
       "      <td>4.528920</td>\n",
       "      <td>-4.010499</td>\n",
       "      <td>-0.570129</td>\n",
       "      <td>-1.910726</td>\n",
       "      <td>-5.247164</td>\n",
       "      <td>-3.205733</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.448418</td>\n",
       "      <td>-0.832548</td>\n",
       "      <td>2.501409</td>\n",
       "      <td>4.901713</td>\n",
       "      <td>1.762507</td>\n",
       "      <td>-3.798529</td>\n",
       "      <td>-2.656816</td>\n",
       "      <td>1.570357</td>\n",
       "      <td>1.879632</td>\n",
       "      <td>4.546102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AKIRIN2</th>\n",
       "      <td>0.182562</td>\n",
       "      <td>-2.814136</td>\n",
       "      <td>-1.704658</td>\n",
       "      <td>-2.635369</td>\n",
       "      <td>-2.622177</td>\n",
       "      <td>-2.169776</td>\n",
       "      <td>3.083701</td>\n",
       "      <td>-4.855234</td>\n",
       "      <td>-1.960743</td>\n",
       "      <td>-1.231475</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.941039</td>\n",
       "      <td>-4.338502</td>\n",
       "      <td>-6.590048</td>\n",
       "      <td>-6.723866</td>\n",
       "      <td>2.562907</td>\n",
       "      <td>-4.462658</td>\n",
       "      <td>-3.158130</td>\n",
       "      <td>-1.700410</td>\n",
       "      <td>-3.032855</td>\n",
       "      <td>-1.274919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AKT1</th>\n",
       "      <td>-0.600257</td>\n",
       "      <td>-1.156250</td>\n",
       "      <td>4.269575</td>\n",
       "      <td>-2.652594</td>\n",
       "      <td>1.637202</td>\n",
       "      <td>6.342937</td>\n",
       "      <td>2.674969</td>\n",
       "      <td>-0.104879</td>\n",
       "      <td>-0.478754</td>\n",
       "      <td>-0.124468</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.399697</td>\n",
       "      <td>-2.622513</td>\n",
       "      <td>1.185376</td>\n",
       "      <td>4.711334</td>\n",
       "      <td>-1.332219</td>\n",
       "      <td>-2.756439</td>\n",
       "      <td>3.066091</td>\n",
       "      <td>3.494977</td>\n",
       "      <td>-2.954487</td>\n",
       "      <td>-1.335418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALG14</th>\n",
       "      <td>1.457071</td>\n",
       "      <td>-1.123955</td>\n",
       "      <td>1.168836</td>\n",
       "      <td>-4.140685</td>\n",
       "      <td>-0.767143</td>\n",
       "      <td>2.490318</td>\n",
       "      <td>3.910973</td>\n",
       "      <td>1.030674</td>\n",
       "      <td>3.085856</td>\n",
       "      <td>-1.179364</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.414906</td>\n",
       "      <td>-1.151661</td>\n",
       "      <td>3.431068</td>\n",
       "      <td>2.462015</td>\n",
       "      <td>-0.172023</td>\n",
       "      <td>4.241700</td>\n",
       "      <td>2.498942</td>\n",
       "      <td>1.538316</td>\n",
       "      <td>4.271883</td>\n",
       "      <td>9.790623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALPPL2</th>\n",
       "      <td>0.278612</td>\n",
       "      <td>-3.347032</td>\n",
       "      <td>-1.047794</td>\n",
       "      <td>-3.167413</td>\n",
       "      <td>-0.413878</td>\n",
       "      <td>-1.187347</td>\n",
       "      <td>-2.064943</td>\n",
       "      <td>1.779923</td>\n",
       "      <td>-4.103784</td>\n",
       "      <td>-3.772628</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.636912</td>\n",
       "      <td>-0.258916</td>\n",
       "      <td>-4.925722</td>\n",
       "      <td>0.192451</td>\n",
       "      <td>3.438164</td>\n",
       "      <td>0.687503</td>\n",
       "      <td>0.757951</td>\n",
       "      <td>-1.844243</td>\n",
       "      <td>-4.572469</td>\n",
       "      <td>0.661904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMDHD1</th>\n",
       "      <td>1.442675</td>\n",
       "      <td>2.435977</td>\n",
       "      <td>-2.655512</td>\n",
       "      <td>-0.262620</td>\n",
       "      <td>-0.133281</td>\n",
       "      <td>0.071852</td>\n",
       "      <td>-0.937521</td>\n",
       "      <td>3.380939</td>\n",
       "      <td>1.799345</td>\n",
       "      <td>2.822316</td>\n",
       "      <td>...</td>\n",
       "      <td>2.361554</td>\n",
       "      <td>-3.860945</td>\n",
       "      <td>2.688930</td>\n",
       "      <td>-1.500954</td>\n",
       "      <td>-2.484701</td>\n",
       "      <td>-4.613864</td>\n",
       "      <td>2.601704</td>\n",
       "      <td>1.524390</td>\n",
       "      <td>-0.309889</td>\n",
       "      <td>6.049197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMIGO1</th>\n",
       "      <td>-0.632069</td>\n",
       "      <td>-4.026140</td>\n",
       "      <td>-0.023985</td>\n",
       "      <td>1.703159</td>\n",
       "      <td>2.216783</td>\n",
       "      <td>-2.408620</td>\n",
       "      <td>2.931212</td>\n",
       "      <td>-3.947217</td>\n",
       "      <td>0.862303</td>\n",
       "      <td>-2.412431</td>\n",
       "      <td>...</td>\n",
       "      <td>1.330087</td>\n",
       "      <td>-0.709538</td>\n",
       "      <td>4.998298</td>\n",
       "      <td>5.434680</td>\n",
       "      <td>0.447682</td>\n",
       "      <td>0.431151</td>\n",
       "      <td>0.342613</td>\n",
       "      <td>0.362589</td>\n",
       "      <td>0.124003</td>\n",
       "      <td>-4.286540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YY1</th>\n",
       "      <td>1.955807</td>\n",
       "      <td>0.441289</td>\n",
       "      <td>-4.025722</td>\n",
       "      <td>3.241794</td>\n",
       "      <td>-1.471559</td>\n",
       "      <td>2.546710</td>\n",
       "      <td>-3.392705</td>\n",
       "      <td>-0.801326</td>\n",
       "      <td>8.747804</td>\n",
       "      <td>-0.891846</td>\n",
       "      <td>...</td>\n",
       "      <td>3.968216</td>\n",
       "      <td>-4.094314</td>\n",
       "      <td>-0.874615</td>\n",
       "      <td>1.884760</td>\n",
       "      <td>0.013866</td>\n",
       "      <td>0.810367</td>\n",
       "      <td>-1.970884</td>\n",
       "      <td>-2.359943</td>\n",
       "      <td>4.105038</td>\n",
       "      <td>-1.566941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZBTB25</th>\n",
       "      <td>-0.465745</td>\n",
       "      <td>2.731525</td>\n",
       "      <td>0.583832</td>\n",
       "      <td>0.952136</td>\n",
       "      <td>3.981221</td>\n",
       "      <td>-6.242749</td>\n",
       "      <td>0.274338</td>\n",
       "      <td>-1.281054</td>\n",
       "      <td>3.280473</td>\n",
       "      <td>5.293598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.832881</td>\n",
       "      <td>-0.647692</td>\n",
       "      <td>0.002772</td>\n",
       "      <td>-1.540829</td>\n",
       "      <td>-0.289737</td>\n",
       "      <td>-4.011846</td>\n",
       "      <td>-1.242774</td>\n",
       "      <td>1.763224</td>\n",
       "      <td>-4.739805</td>\n",
       "      <td>4.830278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZCCHC6</th>\n",
       "      <td>2.616518</td>\n",
       "      <td>3.725200</td>\n",
       "      <td>-3.604447</td>\n",
       "      <td>0.953095</td>\n",
       "      <td>-1.414919</td>\n",
       "      <td>-1.686620</td>\n",
       "      <td>1.115142</td>\n",
       "      <td>-1.938877</td>\n",
       "      <td>-0.005253</td>\n",
       "      <td>-5.008178</td>\n",
       "      <td>...</td>\n",
       "      <td>2.148678</td>\n",
       "      <td>-3.049896</td>\n",
       "      <td>-7.382941</td>\n",
       "      <td>-0.647739</td>\n",
       "      <td>-1.921551</td>\n",
       "      <td>-1.810419</td>\n",
       "      <td>-4.233128</td>\n",
       "      <td>3.262738</td>\n",
       "      <td>-0.812537</td>\n",
       "      <td>3.522548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZDHHC15</th>\n",
       "      <td>0.287416</td>\n",
       "      <td>4.228220</td>\n",
       "      <td>-1.932585</td>\n",
       "      <td>1.280172</td>\n",
       "      <td>-0.361667</td>\n",
       "      <td>-3.245687</td>\n",
       "      <td>-4.729219</td>\n",
       "      <td>-3.941767</td>\n",
       "      <td>3.047753</td>\n",
       "      <td>0.983202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>0.802575</td>\n",
       "      <td>0.580516</td>\n",
       "      <td>-2.626980</td>\n",
       "      <td>1.650896</td>\n",
       "      <td>-0.486386</td>\n",
       "      <td>-1.465460</td>\n",
       "      <td>2.756558</td>\n",
       "      <td>1.240380</td>\n",
       "      <td>1.369940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZDHHC16</th>\n",
       "      <td>3.983208</td>\n",
       "      <td>-4.736054</td>\n",
       "      <td>1.431894</td>\n",
       "      <td>-0.534885</td>\n",
       "      <td>1.189214</td>\n",
       "      <td>2.050506</td>\n",
       "      <td>1.918995</td>\n",
       "      <td>-1.047002</td>\n",
       "      <td>4.168625</td>\n",
       "      <td>-1.529144</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.787969</td>\n",
       "      <td>-0.655874</td>\n",
       "      <td>3.368040</td>\n",
       "      <td>0.137512</td>\n",
       "      <td>-2.539163</td>\n",
       "      <td>3.851830</td>\n",
       "      <td>-0.946804</td>\n",
       "      <td>1.647447</td>\n",
       "      <td>1.379092</td>\n",
       "      <td>-5.007336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZDHHC2</th>\n",
       "      <td>-5.452097</td>\n",
       "      <td>-2.864681</td>\n",
       "      <td>6.239840</td>\n",
       "      <td>4.199863</td>\n",
       "      <td>-1.725535</td>\n",
       "      <td>-3.715479</td>\n",
       "      <td>-3.816561</td>\n",
       "      <td>-2.090490</td>\n",
       "      <td>2.274487</td>\n",
       "      <td>-3.430965</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157227</td>\n",
       "      <td>-2.579084</td>\n",
       "      <td>0.958787</td>\n",
       "      <td>3.746606</td>\n",
       "      <td>-0.782082</td>\n",
       "      <td>0.901215</td>\n",
       "      <td>-4.032160</td>\n",
       "      <td>3.597801</td>\n",
       "      <td>-0.142247</td>\n",
       "      <td>-2.502663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZDHHC7</th>\n",
       "      <td>-1.438812</td>\n",
       "      <td>0.525433</td>\n",
       "      <td>1.646436</td>\n",
       "      <td>1.928655</td>\n",
       "      <td>3.225473</td>\n",
       "      <td>-2.751004</td>\n",
       "      <td>-2.989698</td>\n",
       "      <td>-5.601409</td>\n",
       "      <td>2.200852</td>\n",
       "      <td>1.691665</td>\n",
       "      <td>...</td>\n",
       "      <td>3.800466</td>\n",
       "      <td>5.007216</td>\n",
       "      <td>-6.883397</td>\n",
       "      <td>0.784845</td>\n",
       "      <td>0.651235</td>\n",
       "      <td>-0.521085</td>\n",
       "      <td>-4.866340</td>\n",
       "      <td>-0.103401</td>\n",
       "      <td>0.323577</td>\n",
       "      <td>0.927459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZDHHC9</th>\n",
       "      <td>-5.000444</td>\n",
       "      <td>-0.419128</td>\n",
       "      <td>-1.826055</td>\n",
       "      <td>-7.374587</td>\n",
       "      <td>0.431975</td>\n",
       "      <td>0.447008</td>\n",
       "      <td>0.498743</td>\n",
       "      <td>-1.178437</td>\n",
       "      <td>0.252175</td>\n",
       "      <td>-0.106776</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178797</td>\n",
       "      <td>-0.721575</td>\n",
       "      <td>10.173612</td>\n",
       "      <td>-0.566138</td>\n",
       "      <td>1.952077</td>\n",
       "      <td>0.953530</td>\n",
       "      <td>-4.176484</td>\n",
       "      <td>-2.543994</td>\n",
       "      <td>-4.591273</td>\n",
       "      <td>1.720633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZFAT</th>\n",
       "      <td>-2.931567</td>\n",
       "      <td>-1.559858</td>\n",
       "      <td>-1.483557</td>\n",
       "      <td>-1.574698</td>\n",
       "      <td>-0.874071</td>\n",
       "      <td>-2.881306</td>\n",
       "      <td>4.222129</td>\n",
       "      <td>-1.200172</td>\n",
       "      <td>-0.990453</td>\n",
       "      <td>5.082496</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.772075</td>\n",
       "      <td>3.798225</td>\n",
       "      <td>0.403502</td>\n",
       "      <td>-3.078181</td>\n",
       "      <td>4.188723</td>\n",
       "      <td>5.179436</td>\n",
       "      <td>2.422908</td>\n",
       "      <td>2.133740</td>\n",
       "      <td>-0.258086</td>\n",
       "      <td>-5.022865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZFP62</th>\n",
       "      <td>-1.271041</td>\n",
       "      <td>-2.920962</td>\n",
       "      <td>-3.853412</td>\n",
       "      <td>0.748130</td>\n",
       "      <td>4.703092</td>\n",
       "      <td>3.214546</td>\n",
       "      <td>0.476954</td>\n",
       "      <td>0.005453</td>\n",
       "      <td>0.526652</td>\n",
       "      <td>1.999028</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.526324</td>\n",
       "      <td>-0.229234</td>\n",
       "      <td>0.321802</td>\n",
       "      <td>1.108952</td>\n",
       "      <td>3.446266</td>\n",
       "      <td>-5.234739</td>\n",
       "      <td>1.145793</td>\n",
       "      <td>1.785877</td>\n",
       "      <td>6.950120</td>\n",
       "      <td>1.517959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZFR</th>\n",
       "      <td>-4.744726</td>\n",
       "      <td>4.316600</td>\n",
       "      <td>-4.468643</td>\n",
       "      <td>5.237806</td>\n",
       "      <td>1.316022</td>\n",
       "      <td>-0.874962</td>\n",
       "      <td>5.295222</td>\n",
       "      <td>-3.100937</td>\n",
       "      <td>-0.505455</td>\n",
       "      <td>-4.697889</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.011458</td>\n",
       "      <td>-2.350506</td>\n",
       "      <td>-6.253545</td>\n",
       "      <td>0.666908</td>\n",
       "      <td>1.212308</td>\n",
       "      <td>-0.688220</td>\n",
       "      <td>0.660413</td>\n",
       "      <td>-2.524871</td>\n",
       "      <td>0.319327</td>\n",
       "      <td>2.381417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZMYM3</th>\n",
       "      <td>1.471251</td>\n",
       "      <td>0.652375</td>\n",
       "      <td>-1.932988</td>\n",
       "      <td>2.670401</td>\n",
       "      <td>-2.737013</td>\n",
       "      <td>0.057174</td>\n",
       "      <td>2.756968</td>\n",
       "      <td>1.152510</td>\n",
       "      <td>-2.647043</td>\n",
       "      <td>7.612432</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.551895</td>\n",
       "      <td>1.795703</td>\n",
       "      <td>1.051413</td>\n",
       "      <td>-1.358606</td>\n",
       "      <td>1.383776</td>\n",
       "      <td>-2.503557</td>\n",
       "      <td>3.385820</td>\n",
       "      <td>-0.745977</td>\n",
       "      <td>-0.627234</td>\n",
       "      <td>-1.546457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZMYND17</th>\n",
       "      <td>4.619858</td>\n",
       "      <td>-3.475264</td>\n",
       "      <td>0.038139</td>\n",
       "      <td>4.313893</td>\n",
       "      <td>-1.061013</td>\n",
       "      <td>-1.304709</td>\n",
       "      <td>0.256868</td>\n",
       "      <td>0.419320</td>\n",
       "      <td>3.540630</td>\n",
       "      <td>3.040002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.813865</td>\n",
       "      <td>5.155772</td>\n",
       "      <td>-5.260261</td>\n",
       "      <td>1.344561</td>\n",
       "      <td>-2.431417</td>\n",
       "      <td>4.107977</td>\n",
       "      <td>0.237943</td>\n",
       "      <td>-1.939147</td>\n",
       "      <td>-0.133882</td>\n",
       "      <td>3.983232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF124</th>\n",
       "      <td>3.330737</td>\n",
       "      <td>5.589767</td>\n",
       "      <td>-4.290391</td>\n",
       "      <td>-0.717922</td>\n",
       "      <td>0.811929</td>\n",
       "      <td>4.553090</td>\n",
       "      <td>-2.496052</td>\n",
       "      <td>1.730051</td>\n",
       "      <td>0.636785</td>\n",
       "      <td>0.818016</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.356317</td>\n",
       "      <td>-1.932498</td>\n",
       "      <td>-3.455120</td>\n",
       "      <td>-0.084919</td>\n",
       "      <td>-1.237713</td>\n",
       "      <td>3.506137</td>\n",
       "      <td>-1.885701</td>\n",
       "      <td>0.273054</td>\n",
       "      <td>-2.894267</td>\n",
       "      <td>-0.983415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF132</th>\n",
       "      <td>-2.168550</td>\n",
       "      <td>-2.412464</td>\n",
       "      <td>-3.149500</td>\n",
       "      <td>-2.440988</td>\n",
       "      <td>10.078211</td>\n",
       "      <td>1.902683</td>\n",
       "      <td>3.813724</td>\n",
       "      <td>-3.439067</td>\n",
       "      <td>1.787848</td>\n",
       "      <td>2.228660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.986353</td>\n",
       "      <td>-1.858777</td>\n",
       "      <td>8.075192</td>\n",
       "      <td>7.790025</td>\n",
       "      <td>-2.206606</td>\n",
       "      <td>6.022623</td>\n",
       "      <td>-0.591418</td>\n",
       "      <td>-5.578319</td>\n",
       "      <td>-6.060207</td>\n",
       "      <td>1.476551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF138</th>\n",
       "      <td>5.501906</td>\n",
       "      <td>-0.524335</td>\n",
       "      <td>-4.959888</td>\n",
       "      <td>1.751520</td>\n",
       "      <td>-1.605983</td>\n",
       "      <td>-1.376737</td>\n",
       "      <td>4.639871</td>\n",
       "      <td>2.583844</td>\n",
       "      <td>1.688727</td>\n",
       "      <td>-0.609936</td>\n",
       "      <td>...</td>\n",
       "      <td>2.701609</td>\n",
       "      <td>-1.919130</td>\n",
       "      <td>-1.581302</td>\n",
       "      <td>-2.705314</td>\n",
       "      <td>-1.775207</td>\n",
       "      <td>7.263195</td>\n",
       "      <td>-5.359276</td>\n",
       "      <td>-1.377007</td>\n",
       "      <td>-2.831965</td>\n",
       "      <td>2.458768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF283</th>\n",
       "      <td>-3.034221</td>\n",
       "      <td>2.111506</td>\n",
       "      <td>-0.719169</td>\n",
       "      <td>0.225898</td>\n",
       "      <td>-2.721097</td>\n",
       "      <td>1.546914</td>\n",
       "      <td>-3.126284</td>\n",
       "      <td>0.706536</td>\n",
       "      <td>2.476561</td>\n",
       "      <td>-1.737517</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.950084</td>\n",
       "      <td>0.811290</td>\n",
       "      <td>7.654198</td>\n",
       "      <td>-0.602563</td>\n",
       "      <td>-0.485668</td>\n",
       "      <td>0.706387</td>\n",
       "      <td>0.637638</td>\n",
       "      <td>1.044513</td>\n",
       "      <td>1.126691</td>\n",
       "      <td>1.881236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF322B</th>\n",
       "      <td>-5.860660</td>\n",
       "      <td>6.129064</td>\n",
       "      <td>1.159615</td>\n",
       "      <td>-2.537294</td>\n",
       "      <td>-3.155169</td>\n",
       "      <td>-1.897060</td>\n",
       "      <td>1.876461</td>\n",
       "      <td>-6.812772</td>\n",
       "      <td>-0.795365</td>\n",
       "      <td>7.940787</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.441415</td>\n",
       "      <td>3.249069</td>\n",
       "      <td>-4.036189</td>\n",
       "      <td>0.334181</td>\n",
       "      <td>-2.639774</td>\n",
       "      <td>7.711171</td>\n",
       "      <td>5.733905</td>\n",
       "      <td>0.518429</td>\n",
       "      <td>-2.275928</td>\n",
       "      <td>-1.721815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF426</th>\n",
       "      <td>6.177740</td>\n",
       "      <td>-2.424839</td>\n",
       "      <td>-0.226201</td>\n",
       "      <td>2.234599</td>\n",
       "      <td>-1.458643</td>\n",
       "      <td>-3.694419</td>\n",
       "      <td>5.435978</td>\n",
       "      <td>2.113951</td>\n",
       "      <td>-0.548585</td>\n",
       "      <td>-0.587499</td>\n",
       "      <td>...</td>\n",
       "      <td>1.810145</td>\n",
       "      <td>-0.509916</td>\n",
       "      <td>-1.958090</td>\n",
       "      <td>-2.925023</td>\n",
       "      <td>-1.296322</td>\n",
       "      <td>3.874413</td>\n",
       "      <td>5.068583</td>\n",
       "      <td>-7.030104</td>\n",
       "      <td>-1.901056</td>\n",
       "      <td>-1.727810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF451</th>\n",
       "      <td>3.249119</td>\n",
       "      <td>1.893897</td>\n",
       "      <td>1.717736</td>\n",
       "      <td>-0.659733</td>\n",
       "      <td>1.719676</td>\n",
       "      <td>3.294666</td>\n",
       "      <td>-0.481807</td>\n",
       "      <td>0.529062</td>\n",
       "      <td>-6.321060</td>\n",
       "      <td>-1.217092</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.084075</td>\n",
       "      <td>2.081276</td>\n",
       "      <td>-2.132384</td>\n",
       "      <td>0.780504</td>\n",
       "      <td>-2.191865</td>\n",
       "      <td>-3.959255</td>\n",
       "      <td>-5.588040</td>\n",
       "      <td>-1.734270</td>\n",
       "      <td>6.236782</td>\n",
       "      <td>-4.415510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF480</th>\n",
       "      <td>-0.140409</td>\n",
       "      <td>-1.341108</td>\n",
       "      <td>0.018698</td>\n",
       "      <td>-1.008055</td>\n",
       "      <td>0.655969</td>\n",
       "      <td>-0.532847</td>\n",
       "      <td>1.634830</td>\n",
       "      <td>1.264774</td>\n",
       "      <td>1.879042</td>\n",
       "      <td>1.682834</td>\n",
       "      <td>...</td>\n",
       "      <td>2.166815</td>\n",
       "      <td>-0.138187</td>\n",
       "      <td>0.269705</td>\n",
       "      <td>2.945537</td>\n",
       "      <td>-6.374132</td>\n",
       "      <td>-4.319404</td>\n",
       "      <td>-1.417171</td>\n",
       "      <td>-3.086070</td>\n",
       "      <td>-4.755754</td>\n",
       "      <td>2.109595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF660</th>\n",
       "      <td>-5.134059</td>\n",
       "      <td>-2.887868</td>\n",
       "      <td>-5.682801</td>\n",
       "      <td>-4.584404</td>\n",
       "      <td>0.923337</td>\n",
       "      <td>-3.747658</td>\n",
       "      <td>2.117484</td>\n",
       "      <td>-5.564895</td>\n",
       "      <td>-2.821189</td>\n",
       "      <td>-1.254334</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.868436</td>\n",
       "      <td>-0.401334</td>\n",
       "      <td>1.951397</td>\n",
       "      <td>4.396688</td>\n",
       "      <td>4.859973</td>\n",
       "      <td>4.140445</td>\n",
       "      <td>2.204386</td>\n",
       "      <td>1.565369</td>\n",
       "      <td>1.391193</td>\n",
       "      <td>0.320414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF696</th>\n",
       "      <td>1.536629</td>\n",
       "      <td>-1.218671</td>\n",
       "      <td>-1.244199</td>\n",
       "      <td>-4.751498</td>\n",
       "      <td>-2.160287</td>\n",
       "      <td>1.626458</td>\n",
       "      <td>-0.817378</td>\n",
       "      <td>0.777225</td>\n",
       "      <td>-0.594485</td>\n",
       "      <td>0.026376</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.468312</td>\n",
       "      <td>-0.279803</td>\n",
       "      <td>5.001230</td>\n",
       "      <td>2.464151</td>\n",
       "      <td>-0.827188</td>\n",
       "      <td>-2.598237</td>\n",
       "      <td>-1.420780</td>\n",
       "      <td>2.221469</td>\n",
       "      <td>0.488647</td>\n",
       "      <td>-7.675989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF772</th>\n",
       "      <td>1.805567</td>\n",
       "      <td>4.059267</td>\n",
       "      <td>-7.479062</td>\n",
       "      <td>-0.471547</td>\n",
       "      <td>-0.378794</td>\n",
       "      <td>-5.751023</td>\n",
       "      <td>6.168965</td>\n",
       "      <td>2.467858</td>\n",
       "      <td>-0.962984</td>\n",
       "      <td>-2.699509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.734462</td>\n",
       "      <td>-4.688808</td>\n",
       "      <td>-1.912480</td>\n",
       "      <td>-3.396070</td>\n",
       "      <td>1.147704</td>\n",
       "      <td>-3.591208</td>\n",
       "      <td>-3.216311</td>\n",
       "      <td>2.329710</td>\n",
       "      <td>2.226982</td>\n",
       "      <td>-0.744886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF776</th>\n",
       "      <td>-0.739313</td>\n",
       "      <td>4.252302</td>\n",
       "      <td>-1.231494</td>\n",
       "      <td>1.584117</td>\n",
       "      <td>5.788495</td>\n",
       "      <td>0.016007</td>\n",
       "      <td>2.675882</td>\n",
       "      <td>3.143431</td>\n",
       "      <td>-1.671526</td>\n",
       "      <td>-1.230410</td>\n",
       "      <td>...</td>\n",
       "      <td>1.841290</td>\n",
       "      <td>3.199943</td>\n",
       "      <td>5.509921</td>\n",
       "      <td>-0.675689</td>\n",
       "      <td>-0.410618</td>\n",
       "      <td>2.722958</td>\n",
       "      <td>-2.857927</td>\n",
       "      <td>0.232340</td>\n",
       "      <td>0.472510</td>\n",
       "      <td>-0.760252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF826</th>\n",
       "      <td>-4.294209</td>\n",
       "      <td>-4.498573</td>\n",
       "      <td>2.786462</td>\n",
       "      <td>-1.588052</td>\n",
       "      <td>1.542248</td>\n",
       "      <td>3.222761</td>\n",
       "      <td>-0.005525</td>\n",
       "      <td>-4.711981</td>\n",
       "      <td>-0.580551</td>\n",
       "      <td>2.152243</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.023105</td>\n",
       "      <td>-1.675647</td>\n",
       "      <td>3.320340</td>\n",
       "      <td>2.108714</td>\n",
       "      <td>-5.961589</td>\n",
       "      <td>1.679723</td>\n",
       "      <td>-1.473783</td>\n",
       "      <td>1.871397</td>\n",
       "      <td>1.968646</td>\n",
       "      <td>-1.017821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF845</th>\n",
       "      <td>-1.661144</td>\n",
       "      <td>-6.986089</td>\n",
       "      <td>2.273928</td>\n",
       "      <td>-2.426933</td>\n",
       "      <td>-4.627002</td>\n",
       "      <td>-4.044476</td>\n",
       "      <td>-3.991184</td>\n",
       "      <td>-0.903110</td>\n",
       "      <td>1.558416</td>\n",
       "      <td>0.672473</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.990304</td>\n",
       "      <td>-1.806129</td>\n",
       "      <td>1.643056</td>\n",
       "      <td>1.932765</td>\n",
       "      <td>1.084221</td>\n",
       "      <td>-1.214410</td>\n",
       "      <td>-2.985126</td>\n",
       "      <td>-10.986240</td>\n",
       "      <td>0.159237</td>\n",
       "      <td>0.906706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF878</th>\n",
       "      <td>3.395504</td>\n",
       "      <td>-6.274497</td>\n",
       "      <td>0.455548</td>\n",
       "      <td>0.592239</td>\n",
       "      <td>-0.852212</td>\n",
       "      <td>1.373684</td>\n",
       "      <td>-2.638325</td>\n",
       "      <td>4.028651</td>\n",
       "      <td>-1.117790</td>\n",
       "      <td>-2.844150</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.858309</td>\n",
       "      <td>1.752461</td>\n",
       "      <td>1.630060</td>\n",
       "      <td>-0.079563</td>\n",
       "      <td>-0.545380</td>\n",
       "      <td>-2.694063</td>\n",
       "      <td>-0.535988</td>\n",
       "      <td>-0.038242</td>\n",
       "      <td>-1.353040</td>\n",
       "      <td>-1.713555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZSWIM3</th>\n",
       "      <td>-0.494841</td>\n",
       "      <td>2.840674</td>\n",
       "      <td>-3.816640</td>\n",
       "      <td>3.052187</td>\n",
       "      <td>4.979421</td>\n",
       "      <td>-5.889279</td>\n",
       "      <td>-6.640408</td>\n",
       "      <td>3.711903</td>\n",
       "      <td>-3.156978</td>\n",
       "      <td>0.475878</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.065756</td>\n",
       "      <td>1.069487</td>\n",
       "      <td>-1.682956</td>\n",
       "      <td>4.824188</td>\n",
       "      <td>-0.930484</td>\n",
       "      <td>-3.155203</td>\n",
       "      <td>-4.252213</td>\n",
       "      <td>-6.578125</td>\n",
       "      <td>-6.558922</td>\n",
       "      <td>-0.104826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZWILCH</th>\n",
       "      <td>0.694298</td>\n",
       "      <td>-2.725693</td>\n",
       "      <td>-1.752258</td>\n",
       "      <td>-1.789789</td>\n",
       "      <td>2.228141</td>\n",
       "      <td>-4.494592</td>\n",
       "      <td>1.233303</td>\n",
       "      <td>-1.748305</td>\n",
       "      <td>2.534993</td>\n",
       "      <td>0.819987</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.033845</td>\n",
       "      <td>2.497995</td>\n",
       "      <td>-3.744730</td>\n",
       "      <td>-2.536536</td>\n",
       "      <td>-3.004383</td>\n",
       "      <td>3.336575</td>\n",
       "      <td>-1.095170</td>\n",
       "      <td>-3.466885</td>\n",
       "      <td>1.519252</td>\n",
       "      <td>-0.729152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 1         2         3         4          5         6   \\\n",
       "0                                                                        \n",
       "AACSL      0.141431 -4.153056  2.854971  0.413670   1.082093  1.882361   \n",
       "AAK1       3.528478 -0.949701  1.039986 -1.618816  -1.228012 -0.171763   \n",
       "ABCA17P   -2.597842  3.970710 -2.809212  0.474679  -2.714377 -0.474146   \n",
       "ABCB8      0.352052 -1.866545 -0.007765  3.289632   2.675149  3.819294   \n",
       "ABCC1     -4.638927  2.440799 -1.655580  0.506424   3.289914  2.460479   \n",
       "ABCF3      3.822458  0.241117 -0.629730 -6.448074  -2.221022  0.559189   \n",
       "ABCG1      0.562681  0.348409 -1.800319 -3.869393   6.949607 -0.700023   \n",
       "ACOT2      2.411224 -4.461962  2.932685 -3.482673   1.622149  1.970574   \n",
       "ACSF2      0.452929  2.932288  2.580745  0.576073   1.568796 -3.090774   \n",
       "ADAM2      0.355641 -0.011333  3.167627 -2.292000   0.309439  1.242176   \n",
       "ADAMTS10  -6.435335 -3.484197 -1.334098 -6.257098   1.414799  1.847578   \n",
       "ADAMTS8   -1.500518 -1.781252  0.390679 -4.498557  -2.950740  2.187399   \n",
       "ADCY6      0.239552  3.663878 -2.478119 -1.249731  -4.508629 -1.559120   \n",
       "ADCYAP1R1  0.458076  5.646495  1.054934  2.420345   1.272796  2.603719   \n",
       "ADH1B     -2.498004  4.363269  3.047640 -2.887588  -0.382138 -2.791821   \n",
       "ADH6      -0.224751 -4.080510  3.996458  2.259859  -0.291146  0.586628   \n",
       "AFF1       1.267301 -1.743443 -0.558082  0.711610  -1.599487 -4.036190   \n",
       "AGR3       2.689198  0.551704  4.595840  5.758756   2.143647  6.210038   \n",
       "AGRN      -3.803666  3.013388  0.094045 -3.100119   3.327980 -3.495524   \n",
       "AHR       -0.206353  5.031601 -6.772378  0.658805  -4.815799  2.131688   \n",
       "AIM1      -3.253885 -1.344785  0.778770 -3.178421  -2.241247  0.076789   \n",
       "AJAP1     -0.671359 -1.453216  1.583014  2.223739   6.980131 -3.211525   \n",
       "AKAP10     2.111122  1.642002  1.268021  0.915710  -2.686503  0.648780   \n",
       "AKAP8L    -2.076503  5.121783  0.120849  4.835900   4.528920 -4.010499   \n",
       "AKIRIN2    0.182562 -2.814136 -1.704658 -2.635369  -2.622177 -2.169776   \n",
       "AKT1      -0.600257 -1.156250  4.269575 -2.652594   1.637202  6.342937   \n",
       "ALG14      1.457071 -1.123955  1.168836 -4.140685  -0.767143  2.490318   \n",
       "ALPPL2     0.278612 -3.347032 -1.047794 -3.167413  -0.413878 -1.187347   \n",
       "AMDHD1     1.442675  2.435977 -2.655512 -0.262620  -0.133281  0.071852   \n",
       "AMIGO1    -0.632069 -4.026140 -0.023985  1.703159   2.216783 -2.408620   \n",
       "...             ...       ...       ...       ...        ...       ...   \n",
       "YY1        1.955807  0.441289 -4.025722  3.241794  -1.471559  2.546710   \n",
       "ZBTB25    -0.465745  2.731525  0.583832  0.952136   3.981221 -6.242749   \n",
       "ZCCHC6     2.616518  3.725200 -3.604447  0.953095  -1.414919 -1.686620   \n",
       "ZDHHC15    0.287416  4.228220 -1.932585  1.280172  -0.361667 -3.245687   \n",
       "ZDHHC16    3.983208 -4.736054  1.431894 -0.534885   1.189214  2.050506   \n",
       "ZDHHC2    -5.452097 -2.864681  6.239840  4.199863  -1.725535 -3.715479   \n",
       "ZDHHC7    -1.438812  0.525433  1.646436  1.928655   3.225473 -2.751004   \n",
       "ZDHHC9    -5.000444 -0.419128 -1.826055 -7.374587   0.431975  0.447008   \n",
       "ZFAT      -2.931567 -1.559858 -1.483557 -1.574698  -0.874071 -2.881306   \n",
       "ZFP62     -1.271041 -2.920962 -3.853412  0.748130   4.703092  3.214546   \n",
       "ZFR       -4.744726  4.316600 -4.468643  5.237806   1.316022 -0.874962   \n",
       "ZMYM3      1.471251  0.652375 -1.932988  2.670401  -2.737013  0.057174   \n",
       "ZMYND17    4.619858 -3.475264  0.038139  4.313893  -1.061013 -1.304709   \n",
       "ZNF124     3.330737  5.589767 -4.290391 -0.717922   0.811929  4.553090   \n",
       "ZNF132    -2.168550 -2.412464 -3.149500 -2.440988  10.078211  1.902683   \n",
       "ZNF138     5.501906 -0.524335 -4.959888  1.751520  -1.605983 -1.376737   \n",
       "ZNF283    -3.034221  2.111506 -0.719169  0.225898  -2.721097  1.546914   \n",
       "ZNF322B   -5.860660  6.129064  1.159615 -2.537294  -3.155169 -1.897060   \n",
       "ZNF426     6.177740 -2.424839 -0.226201  2.234599  -1.458643 -3.694419   \n",
       "ZNF451     3.249119  1.893897  1.717736 -0.659733   1.719676  3.294666   \n",
       "ZNF480    -0.140409 -1.341108  0.018698 -1.008055   0.655969 -0.532847   \n",
       "ZNF660    -5.134059 -2.887868 -5.682801 -4.584404   0.923337 -3.747658   \n",
       "ZNF696     1.536629 -1.218671 -1.244199 -4.751498  -2.160287  1.626458   \n",
       "ZNF772     1.805567  4.059267 -7.479062 -0.471547  -0.378794 -5.751023   \n",
       "ZNF776    -0.739313  4.252302 -1.231494  1.584117   5.788495  0.016007   \n",
       "ZNF826    -4.294209 -4.498573  2.786462 -1.588052   1.542248  3.222761   \n",
       "ZNF845    -1.661144 -6.986089  2.273928 -2.426933  -4.627002 -4.044476   \n",
       "ZNF878     3.395504 -6.274497  0.455548  0.592239  -0.852212  1.373684   \n",
       "ZSWIM3    -0.494841  2.840674 -3.816640  3.052187   4.979421 -5.889279   \n",
       "ZWILCH     0.694298 -2.725693 -1.752258 -1.789789   2.228141 -4.494592   \n",
       "\n",
       "                 7         8         9         10  ...        41        42  \\\n",
       "0                                                  ...                       \n",
       "AACSL      1.450223  2.130209  0.548923  0.583043  ... -4.551870 -6.645621   \n",
       "AAK1      -2.822020  0.047464 -0.456019  1.134087  ...  2.808942 -0.035529   \n",
       "ABCA17P   -6.738092 -2.811364 -1.017466 -1.646993  ... -2.324379 -0.999586   \n",
       "ABCB8      0.668285  2.608310  3.342104 -2.792534  ... -3.666569 -0.074404   \n",
       "ABCC1     -1.003678  1.537393 -1.342323 -1.003316  ... -0.375289 -3.214583   \n",
       "ABCF3     -0.817507  0.404023 -2.105254  6.397036  ...  0.416917 -1.061398   \n",
       "ABCG1      0.613762 -0.194774  0.477825  1.010311  ... -6.687904 -2.232982   \n",
       "ACOT2      3.783328  2.018564 -4.157278 -0.454352  ...  3.496161 -4.542483   \n",
       "ACSF2     -2.057883 -5.726421 -5.013427  1.913956  ... -3.264008  0.828671   \n",
       "ADAM2     -0.447120  0.028113 -3.598405  1.442208  ...  0.352784  2.091205   \n",
       "ADAMTS10  -0.854010 -1.351184 -5.122214  2.551296  ...  4.033988  3.940538   \n",
       "ADAMTS8   -4.911543 -2.987357  1.188654 -4.559955  ... -1.554750 -1.501601   \n",
       "ADCY6     -1.018709  0.820547 -0.839912 -2.340473  ...  4.726817 -2.105939   \n",
       "ADCYAP1R1  1.006353  5.376351 -0.302532  6.419028  ...  4.236462 -1.691084   \n",
       "ADH1B     -3.554774 -4.072156  0.171069 -4.810303  ... -3.070164 -2.778129   \n",
       "ADH6       3.458583  3.730949  5.144894 -0.546617  ...  3.345719  1.916228   \n",
       "AFF1       1.768361  3.132970  0.851968 -0.930023  ...  0.079065 -7.419378   \n",
       "AGR3       3.802123  4.112670  3.055105 -2.853347  ...  0.480619 -1.733525   \n",
       "AGRN       3.172164  0.686780  1.848163 -4.764568  ... -0.059656  1.874475   \n",
       "AHR       -0.616780  2.924277 -0.459491  1.096779  ... -2.079071  7.787621   \n",
       "AIM1       0.705845 -3.178349 -3.290212 -4.055884  ...  2.506882  0.238419   \n",
       "AJAP1     -2.033678 -3.638395  1.529547  0.024004  ... -1.760967  0.402947   \n",
       "AKAP10     3.363863 -1.063846 -1.164144  2.371818  ... -1.532954  0.104894   \n",
       "AKAP8L    -0.570129 -1.910726 -5.247164 -3.205733  ... -0.448418 -0.832548   \n",
       "AKIRIN2    3.083701 -4.855234 -1.960743 -1.231475  ... -1.941039 -4.338502   \n",
       "AKT1       2.674969 -0.104879 -0.478754 -0.124468  ... -0.399697 -2.622513   \n",
       "ALG14      3.910973  1.030674  3.085856 -1.179364  ... -1.414906 -1.151661   \n",
       "ALPPL2    -2.064943  1.779923 -4.103784 -3.772628  ... -0.636912 -0.258916   \n",
       "AMDHD1    -0.937521  3.380939  1.799345  2.822316  ...  2.361554 -3.860945   \n",
       "AMIGO1     2.931212 -3.947217  0.862303 -2.412431  ...  1.330087 -0.709538   \n",
       "...             ...       ...       ...       ...  ...       ...       ...   \n",
       "YY1       -3.392705 -0.801326  8.747804 -0.891846  ...  3.968216 -4.094314   \n",
       "ZBTB25     0.274338 -1.281054  3.280473  5.293598  ...  0.832881 -0.647692   \n",
       "ZCCHC6     1.115142 -1.938877 -0.005253 -5.008178  ...  2.148678 -3.049896   \n",
       "ZDHHC15   -4.729219 -3.941767  3.047753  0.983202  ...  0.924528  0.802575   \n",
       "ZDHHC16    1.918995 -1.047002  4.168625 -1.529144  ... -1.787969 -0.655874   \n",
       "ZDHHC2    -3.816561 -2.090490  2.274487 -3.430965  ...  0.157227 -2.579084   \n",
       "ZDHHC7    -2.989698 -5.601409  2.200852  1.691665  ...  3.800466  5.007216   \n",
       "ZDHHC9     0.498743 -1.178437  0.252175 -0.106776  ... -0.178797 -0.721575   \n",
       "ZFAT       4.222129 -1.200172 -0.990453  5.082496  ... -1.772075  3.798225   \n",
       "ZFP62      0.476954  0.005453  0.526652  1.999028  ... -0.526324 -0.229234   \n",
       "ZFR        5.295222 -3.100937 -0.505455 -4.697889  ... -1.011458 -2.350506   \n",
       "ZMYM3      2.756968  1.152510 -2.647043  7.612432  ... -6.551895  1.795703   \n",
       "ZMYND17    0.256868  0.419320  3.540630  3.040002  ...  0.813865  5.155772   \n",
       "ZNF124    -2.496052  1.730051  0.636785  0.818016  ... -1.356317 -1.932498   \n",
       "ZNF132     3.813724 -3.439067  1.787848  2.228660  ...  0.986353 -1.858777   \n",
       "ZNF138     4.639871  2.583844  1.688727 -0.609936  ...  2.701609 -1.919130   \n",
       "ZNF283    -3.126284  0.706536  2.476561 -1.737517  ... -0.950084  0.811290   \n",
       "ZNF322B    1.876461 -6.812772 -0.795365  7.940787  ... -1.441415  3.249069   \n",
       "ZNF426     5.435978  2.113951 -0.548585 -0.587499  ...  1.810145 -0.509916   \n",
       "ZNF451    -0.481807  0.529062 -6.321060 -1.217092  ... -1.084075  2.081276   \n",
       "ZNF480     1.634830  1.264774  1.879042  1.682834  ...  2.166815 -0.138187   \n",
       "ZNF660     2.117484 -5.564895 -2.821189 -1.254334  ... -1.868436 -0.401334   \n",
       "ZNF696    -0.817378  0.777225 -0.594485  0.026376  ... -1.468312 -0.279803   \n",
       "ZNF772     6.168965  2.467858 -0.962984 -2.699509  ...  0.734462 -4.688808   \n",
       "ZNF776     2.675882  3.143431 -1.671526 -1.230410  ...  1.841290  3.199943   \n",
       "ZNF826    -0.005525 -4.711981 -0.580551  2.152243  ... -1.023105 -1.675647   \n",
       "ZNF845    -3.991184 -0.903110  1.558416  0.672473  ... -0.990304 -1.806129   \n",
       "ZNF878    -2.638325  4.028651 -1.117790 -2.844150  ... -4.858309  1.752461   \n",
       "ZSWIM3    -6.640408  3.711903 -3.156978  0.475878  ... -2.065756  1.069487   \n",
       "ZWILCH     1.233303 -1.748305  2.534993  0.819987  ... -4.033845  2.497995   \n",
       "\n",
       "                  43        44        45        46        47         48  \\\n",
       "0                                                                         \n",
       "AACSL      -3.970460 -2.041915  0.811989  0.979641  0.063161   3.652624   \n",
       "AAK1        0.473384 -1.971053  1.759803  3.151289 -5.189503  -0.233187   \n",
       "ABCA17P     0.987566  2.591347 -1.445705 -2.788339  2.295727   0.953828   \n",
       "ABCB8       4.630231  0.883074 -1.573444  4.909868  0.866853   2.374492   \n",
       "ABCC1       5.531917 -1.693335  1.506472  1.020980  4.933972   2.268159   \n",
       "ABCF3       4.559200 -1.014232  0.372768  1.563113  2.331843   7.556858   \n",
       "ABCG1      -3.412368 -2.943314 -2.985734  1.551215  1.133610   0.454167   \n",
       "ACOT2       1.584941 -0.167026 -1.091618  0.266757  0.242896  -1.986338   \n",
       "ACSF2       0.139710 -0.038387  1.705630  5.832881 -0.075640  -3.998545   \n",
       "ADAM2      -1.578376  5.417486  1.012646 -2.214839 -2.154928   2.582198   \n",
       "ADAMTS10    3.105636 -0.830551  0.912196 -0.126224 -2.345661   7.605495   \n",
       "ADAMTS8    -4.605737 -2.175497  3.599596  2.921266  3.366518  -3.623441   \n",
       "ADCY6       2.066263  1.598242 -4.352578  2.937616  0.838953   0.873717   \n",
       "ADCYAP1R1  -0.661785  0.803437 -0.274287 -0.872806  1.080127  -1.647711   \n",
       "ADH1B      -0.110080 -3.957719 -1.410711  0.695792  2.330447  -3.053955   \n",
       "ADH6        4.300311 -4.651193  0.775120 -3.972906 -0.698086  -0.430629   \n",
       "AFF1        3.747206 -2.912467 -3.456261  2.962799 -3.951777  -4.874321   \n",
       "AGR3       -0.008265  4.462258 -8.505583 -0.109861 -1.752762  -0.309343   \n",
       "AGRN        2.730239  2.368229  0.895757  1.090120  3.247198   3.194017   \n",
       "AHR         4.173739 -4.853682  1.384796 -2.567371 -4.304014  -2.440764   \n",
       "AIM1        0.795984  0.555804  3.693480  3.648031 -2.599992  -1.235266   \n",
       "AJAP1       2.047986  9.711038 -5.583309 -1.635951 -2.851895  -4.855163   \n",
       "AKAP10      2.236829  1.582274  1.728103  2.456019 -2.895845  -3.776638   \n",
       "AKAP8L      2.501409  4.901713  1.762507 -3.798529 -2.656816   1.570357   \n",
       "AKIRIN2    -6.590048 -6.723866  2.562907 -4.462658 -3.158130  -1.700410   \n",
       "AKT1        1.185376  4.711334 -1.332219 -2.756439  3.066091   3.494977   \n",
       "ALG14       3.431068  2.462015 -0.172023  4.241700  2.498942   1.538316   \n",
       "ALPPL2     -4.925722  0.192451  3.438164  0.687503  0.757951  -1.844243   \n",
       "AMDHD1      2.688930 -1.500954 -2.484701 -4.613864  2.601704   1.524390   \n",
       "AMIGO1      4.998298  5.434680  0.447682  0.431151  0.342613   0.362589   \n",
       "...              ...       ...       ...       ...       ...        ...   \n",
       "YY1        -0.874615  1.884760  0.013866  0.810367 -1.970884  -2.359943   \n",
       "ZBTB25      0.002772 -1.540829 -0.289737 -4.011846 -1.242774   1.763224   \n",
       "ZCCHC6     -7.382941 -0.647739 -1.921551 -1.810419 -4.233128   3.262738   \n",
       "ZDHHC15     0.580516 -2.626980  1.650896 -0.486386 -1.465460   2.756558   \n",
       "ZDHHC16     3.368040  0.137512 -2.539163  3.851830 -0.946804   1.647447   \n",
       "ZDHHC2      0.958787  3.746606 -0.782082  0.901215 -4.032160   3.597801   \n",
       "ZDHHC7     -6.883397  0.784845  0.651235 -0.521085 -4.866340  -0.103401   \n",
       "ZDHHC9     10.173612 -0.566138  1.952077  0.953530 -4.176484  -2.543994   \n",
       "ZFAT        0.403502 -3.078181  4.188723  5.179436  2.422908   2.133740   \n",
       "ZFP62       0.321802  1.108952  3.446266 -5.234739  1.145793   1.785877   \n",
       "ZFR        -6.253545  0.666908  1.212308 -0.688220  0.660413  -2.524871   \n",
       "ZMYM3       1.051413 -1.358606  1.383776 -2.503557  3.385820  -0.745977   \n",
       "ZMYND17    -5.260261  1.344561 -2.431417  4.107977  0.237943  -1.939147   \n",
       "ZNF124     -3.455120 -0.084919 -1.237713  3.506137 -1.885701   0.273054   \n",
       "ZNF132      8.075192  7.790025 -2.206606  6.022623 -0.591418  -5.578319   \n",
       "ZNF138     -1.581302 -2.705314 -1.775207  7.263195 -5.359276  -1.377007   \n",
       "ZNF283      7.654198 -0.602563 -0.485668  0.706387  0.637638   1.044513   \n",
       "ZNF322B    -4.036189  0.334181 -2.639774  7.711171  5.733905   0.518429   \n",
       "ZNF426     -1.958090 -2.925023 -1.296322  3.874413  5.068583  -7.030104   \n",
       "ZNF451     -2.132384  0.780504 -2.191865 -3.959255 -5.588040  -1.734270   \n",
       "ZNF480      0.269705  2.945537 -6.374132 -4.319404 -1.417171  -3.086070   \n",
       "ZNF660      1.951397  4.396688  4.859973  4.140445  2.204386   1.565369   \n",
       "ZNF696      5.001230  2.464151 -0.827188 -2.598237 -1.420780   2.221469   \n",
       "ZNF772     -1.912480 -3.396070  1.147704 -3.591208 -3.216311   2.329710   \n",
       "ZNF776      5.509921 -0.675689 -0.410618  2.722958 -2.857927   0.232340   \n",
       "ZNF826      3.320340  2.108714 -5.961589  1.679723 -1.473783   1.871397   \n",
       "ZNF845      1.643056  1.932765  1.084221 -1.214410 -2.985126 -10.986240   \n",
       "ZNF878      1.630060 -0.079563 -0.545380 -2.694063 -0.535988  -0.038242   \n",
       "ZSWIM3     -1.682956  4.824188 -0.930484 -3.155203 -4.252213  -6.578125   \n",
       "ZWILCH     -3.744730 -2.536536 -3.004383  3.336575 -1.095170  -3.466885   \n",
       "\n",
       "                 49        50  \n",
       "0                              \n",
       "AACSL     -2.387639  0.929521  \n",
       "AAK1       0.349614  0.704183  \n",
       "ABCA17P   -1.094031 -2.104951  \n",
       "ABCB8      1.410069 -3.828003  \n",
       "ABCC1     -0.734398 -2.618825  \n",
       "ABCF3      1.705383 -1.658708  \n",
       "ABCG1     -2.928591  2.399636  \n",
       "ACOT2      1.453353 -3.237589  \n",
       "ACSF2     -0.008504 -0.974825  \n",
       "ADAM2     -0.176870  1.551844  \n",
       "ADAMTS10   1.028245 -3.296487  \n",
       "ADAMTS8    6.498009  0.601931  \n",
       "ADCY6      2.164306 -7.228367  \n",
       "ADCYAP1R1 -7.314351 -0.454197  \n",
       "ADH1B     -4.306005  1.839237  \n",
       "ADH6      -0.816764  0.763499  \n",
       "AFF1      -0.339520  3.577241  \n",
       "AGR3      -0.280030 -3.342774  \n",
       "AGRN       1.101338  2.587900  \n",
       "AHR        2.933068  3.258371  \n",
       "AIM1      -1.932245  1.393250  \n",
       "AJAP1     -1.584887  1.328536  \n",
       "AKAP10    -2.602940  1.132555  \n",
       "AKAP8L     1.879632  4.546102  \n",
       "AKIRIN2   -3.032855 -1.274919  \n",
       "AKT1      -2.954487 -1.335418  \n",
       "ALG14      4.271883  9.790623  \n",
       "ALPPL2    -4.572469  0.661904  \n",
       "AMDHD1    -0.309889  6.049197  \n",
       "AMIGO1     0.124003 -4.286540  \n",
       "...             ...       ...  \n",
       "YY1        4.105038 -1.566941  \n",
       "ZBTB25    -4.739805  4.830278  \n",
       "ZCCHC6    -0.812537  3.522548  \n",
       "ZDHHC15    1.240380  1.369940  \n",
       "ZDHHC16    1.379092 -5.007336  \n",
       "ZDHHC2    -0.142247 -2.502663  \n",
       "ZDHHC7     0.323577  0.927459  \n",
       "ZDHHC9    -4.591273  1.720633  \n",
       "ZFAT      -0.258086 -5.022865  \n",
       "ZFP62      6.950120  1.517959  \n",
       "ZFR        0.319327  2.381417  \n",
       "ZMYM3     -0.627234 -1.546457  \n",
       "ZMYND17   -0.133882  3.983232  \n",
       "ZNF124    -2.894267 -0.983415  \n",
       "ZNF132    -6.060207  1.476551  \n",
       "ZNF138    -2.831965  2.458768  \n",
       "ZNF283     1.126691  1.881236  \n",
       "ZNF322B   -2.275928 -1.721815  \n",
       "ZNF426    -1.901056 -1.727810  \n",
       "ZNF451     6.236782 -4.415510  \n",
       "ZNF480    -4.755754  2.109595  \n",
       "ZNF660     1.391193  0.320414  \n",
       "ZNF696     0.488647 -7.675989  \n",
       "ZNF772     2.226982 -0.744886  \n",
       "ZNF776     0.472510 -0.760252  \n",
       "ZNF826     1.968646 -1.017821  \n",
       "ZNF845     0.159237  0.906706  \n",
       "ZNF878    -1.353040 -1.713555  \n",
       "ZSWIM3    -6.558922 -0.104826  \n",
       "ZWILCH     1.519252 -0.729152  \n",
       "\n",
       "[1000 rows x 50 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motif data should be formatted into a three-column list, where first column contains TF IDs and second column the target gene IDs and third column the interaction scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AHR</td>\n",
       "      <td>41157</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AHR</td>\n",
       "      <td>AAK1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ABCA17P</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ABCB8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ABCC1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ABCF3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ABCG1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ADAM2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ADAMTS10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ADAMTS8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ADCY6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ADCYAP1R1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AHR</td>\n",
       "      <td>AFF1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AHR</td>\n",
       "      <td>AGRN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AHR</td>\n",
       "      <td>AJAP1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AHR</td>\n",
       "      <td>AKAP10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AHR</td>\n",
       "      <td>AKIRIN2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>AHR</td>\n",
       "      <td>AKT1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ALG14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AHR</td>\n",
       "      <td>AMDHD1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>AHR</td>\n",
       "      <td>AMIGO1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>AHR</td>\n",
       "      <td>AMPH</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ANKRD16</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ANKRD36</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ANKRD39</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ANP32E</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ARHGEF2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ASAP3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ASPSCR1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ATL1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14567</th>\n",
       "      <td>YY1</td>\n",
       "      <td>WBSCR26</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14568</th>\n",
       "      <td>YY1</td>\n",
       "      <td>WDR4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14569</th>\n",
       "      <td>YY1</td>\n",
       "      <td>WDR54</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14570</th>\n",
       "      <td>YY1</td>\n",
       "      <td>WDR61</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14571</th>\n",
       "      <td>YY1</td>\n",
       "      <td>WFDC10B</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14572</th>\n",
       "      <td>YY1</td>\n",
       "      <td>WIPF3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14573</th>\n",
       "      <td>YY1</td>\n",
       "      <td>YBX2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14574</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZBTB25</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14575</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZDHHC15</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14576</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZDHHC16</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14577</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZDHHC7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14578</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZDHHC9</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14579</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZFAT</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14580</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZFP62</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14581</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZFR</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14582</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZMYM3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14583</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZMYND17</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14584</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZNF124</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14585</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZNF132</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14586</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZNF138</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14587</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZNF283</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14588</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZNF322B</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14589</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZNF426</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14590</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZNF660</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14591</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZNF772</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14592</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZNF776</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14593</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZNF826</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14594</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZNF878</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14595</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZSWIM3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14596</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14597 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0          1    2\n",
       "0      AHR      41157  1.0\n",
       "1      AHR       AAK1  1.0\n",
       "2      AHR    ABCA17P  1.0\n",
       "3      AHR      ABCB8  1.0\n",
       "4      AHR      ABCC1  1.0\n",
       "5      AHR      ABCF3  1.0\n",
       "6      AHR      ABCG1  1.0\n",
       "7      AHR      ADAM2  1.0\n",
       "8      AHR   ADAMTS10  1.0\n",
       "9      AHR    ADAMTS8  1.0\n",
       "10     AHR      ADCY6  1.0\n",
       "11     AHR  ADCYAP1R1  1.0\n",
       "12     AHR       AFF1  1.0\n",
       "13     AHR       AGRN  1.0\n",
       "14     AHR      AJAP1  1.0\n",
       "15     AHR     AKAP10  1.0\n",
       "16     AHR    AKIRIN2  1.0\n",
       "17     AHR       AKT1  1.0\n",
       "18     AHR      ALG14  1.0\n",
       "19     AHR     AMDHD1  1.0\n",
       "20     AHR     AMIGO1  1.0\n",
       "21     AHR       AMPH  1.0\n",
       "22     AHR    ANKRD16  1.0\n",
       "23     AHR    ANKRD36  1.0\n",
       "24     AHR    ANKRD39  1.0\n",
       "25     AHR     ANP32E  1.0\n",
       "26     AHR    ARHGEF2  1.0\n",
       "27     AHR      ASAP3  1.0\n",
       "28     AHR    ASPSCR1  1.0\n",
       "29     AHR       ATL1  1.0\n",
       "...    ...        ...  ...\n",
       "14567  YY1    WBSCR26  1.0\n",
       "14568  YY1       WDR4  1.0\n",
       "14569  YY1      WDR54  1.0\n",
       "14570  YY1      WDR61  1.0\n",
       "14571  YY1    WFDC10B  1.0\n",
       "14572  YY1      WIPF3  1.0\n",
       "14573  YY1       YBX2  1.0\n",
       "14574  YY1     ZBTB25  1.0\n",
       "14575  YY1    ZDHHC15  1.0\n",
       "14576  YY1    ZDHHC16  1.0\n",
       "14577  YY1     ZDHHC7  1.0\n",
       "14578  YY1     ZDHHC9  1.0\n",
       "14579  YY1       ZFAT  1.0\n",
       "14580  YY1      ZFP62  1.0\n",
       "14581  YY1        ZFR  1.0\n",
       "14582  YY1      ZMYM3  1.0\n",
       "14583  YY1    ZMYND17  1.0\n",
       "14584  YY1     ZNF124  1.0\n",
       "14585  YY1     ZNF132  1.0\n",
       "14586  YY1     ZNF138  1.0\n",
       "14587  YY1     ZNF283  1.0\n",
       "14588  YY1    ZNF322B  1.0\n",
       "14589  YY1     ZNF426  1.0\n",
       "14590  YY1     ZNF660  1.0\n",
       "14591  YY1     ZNF772  1.0\n",
       "14592  YY1     ZNF776  1.0\n",
       "14593  YY1     ZNF826  1.0\n",
       "14594  YY1     ZNF878  1.0\n",
       "14595  YY1     ZSWIM3  1.0\n",
       "14596  YY1     ZWILCH  1.0\n",
       "\n",
       "[14597 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motif_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 87 unique TFs and 913 unique motifs in this motif dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motif_data[0].unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "913"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motif_data[1].unique().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPI (protein protein interaction) data should be formatted into a three-column list, where first two columns contain protein IDs and third column contains a score for each interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([ppi_data[0],ppi_data[1]]).unique().size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This PPI dataset has 238 interactions among 87 TFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Panda\n",
    "Before running Lioness, we will first need to generate a `Panda` object. This will be used later to run `Lioness`. Note that the argument `keep_expression_matrix` should be specified as `True`. As Lioness iteractions need to call Panda function to build networks, which needs expression matrix as input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading motif data ...\n",
      "  Elapsed time: 0.02 sec.\n",
      "Loading expression data ...\n",
      "  Elapsed time: 0.02 sec.\n",
      "Loading PPI data ...\n",
      "Number of PPIs: 238\n",
      "  Elapsed time: 0.00 sec.\n",
      "Calculating coexpression network ...\n",
      "  Elapsed time: 0.01 sec.\n",
      "Creating motif network ...\n",
      "  Elapsed time: 0.01 sec.\n",
      "Creating PPI network ...\n",
      "  Elapsed time: 0.00 sec.\n",
      "Normalizing networks ...\n",
      "  Elapsed time: 0.04 sec.\n",
      "Saving expression matrix and normalized networks ...\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running PANDA algorithm ...\n",
      "step: 0, hamming: 0.7189662815459754\n",
      "step: 1, hamming: 0.3899291546314954\n",
      "step: 2, hamming: 0.4023668388969203\n",
      "step: 3, hamming: 0.40052096181128466\n",
      "step: 4, hamming: 0.38904060163854676\n",
      "step: 5, hamming: 0.37050927774796627\n",
      "step: 6, hamming: 0.346813714233211\n",
      "step: 7, hamming: 0.3197200219092709\n",
      "step: 8, hamming: 0.2908059296381211\n",
      "step: 9, hamming: 0.2614076747991081\n",
      "step: 10, hamming: 0.23256674933108332\n",
      "step: 11, hamming: 0.2050473463652485\n",
      "step: 12, hamming: 0.17936756642941443\n",
      "step: 13, hamming: 0.1558282020394879\n",
      "step: 14, hamming: 0.1345640692729987\n",
      "step: 15, hamming: 0.1155876981777767\n",
      "step: 16, hamming: 0.09882404071423918\n",
      "step: 17, hamming: 0.08414234823461533\n",
      "step: 18, hamming: 0.07137863350560043\n",
      "step: 19, hamming: 0.06035259742114878\n",
      "step: 20, hamming: 0.050879894600761214\n",
      "step: 21, hamming: 0.04278075541305479\n",
      "step: 22, hamming: 0.03588517917018383\n",
      "step: 23, hamming: 0.030036230563844166\n",
      "step: 24, hamming: 0.025091497107547298\n",
      "step: 25, hamming: 0.020923570455323982\n",
      "step: 26, hamming: 0.01741975365490937\n",
      "step: 27, hamming: 0.014481280791973363\n",
      "step: 28, hamming: 0.012022271977958703\n",
      "step: 29, hamming: 0.009968530625360779\n",
      "step: 30, hamming: 0.008256316822080972\n",
      "step: 31, hamming: 0.0068311523959430605\n",
      "step: 32, hamming: 0.005646666396278007\n",
      "step: 33, hamming: 0.0046635404280628776\n",
      "step: 34, hamming: 0.0038485478068236794\n",
      "step: 35, hamming: 0.0031736932436243594\n",
      "step: 36, hamming: 0.0026154528841061666\n",
      "step: 37, hamming: 0.0021541086160812906\n",
      "step: 38, hamming: 0.0017731679664810445\n",
      "step: 39, hamming: 0.0014588635383778888\n",
      "step: 40, hamming: 0.0011997237010732422\n",
      "step: 41, hamming: 0.0009862051158053523\n",
      "Running panda took: 1.02 seconds!\n"
     ]
    }
   ],
   "source": [
    "panda_obj = Panda('~/netZooPy/tests/puma/ToyData/ToyExpressionData.txt',\n",
    "                  '~/netZooPy/tests/puma/ToyData/ToyMotifData.txt',\n",
    "                  '~/netZooPy/tests/puma/ToyData/ToyPPIData.txt',\n",
    "                  remove_missing=False, \n",
    "                  keep_expression_matrix=True, save_memory=False, modeProcess='legacy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Lioness to estimate sample-specific networks\n",
    "We will first use the `Panda` object as input for `Lioness` object. Then `Lioness` will run Panda algorithm in its iterations to estimate sample-specific network for each sample.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading input data ...\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 1:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6451915130131666\n",
      "step: 1, hamming: 0.6067461274659866\n",
      "step: 2, hamming: 0.6105604110178084\n",
      "step: 3, hamming: 0.5875350566544837\n",
      "step: 4, hamming: 0.553598317095621\n",
      "step: 5, hamming: 0.5130977642028344\n",
      "step: 6, hamming: 0.46888897174383576\n",
      "step: 7, hamming: 0.42325335262309544\n",
      "step: 8, hamming: 0.3780071410446651\n",
      "step: 9, hamming: 0.3345126063650471\n",
      "step: 10, hamming: 0.29371889150860553\n",
      "step: 11, hamming: 0.256215635375215\n",
      "step: 12, hamming: 0.22230296688024526\n",
      "step: 13, hamming: 0.19205902295385965\n",
      "step: 14, hamming: 0.1654029965339068\n",
      "step: 15, hamming: 0.14214587924488872\n",
      "step: 16, hamming: 0.12203018089850885\n",
      "step: 17, hamming: 0.10475927721891458\n",
      "step: 18, hamming: 0.09001790128025045\n",
      "step: 19, hamming: 0.07748617353400274\n",
      "step: 20, hamming: 0.06684920778474639\n",
      "step: 21, hamming: 0.05780549039322097\n",
      "step: 22, hamming: 0.05007527318165609\n",
      "step: 23, hamming: 0.04341014640011884\n",
      "step: 24, hamming: 0.037602349632078355\n",
      "step: 25, hamming: 0.03249107812351406\n",
      "step: 26, hamming: 0.027962472467870057\n",
      "step: 27, hamming: 0.023942393402995726\n",
      "step: 28, hamming: 0.02038421216362342\n",
      "step: 29, hamming: 0.017255759085262886\n",
      "step: 30, hamming: 0.014529132197793934\n",
      "step: 31, hamming: 0.012174934686279325\n",
      "step: 32, hamming: 0.01016047354805776\n",
      "step: 33, hamming: 0.00845045102340879\n",
      "step: 34, hamming: 0.007008679501096658\n",
      "step: 35, hamming: 0.005799847383305955\n",
      "step: 36, hamming: 0.004790869857934986\n",
      "step: 37, hamming: 0.003951715021127735\n",
      "step: 38, hamming: 0.0032557729560454486\n",
      "step: 39, hamming: 0.0026798912102927977\n",
      "step: 40, hamming: 0.0022041994917917375\n",
      "step: 41, hamming: 0.0018118190733138495\n",
      "step: 42, hamming: 0.0014885231793603086\n",
      "step: 43, hamming: 0.0012223905275455381\n",
      "step: 44, hamming: 0.0010034762989084784\n",
      "step: 45, hamming: 0.0008235128661467346\n",
      "Running panda took: 1.24 seconds!\n",
      "  Elapsed time: 1.24 sec.\n",
      "Saving LIONESS network 1 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 2:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6445476038755726\n",
      "step: 1, hamming: 0.6066183636796103\n",
      "step: 2, hamming: 0.6105445645592842\n",
      "step: 3, hamming: 0.5875332090279165\n",
      "step: 4, hamming: 0.5536082244075643\n",
      "step: 5, hamming: 0.5131126090450783\n",
      "step: 6, hamming: 0.4689036142976635\n",
      "step: 7, hamming: 0.4232673243729872\n",
      "step: 8, hamming: 0.37801889244651404\n",
      "step: 9, hamming: 0.33452219983661163\n",
      "step: 10, hamming: 0.2937266882099521\n",
      "step: 11, hamming: 0.2562219876264076\n",
      "step: 12, hamming: 0.2223082180455078\n",
      "step: 13, hamming: 0.192063458293167\n",
      "step: 14, hamming: 0.16540684598000693\n",
      "step: 15, hamming: 0.14214929350383068\n",
      "step: 16, hamming: 0.1220332658570197\n",
      "step: 17, hamming: 0.10476213861746783\n",
      "step: 18, hamming: 0.09002061402300265\n",
      "step: 19, hamming: 0.07748873372939052\n",
      "step: 20, hamming: 0.06685160145106346\n",
      "step: 21, hamming: 0.05780769330088384\n",
      "step: 22, hamming: 0.050077286995798435\n",
      "step: 23, hamming: 0.04341196756403582\n",
      "step: 24, hamming: 0.03760397086506173\n",
      "step: 25, hamming: 0.032492502477724144\n",
      "step: 26, hamming: 0.02796370867990172\n",
      "step: 27, hamming: 0.023943455630073412\n",
      "step: 28, hamming: 0.02038511733123427\n",
      "step: 29, hamming: 0.017256526537376277\n",
      "step: 30, hamming: 0.014529780088585702\n",
      "step: 31, hamming: 0.012175478875860603\n",
      "step: 32, hamming: 0.010160929649048585\n",
      "step: 33, hamming: 0.00845083256181436\n",
      "step: 34, hamming: 0.007008998275263659\n",
      "step: 35, hamming: 0.005800112931660261\n",
      "step: 36, hamming: 0.004791090473343346\n",
      "step: 37, hamming: 0.003951898060626235\n",
      "step: 38, hamming: 0.00325592456445805\n",
      "step: 39, hamming: 0.0026800165805499643\n",
      "step: 40, hamming: 0.0022043030339044378\n",
      "step: 41, hamming: 0.0018119045001386961\n",
      "step: 42, hamming: 0.0014885935906914832\n",
      "step: 43, hamming: 0.0012224485182821014\n",
      "step: 44, hamming: 0.001003524027229861\n",
      "step: 45, hamming: 0.000823552124868469\n",
      "Running panda took: 1.27 seconds!\n",
      "  Elapsed time: 1.27 sec.\n",
      "Saving LIONESS network 2 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 3:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6452294464816624\n",
      "step: 1, hamming: 0.6066718767537945\n",
      "step: 2, hamming: 0.6105013812906783\n",
      "step: 3, hamming: 0.5874769465007301\n",
      "step: 4, hamming: 0.5535467800773779\n",
      "step: 5, hamming: 0.5130509623219738\n",
      "step: 6, hamming: 0.46884619807330863\n",
      "step: 7, hamming: 0.42321422343773074\n",
      "step: 8, hamming: 0.37797019212390504\n",
      "step: 9, hamming: 0.3344779785095642\n",
      "step: 10, hamming: 0.29368704884702534\n",
      "step: 11, hamming: 0.25618720406795764\n",
      "step: 12, hamming: 0.22227787688473347\n",
      "step: 13, hamming: 0.19203719585754386\n",
      "step: 14, hamming: 0.16538418880501976\n",
      "step: 15, hamming: 0.14212975837567024\n",
      "step: 16, hamming: 0.12201639360763507\n",
      "step: 17, hamming: 0.10474755327204792\n",
      "step: 18, hamming: 0.09000797360987652\n",
      "step: 19, hamming: 0.07747770691527575\n",
      "step: 20, hamming: 0.06684195286417426\n",
      "step: 21, hamming: 0.05779924885457799\n",
      "step: 22, hamming: 0.0500699126573237\n",
      "step: 23, hamming: 0.04340556363927609\n",
      "step: 24, hamming: 0.03759846201370095\n",
      "step: 25, hamming: 0.032487809256736604\n",
      "step: 26, hamming: 0.02795974130382637\n",
      "step: 27, hamming: 0.023940126981286655\n",
      "step: 28, hamming: 0.020382340568109492\n",
      "step: 29, hamming: 0.017254219929107445\n",
      "step: 30, hamming: 0.01452786846588523\n",
      "step: 31, hamming: 0.01217389759028471\n",
      "step: 32, hamming: 0.010159622712944908\n",
      "step: 33, hamming: 0.008449753233040156\n",
      "step: 34, hamming: 0.00700810750381105\n",
      "step: 35, hamming: 0.00579937824697735\n",
      "step: 36, hamming: 0.004790484906858998\n",
      "step: 37, hamming: 0.003951399167495923\n",
      "step: 38, hamming: 0.003255513771648308\n",
      "step: 39, hamming: 0.002679678513780071\n",
      "step: 40, hamming: 0.002204024959443254\n",
      "step: 41, hamming: 0.0018116758740187903\n",
      "step: 42, hamming: 0.0014884057006634569\n",
      "step: 43, hamming: 0.0012222941612371455\n",
      "step: 44, hamming: 0.0010033972608179476\n",
      "step: 45, hamming: 0.0008234480506902824\n",
      "Running panda took: 1.36 seconds!\n",
      "  Elapsed time: 1.36 sec.\n",
      "Saving LIONESS network 3 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 4:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6451852429809524\n",
      "step: 1, hamming: 0.6065219652377926\n",
      "step: 2, hamming: 0.6104344926479941\n",
      "step: 3, hamming: 0.5874558460006226\n",
      "step: 4, hamming: 0.553544963247136\n",
      "step: 5, hamming: 0.5130643774408143\n",
      "step: 6, hamming: 0.46887054373788917\n",
      "step: 7, hamming: 0.4232496104307795\n",
      "step: 8, hamming: 0.37801701958351697\n",
      "step: 9, hamming: 0.3345340739226694\n",
      "step: 10, hamming: 0.2937480738815744\n",
      "step: 11, hamming: 0.2562488692873377\n",
      "step: 12, hamming: 0.2223372706725651\n",
      "step: 13, hamming: 0.1920924868386737\n",
      "step: 14, hamming: 0.16543446318988872\n",
      "step: 15, hamming: 0.1421747497816679\n",
      "step: 16, hamming: 0.12205609283083736\n",
      "step: 17, hamming: 0.10478214956850972\n",
      "step: 18, hamming: 0.09003785868900757\n",
      "step: 19, hamming: 0.07750337290976046\n",
      "step: 20, hamming: 0.06686386614000323\n",
      "step: 21, hamming: 0.05781785478415813\n",
      "step: 22, hamming: 0.05008562777061508\n",
      "step: 23, hamming: 0.04341877501262821\n",
      "step: 24, hamming: 0.03760951592560503\n",
      "step: 25, hamming: 0.032497011652074345\n",
      "step: 26, hamming: 0.027967369113727127\n",
      "step: 27, hamming: 0.02394642097778111\n",
      "step: 28, hamming: 0.02038752008866367\n",
      "step: 29, hamming: 0.017258473103985816\n",
      "step: 30, hamming: 0.014531357492569013\n",
      "step: 31, hamming: 0.012176759183597894\n",
      "step: 32, hamming: 0.010161970824797957\n",
      "step: 33, hamming: 0.008451681020184565\n",
      "step: 34, hamming: 0.007009691223106566\n",
      "step: 35, hamming: 0.005800680312786877\n",
      "step: 36, hamming: 0.00479155596191386\n",
      "step: 37, hamming: 0.003952280550194625\n",
      "step: 38, hamming: 0.0032562392620936955\n",
      "step: 39, hamming: 0.0026802756996071823\n",
      "step: 40, hamming: 0.0022045164857199514\n",
      "step: 41, hamming: 0.0018120803581844036\n",
      "step: 42, hamming: 0.0014887385206301762\n",
      "step: 43, hamming: 0.0012225679351576793\n",
      "step: 44, hamming: 0.001003622395160614\n",
      "step: 45, hamming: 0.0008236331272743103\n",
      "Running panda took: 1.37 seconds!\n",
      "  Elapsed time: 1.37 sec.\n",
      "Saving LIONESS network 4 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 5:\n",
      "Computing coexpression network:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6446007808140573\n",
      "step: 1, hamming: 0.6066561612626608\n",
      "step: 2, hamming: 0.6105143267305173\n",
      "step: 3, hamming: 0.5875153809819786\n",
      "step: 4, hamming: 0.5536025207133696\n",
      "step: 5, hamming: 0.5131152115952172\n",
      "step: 6, hamming: 0.468908514507496\n",
      "step: 7, hamming: 0.42326886776988787\n",
      "step: 8, hamming: 0.3780158211356529\n",
      "step: 9, hamming: 0.3345152633298887\n",
      "step: 10, hamming: 0.29371784199314155\n",
      "step: 11, hamming: 0.2562128416941286\n",
      "step: 12, hamming: 0.22229937209191522\n",
      "step: 13, hamming: 0.1920554156626861\n",
      "step: 14, hamming: 0.1653996761052681\n",
      "step: 15, hamming: 0.14214304912137624\n",
      "step: 16, hamming: 0.12202778290912643\n",
      "step: 17, hamming: 0.10475728352365837\n",
      "step: 18, hamming: 0.090016151593947\n",
      "step: 19, hamming: 0.07748448858975361\n",
      "step: 20, hamming: 0.06684754079961648\n",
      "step: 21, hamming: 0.05780382637771552\n",
      "step: 22, hamming: 0.050073651393493325\n",
      "step: 23, hamming: 0.043408598021709384\n",
      "step: 24, hamming: 0.0376009243886287\n",
      "step: 25, hamming: 0.03248980843262918\n",
      "step: 26, hamming: 0.027961372564714905\n",
      "step: 27, hamming: 0.023941467122490446\n",
      "step: 28, hamming: 0.020383446949155254\n",
      "step: 29, hamming: 0.017255136000071034\n",
      "step: 30, hamming: 0.014528627076186464\n",
      "step: 31, hamming: 0.012174525611025125\n",
      "step: 32, hamming: 0.010160143004486259\n",
      "step: 33, hamming: 0.008450183672932479\n",
      "step: 34, hamming: 0.007008462914496333\n",
      "step: 35, hamming: 0.005799671567508499\n",
      "step: 36, hamming: 0.004790726755136287\n",
      "step: 37, hamming: 0.003951598399939781\n",
      "step: 38, hamming: 0.0032556778303245064\n",
      "step: 39, hamming: 0.002679813604130496\n",
      "step: 40, hamming: 0.002204136137859893\n",
      "step: 41, hamming: 0.0018117673263748342\n",
      "step: 42, hamming: 0.0014884808938152945\n",
      "step: 43, hamming: 0.001222355966581887\n",
      "step: 44, hamming: 0.0010034480459104299\n",
      "step: 45, hamming: 0.0008234897655709252\n",
      "Running panda took: 1.52 seconds!\n",
      "  Elapsed time: 1.53 sec.\n",
      "Saving LIONESS network 5 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 6:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.644914727175676\n",
      "step: 1, hamming: 0.6066459340545902\n",
      "step: 2, hamming: 0.6105137892381168\n",
      "step: 3, hamming: 0.5875076422353582\n",
      "step: 4, hamming: 0.553580794787596\n",
      "step: 5, hamming: 0.5130868604034443\n",
      "step: 6, hamming: 0.4688806568825481\n",
      "step: 7, hamming: 0.42324491174509904\n",
      "step: 8, hamming: 0.37799750607572563\n",
      "step: 9, hamming: 0.33450195921162135\n",
      "step: 10, hamming: 0.29370783535904643\n",
      "step: 11, hamming: 0.25620490280375174\n",
      "step: 12, hamming: 0.22229274056922746\n",
      "step: 13, hamming: 0.19204960851436947\n",
      "step: 14, hamming: 0.1653945212440373\n",
      "step: 15, hamming: 0.142138365221463\n",
      "step: 16, hamming: 0.12202359171841876\n",
      "step: 17, hamming: 0.1047535348014802\n",
      "step: 18, hamming: 0.09001300549732213\n",
      "step: 19, hamming: 0.07748198595168344\n",
      "step: 20, hamming: 0.06684561523161113\n",
      "step: 21, hamming: 0.05780239331926075\n",
      "step: 22, hamming: 0.05007261907425589\n",
      "step: 23, hamming: 0.043407887819397944\n",
      "step: 24, hamming: 0.03760046113425872\n",
      "step: 25, hamming: 0.032489527040096326\n",
      "step: 26, hamming: 0.027961217331469623\n",
      "step: 27, hamming: 0.023941395280352773\n",
      "step: 28, hamming: 0.020383428084858147\n",
      "step: 29, hamming: 0.017255148056676124\n",
      "step: 30, hamming: 0.01452865748618028\n",
      "step: 31, hamming: 0.012174566104760918\n",
      "step: 32, hamming: 0.010160186691797838\n",
      "step: 33, hamming: 0.008450226110780602\n",
      "step: 34, hamming: 0.007008501791631317\n",
      "step: 35, hamming: 0.005799706008307911\n",
      "step: 36, hamming: 0.004790756420696586\n",
      "step: 37, hamming: 0.003951623456964684\n",
      "step: 38, hamming: 0.003255698700340377\n",
      "step: 39, hamming: 0.0026798307735675883\n",
      "step: 40, hamming: 0.0022041501770806667\n",
      "step: 41, hamming: 0.0018117787582195672\n",
      "step: 42, hamming: 0.0014884901773124198\n",
      "step: 43, hamming: 0.0012223635096057423\n",
      "step: 44, hamming: 0.0010034541834039337\n",
      "step: 45, hamming: 0.0008234947565908471\n",
      "Running panda took: 1.70 seconds!\n",
      "  Elapsed time: 1.71 sec.\n",
      "Saving LIONESS network 6 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 7:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6446977027334075\n",
      "step: 1, hamming: 0.6064843916807413\n",
      "step: 2, hamming: 0.6103824327902414\n",
      "step: 3, hamming: 0.5873860105971278\n",
      "step: 4, hamming: 0.553462428350063\n",
      "step: 5, hamming: 0.5129759831080029\n",
      "step: 6, hamming: 0.46877915323218355\n",
      "step: 7, hamming: 0.42315760514309175\n",
      "step: 8, hamming: 0.37792629014527557\n",
      "step: 9, hamming: 0.3344456468150189\n",
      "step: 10, hamming: 0.29366421996136893\n",
      "step: 11, hamming: 0.2561711405522022\n",
      "step: 12, hamming: 0.22226630385566215\n",
      "step: 13, hamming: 0.1920287469868379\n",
      "step: 14, hamming: 0.16537787361436804\n",
      "step: 15, hamming: 0.1421248493580307\n",
      "step: 16, hamming: 0.12201243940238896\n",
      "step: 17, hamming: 0.10474418411446189\n",
      "step: 18, hamming: 0.09000497478051855\n",
      "step: 19, hamming: 0.07747497601974163\n",
      "step: 20, hamming: 0.06683946462194992\n",
      "step: 21, hamming: 0.05779698175620104\n",
      "step: 22, hamming: 0.05006786610992063\n",
      "step: 23, hamming: 0.043403719979106144\n",
      "step: 24, hamming: 0.03759681143075523\n",
      "step: 25, hamming: 0.03248634755930703\n",
      "step: 26, hamming: 0.02795846902706733\n",
      "step: 27, hamming: 0.023939039176463242\n",
      "step: 28, hamming: 0.020381427355326855\n",
      "step: 29, hamming: 0.017253462963678152\n",
      "step: 30, hamming: 0.014527247466191108\n",
      "step: 31, hamming: 0.012173391298399963\n",
      "step: 32, hamming: 0.01015921171833301\n",
      "step: 33, hamming: 0.008449420102928762\n",
      "step: 34, hamming: 0.007007837027271363\n",
      "step: 35, hamming: 0.0057991584489345185\n",
      "step: 36, hamming: 0.004790305989739413\n",
      "step: 37, hamming: 0.00395125337010865\n",
      "step: 38, hamming: 0.0032553948541319596\n",
      "step: 39, hamming: 0.0026795814132303626\n",
      "step: 40, hamming: 0.0022039455997269405\n",
      "step: 41, hamming: 0.0018116109753956102\n",
      "step: 42, hamming: 0.0014883526051433333\n",
      "step: 43, hamming: 0.001222250710520977\n",
      "step: 44, hamming: 0.0010033616948213141\n",
      "step: 45, hamming: 0.0008234189320553601\n",
      "Running panda took: 1.71 seconds!\n",
      "  Elapsed time: 1.71 sec.\n",
      "Saving LIONESS network 7 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 8:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6459338403224688\n",
      "step: 1, hamming: 0.6066595645190107\n",
      "step: 2, hamming: 0.6104610627797933\n",
      "step: 3, hamming: 0.5874377958138726\n",
      "step: 4, hamming: 0.5535034427944524\n",
      "step: 5, hamming: 0.5130076219885702\n",
      "step: 6, hamming: 0.4688044641107459\n",
      "step: 7, hamming: 0.423176090435273\n",
      "step: 8, hamming: 0.37793707228730244\n",
      "step: 9, hamming: 0.3344497542143437\n",
      "step: 10, hamming: 0.29366290145773005\n",
      "step: 11, hamming: 0.2561661870411493\n",
      "step: 12, hamming: 0.2222596495321926\n",
      "step: 13, hamming: 0.19202134254653255\n",
      "step: 14, hamming: 0.16537035801892333\n",
      "step: 15, hamming: 0.14211767839413375\n",
      "step: 16, hamming: 0.1220058624837419\n",
      "step: 17, hamming: 0.10473836447075485\n",
      "step: 18, hamming: 0.0899999344875568\n",
      "step: 19, hamming: 0.07747066783111718\n",
      "step: 20, hamming: 0.06683582626078942\n",
      "step: 21, hamming: 0.05779392295165679\n",
      "step: 22, hamming: 0.050065285528220704\n",
      "step: 23, hamming: 0.04340155522633314\n",
      "step: 24, hamming: 0.03759500641756283\n",
      "step: 25, hamming: 0.03248484646178138\n",
      "step: 26, hamming: 0.02795721995514243\n",
      "step: 27, hamming: 0.023938001058869156\n",
      "step: 28, hamming: 0.020380561586156625\n",
      "step: 29, hamming: 0.017252738442971306\n",
      "step: 30, hamming: 0.014526640538941682\n",
      "step: 31, hamming: 0.012172882604496534\n",
      "step: 32, hamming: 0.010158785500792763\n",
      "step: 33, hamming: 0.008449063051904349\n",
      "step: 34, hamming: 0.0070075388029142436\n",
      "step: 35, hamming: 0.005798910108587988\n",
      "step: 36, hamming: 0.004790099693035461\n",
      "step: 37, hamming: 0.003951082234771274\n",
      "step: 38, hamming: 0.0032552531011320086\n",
      "step: 39, hamming: 0.002679464173946342\n",
      "step: 40, hamming: 0.0022038487570621634\n",
      "step: 41, hamming: 0.0018115310562277804\n",
      "step: 42, hamming: 0.0014882867079583839\n",
      "step: 43, hamming: 0.0012221964189122294\n",
      "step: 44, hamming: 0.0010033169964392796\n",
      "step: 45, hamming: 0.0008233821537042094\n",
      "Running panda took: 1.56 seconds!\n",
      "  Elapsed time: 1.56 sec.\n",
      "Saving LIONESS network 8 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 9:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.644835199383118\n",
      "step: 1, hamming: 0.6064879489440818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2, hamming: 0.6103640777269351\n",
      "step: 3, hamming: 0.5873512050264047\n",
      "step: 4, hamming: 0.5534219407870922\n",
      "step: 5, hamming: 0.5129356817661759\n",
      "step: 6, hamming: 0.46874212604373694\n",
      "step: 7, hamming: 0.4231236796419835\n",
      "step: 8, hamming: 0.3778956914256716\n",
      "step: 9, hamming: 0.3344187635926774\n",
      "step: 10, hamming: 0.2936404320381599\n",
      "step: 11, hamming: 0.25615039464431727\n",
      "step: 12, hamming: 0.22224837772568842\n",
      "step: 13, hamming: 0.1920133441212464\n",
      "step: 14, hamming: 0.16536460861948546\n",
      "step: 15, hamming: 0.14211341395771593\n",
      "step: 16, hamming: 0.12200253916357386\n",
      "step: 17, hamming: 0.10473558500674479\n",
      "step: 18, hamming: 0.0899975091761514\n",
      "step: 19, hamming: 0.07746846011445685\n",
      "step: 20, hamming: 0.0668337240386083\n",
      "step: 21, hamming: 0.05779188696902117\n",
      "step: 22, hamming: 0.05006331165089909\n",
      "step: 23, hamming: 0.04339965073511143\n",
      "step: 24, hamming: 0.03759319857032192\n",
      "step: 25, hamming: 0.03248315937467175\n",
      "step: 26, hamming: 0.02795567387570829\n",
      "step: 27, hamming: 0.023936604678199185\n",
      "step: 28, hamming: 0.020379322269235813\n",
      "step: 29, hamming: 0.017251656706988562\n",
      "step: 30, hamming: 0.01452570782373028\n",
      "step: 31, hamming: 0.012172086330504326\n",
      "step: 32, hamming: 0.010158111149147997\n",
      "step: 33, hamming: 0.008448495729481614\n",
      "step: 34, hamming: 0.00700706386061792\n",
      "step: 35, hamming: 0.005798513835817575\n",
      "step: 36, hamming: 0.004789769960958667\n",
      "step: 37, hamming: 0.003950808640232261\n",
      "step: 38, hamming: 0.0032550265635088484\n",
      "step: 39, hamming: 0.0026792769994106813\n",
      "step: 40, hamming: 0.002203694319478647\n",
      "step: 41, hamming: 0.0018114037631512844\n",
      "step: 42, hamming: 0.001488181878943554\n",
      "step: 43, hamming: 0.0012221101496973901\n",
      "step: 44, hamming: 0.0010032460442440806\n",
      "step: 45, hamming: 0.0008233238300691849\n",
      "Running panda took: 1.53 seconds!\n",
      "  Elapsed time: 1.53 sec.\n",
      "Saving LIONESS network 9 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 10:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6446385831077223\n",
      "step: 1, hamming: 0.6067294163043445\n",
      "step: 2, hamming: 0.6105288665126055\n",
      "step: 3, hamming: 0.5875025168461765\n",
      "step: 4, hamming: 0.5535668756916816\n",
      "step: 5, hamming: 0.5130674611873673\n",
      "step: 6, hamming: 0.46885934016141356\n",
      "step: 7, hamming: 0.4232239036245543\n",
      "step: 8, hamming: 0.37797846107383204\n",
      "step: 9, hamming: 0.3344846104039678\n",
      "step: 10, hamming: 0.29369144682583337\n",
      "step: 11, hamming: 0.25618942581891635\n",
      "step: 12, hamming: 0.22227830891129285\n",
      "step: 13, hamming: 0.19203629520565335\n",
      "step: 14, hamming: 0.16538234596848075\n",
      "step: 15, hamming: 0.14212736448871938\n",
      "step: 16, hamming: 0.12201379782743488\n",
      "step: 17, hamming: 0.10474497350107496\n",
      "step: 18, hamming: 0.09000555414562793\n",
      "step: 19, hamming: 0.07747554704258396\n",
      "step: 20, hamming: 0.06684008398127479\n",
      "step: 21, hamming: 0.05779766899483369\n",
      "step: 22, hamming: 0.05006859159088394\n",
      "step: 23, hamming: 0.04340445929636735\n",
      "step: 24, hamming: 0.03759753082194868\n",
      "step: 25, hamming: 0.032487015363301903\n",
      "step: 26, hamming: 0.027959057287518883\n",
      "step: 27, hamming: 0.023939533290787757\n",
      "step: 28, hamming: 0.020381824851220524\n",
      "step: 29, hamming: 0.017253772055378196\n",
      "step: 30, hamming: 0.014527481551427154\n",
      "step: 31, hamming: 0.012173565486877314\n",
      "step: 32, hamming: 0.01015933951894841\n",
      "step: 33, hamming: 0.008449513032409811\n",
      "step: 34, hamming: 0.0070079049394139465\n",
      "step: 35, hamming: 0.005799208359310958\n",
      "step: 36, hamming: 0.004790342845117795\n",
      "step: 37, hamming: 0.0039512806754933355\n",
      "step: 38, hamming: 0.003255415200843872\n",
      "step: 39, hamming: 0.002679596688821237\n",
      "step: 40, hamming: 0.0022039571502874525\n",
      "step: 41, hamming: 0.0018116197541239644\n",
      "step: 42, hamming: 0.0014883593143952011\n",
      "step: 43, hamming: 0.0012222558625903583\n",
      "step: 44, hamming: 0.0010033656704848061\n",
      "step: 45, hamming: 0.0008234220134066721\n",
      "Running panda took: 1.16 seconds!\n",
      "  Elapsed time: 1.16 sec.\n",
      "Saving LIONESS network 10 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 11:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6445970723339551\n",
      "step: 1, hamming: 0.6063968880265757\n",
      "step: 2, hamming: 0.6103628950894618\n",
      "step: 3, hamming: 0.5873891776463085\n",
      "step: 4, hamming: 0.5534802081887704\n",
      "step: 5, hamming: 0.513002081253175\n",
      "step: 6, hamming: 0.4688120305062983\n",
      "step: 7, hamming: 0.42319558082755526\n",
      "step: 8, hamming: 0.37796866634930115\n",
      "step: 9, hamming: 0.33449062529413504\n",
      "step: 10, hamming: 0.2937094300379334\n",
      "step: 11, hamming: 0.25621479946024567\n",
      "step: 12, hamming: 0.22230732905919462\n",
      "step: 13, hamming: 0.1920663351741951\n",
      "step: 14, hamming: 0.16541153798715835\n",
      "step: 15, hamming: 0.14215454775653846\n",
      "step: 16, hamming: 0.12203842025372977\n",
      "step: 17, hamming: 0.10476672314281758\n",
      "step: 18, hamming: 0.09002441117486851\n",
      "step: 19, hamming: 0.0774916478941815\n",
      "step: 20, hamming: 0.06685368125563591\n",
      "step: 21, hamming: 0.057809051101809394\n",
      "step: 22, hamming: 0.05007803092604654\n",
      "step: 23, hamming: 0.043412213334691245\n",
      "step: 24, hamming: 0.03760386116781324\n",
      "step: 25, hamming: 0.03249215842908324\n",
      "step: 26, hamming: 0.027963237690867417\n",
      "step: 27, hamming: 0.023942944227938814\n",
      "step: 28, hamming: 0.020384620173815014\n",
      "step: 29, hamming: 0.01725607696542481\n",
      "step: 30, hamming: 0.014529391608107673\n",
      "step: 31, hamming: 0.012175154165384169\n",
      "step: 32, hamming: 0.010160664950073586\n",
      "step: 33, hamming: 0.008450619213181503\n",
      "step: 34, hamming: 0.0070088270324212805\n",
      "step: 35, hamming: 0.005799976086644725\n",
      "step: 36, hamming: 0.004790981350177024\n",
      "step: 37, hamming: 0.00395181094359943\n",
      "step: 38, hamming: 0.003255854969433178\n",
      "step: 39, hamming: 0.0026799609209546085\n",
      "step: 40, hamming: 0.002204258454443337\n",
      "step: 41, hamming: 0.001811868738304759\n",
      "step: 42, hamming: 0.0014885648646981047\n",
      "step: 43, hamming: 0.0012224254093928502\n",
      "step: 44, hamming: 0.0010035054146104428\n",
      "step: 45, hamming: 0.000823537115427815\n",
      "Running panda took: 1.15 seconds!\n",
      "  Elapsed time: 1.15 sec.\n",
      "Saving LIONESS network 11 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 12:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6449227035246179\n",
      "step: 1, hamming: 0.6067351153769638\n",
      "step: 2, hamming: 0.6106375012468723\n",
      "step: 3, hamming: 0.5876437952563218\n",
      "step: 4, hamming: 0.5537178078810691\n",
      "step: 5, hamming: 0.5132213547717155\n",
      "step: 6, hamming: 0.4690101547789507\n",
      "step: 7, hamming: 0.4233685648402591\n",
      "step: 8, hamming: 0.37811364961234867\n",
      "step: 9, hamming: 0.33460905552916437\n",
      "step: 10, hamming: 0.2938046259139321\n",
      "step: 11, hamming: 0.2562907749359493\n",
      "step: 12, hamming: 0.22236807687185267\n",
      "step: 13, hamming: 0.19211508180706444\n",
      "step: 14, hamming: 0.16545097662140518\n",
      "step: 15, hamming: 0.14218673742040688\n",
      "step: 16, hamming: 0.12206484516686276\n",
      "step: 17, hamming: 0.10478861898320703\n",
      "step: 18, hamming: 0.09004269287907357\n",
      "step: 19, hamming: 0.07750702507205871\n",
      "step: 20, hamming: 0.06686675609772615\n",
      "step: 21, hamming: 0.05782028426552099\n",
      "step: 22, hamming: 0.050087769533937905\n",
      "step: 23, hamming: 0.043420696376732626\n",
      "step: 24, hamming: 0.037611262734358726\n",
      "step: 25, hamming: 0.032498609240512365\n",
      "step: 26, hamming: 0.027968830176749452\n",
      "step: 27, hamming: 0.023947752343236345\n",
      "step: 28, hamming: 0.020388722079863027\n",
      "step: 29, hamming: 0.01725954609362629\n",
      "step: 30, hamming: 0.014532302553875656\n",
      "step: 31, hamming: 0.012177581011002225\n",
      "step: 32, hamming: 0.010162676990644746\n",
      "step: 33, hamming: 0.008452281514633632\n",
      "step: 34, hamming: 0.007010197176704603\n",
      "step: 35, hamming: 0.0058011034340297776\n",
      "step: 36, hamming: 0.0047919078009273945\n",
      "step: 37, hamming: 0.003952571735671334\n",
      "step: 38, hamming: 0.0032564793806740488\n",
      "step: 39, hamming: 0.002680473211329055\n",
      "step: 40, hamming: 0.0022046786306772798\n",
      "step: 41, hamming: 0.0018122132860573493\n",
      "step: 42, hamming: 0.001488847347481173\n",
      "step: 43, hamming: 0.001222656971478183\n",
      "step: 44, hamming: 0.0010036952106255847\n",
      "step: 45, hamming: 0.0008236926599718458\n",
      "Running panda took: 1.16 seconds!\n",
      "  Elapsed time: 1.16 sec.\n",
      "Saving LIONESS network 12 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 13:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6454815045482025\n",
      "step: 1, hamming: 0.6064888334615246\n",
      "step: 2, hamming: 0.6103537850467549\n",
      "step: 3, hamming: 0.5873467987983988\n",
      "step: 4, hamming: 0.5534173891580667\n",
      "step: 5, hamming: 0.5129251262883209\n",
      "step: 6, hamming: 0.4687264900712108\n",
      "step: 7, hamming: 0.4231036921678549\n",
      "step: 8, hamming: 0.3778721072843273\n",
      "step: 9, hamming: 0.33439250879973836\n",
      "step: 10, hamming: 0.29361248905541215\n",
      "step: 11, hamming: 0.25612178099974314\n",
      "step: 12, hamming: 0.22222032595119312\n",
      "step: 13, hamming: 0.19198634671852602\n",
      "step: 14, hamming: 0.16533917584848357\n",
      "step: 15, hamming: 0.14208990495011609\n",
      "step: 16, hamming: 0.1219811952488296\n",
      "step: 17, hamming: 0.10471648707194937\n",
      "step: 18, hamming: 0.08998053903852753\n",
      "step: 19, hamming: 0.07745350260977646\n",
      "step: 20, hamming: 0.06682062957867617\n",
      "step: 21, hamming: 0.05778050118366357\n",
      "step: 22, hamming: 0.05005347176858947\n",
      "step: 23, hamming: 0.04339119603182728\n",
      "step: 24, hamming: 0.03758595561496705\n",
      "step: 25, hamming: 0.03247697829457956\n",
      "step: 26, hamming: 0.0279504168476757\n",
      "step: 27, hamming: 0.023932149875743866\n",
      "step: 28, hamming: 0.020375560744930257\n",
      "step: 29, hamming: 0.017248488850848613\n",
      "step: 30, hamming: 0.014523048953008228\n",
      "step: 31, hamming: 0.012169862212797651\n",
      "step: 32, hamming: 0.010156256059334045\n",
      "step: 33, hamming: 0.00844695177768301\n",
      "step: 34, hamming: 0.007005781213394503\n",
      "step: 35, hamming: 0.005797450270253753\n",
      "step: 36, hamming: 0.004788889434590357\n",
      "step: 37, hamming: 0.003950080544120968\n",
      "step: 38, hamming: 0.0032544251655959506\n",
      "step: 39, hamming: 0.002678780646612269\n",
      "step: 40, hamming: 0.0022032850078452723\n",
      "step: 41, hamming: 0.0018110664850589538\n",
      "step: 42, hamming: 0.001487904142285912\n",
      "step: 43, hamming: 0.0012218815774630246\n",
      "step: 44, hamming: 0.001003058031224306\n",
      "step: 45, hamming: 0.0008231692507589993\n",
      "Running panda took: 1.25 seconds!\n",
      "  Elapsed time: 1.25 sec.\n",
      "Saving LIONESS network 13 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 14:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6443394902350985\n",
      "step: 1, hamming: 0.6066808238313084\n",
      "step: 2, hamming: 0.6105000628534961\n",
      "step: 3, hamming: 0.5874753808094995\n",
      "step: 4, hamming: 0.5535342710047162\n",
      "step: 5, hamming: 0.5130322757657333\n",
      "step: 6, hamming: 0.46882485488303\n",
      "step: 7, hamming: 0.42319334974995565\n",
      "step: 8, hamming: 0.37795210281175695\n",
      "step: 9, hamming: 0.3344633679474345\n",
      "step: 10, hamming: 0.2936753764324572\n",
      "step: 11, hamming: 0.2561774279980078\n",
      "step: 12, hamming: 0.2222694459009427\n",
      "step: 13, hamming: 0.19203006056522645\n",
      "step: 14, hamming: 0.16537809623297536\n",
      "step: 15, hamming: 0.14212455805772894\n",
      "step: 16, hamming: 0.12201199051973263\n",
      "step: 17, hamming: 0.10474380875818415\n",
      "step: 18, hamming: 0.09000482045293938\n",
      "step: 19, hamming: 0.07747506491160118\n",
      "step: 20, hamming: 0.06683975370409435\n",
      "step: 21, hamming: 0.05779742984599141\n",
      "step: 22, hamming: 0.050068421477429015\n",
      "step: 23, hamming: 0.04340433264758675\n",
      "step: 24, hamming: 0.03759742735740402\n",
      "step: 25, hamming: 0.032486926776789646\n",
      "step: 26, hamming: 0.0279589832412535\n",
      "step: 27, hamming: 0.023939475652045558\n",
      "step: 28, hamming: 0.020381784048555823\n",
      "step: 29, hamming: 0.017253746643295972\n",
      "step: 30, hamming: 0.014527468810299439\n",
      "step: 31, hamming: 0.012173562253857687\n",
      "step: 32, hamming: 0.010159342835363386\n",
      "step: 33, hamming: 0.008449520673285476\n",
      "step: 34, hamming: 0.00700791419914384\n",
      "step: 35, hamming: 0.005799217819609791\n",
      "step: 36, hamming: 0.004790352300931195\n",
      "step: 37, hamming: 0.00395128963016098\n",
      "step: 38, hamming: 0.003255423348636021\n",
      "step: 39, hamming: 0.0026796039355505563\n",
      "step: 40, hamming: 0.002203963484083055\n",
      "step: 41, hamming: 0.0018116252199462354\n",
      "step: 42, hamming: 0.0014883639807463868\n",
      "step: 43, hamming: 0.0012222598171972396\n",
      "step: 44, hamming: 0.0010033690024990117\n",
      "step: 45, hamming: 0.000823424809994582\n",
      "Running panda took: 1.47 seconds!\n",
      "  Elapsed time: 1.47 sec.\n",
      "Saving LIONESS network 14 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 15:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6442838140322694\n",
      "step: 1, hamming: 0.6065432591877655\n",
      "step: 2, hamming: 0.6104256202756484\n",
      "step: 3, hamming: 0.587423936441978\n",
      "step: 4, hamming: 0.553504385366511\n",
      "step: 5, hamming: 0.5130177294826509\n",
      "step: 6, hamming: 0.4688195259817124\n",
      "step: 7, hamming: 0.42319353876507343\n",
      "step: 8, hamming: 0.37795644651552873\n",
      "step: 9, hamming: 0.3344697612732674\n",
      "step: 10, hamming: 0.29368304260522193\n",
      "step: 11, hamming: 0.25618591729315593\n",
      "step: 12, hamming: 0.22227818582319145\n",
      "step: 13, hamming: 0.19203847272328892\n",
      "step: 14, hamming: 0.16538591145842108\n",
      "step: 15, hamming: 0.14213154151205495\n",
      "step: 16, hamming: 0.12201804684367133\n",
      "step: 17, hamming: 0.10474891800358453\n",
      "step: 18, hamming: 0.09000896576805426\n",
      "step: 19, hamming: 0.07747832725281116\n",
      "step: 20, hamming: 0.06684227158737589\n",
      "step: 21, hamming: 0.05779932753307891\n",
      "step: 22, hamming: 0.05006981276657545\n",
      "step: 23, hamming: 0.04340535193546641\n",
      "step: 24, hamming: 0.037598189054180546\n",
      "step: 25, hamming: 0.0324875206504374\n",
      "step: 26, hamming: 0.027959467696440896\n",
      "step: 27, hamming: 0.023939886206924728\n",
      "step: 28, hamming: 0.020382138917549576\n",
      "step: 29, hamming: 0.017254056868073377\n",
      "step: 30, hamming: 0.014527740108929025\n",
      "step: 31, hamming: 0.01217379823366811\n",
      "step: 32, hamming: 0.010159546268615196\n",
      "step: 33, hamming: 0.008449694062619572\n",
      "step: 34, hamming: 0.007008060926552581\n",
      "step: 35, hamming: 0.005799341301245976\n",
      "step: 36, hamming: 0.004790455394888103\n",
      "step: 37, hamming: 0.003951375363936759\n",
      "step: 38, hamming: 0.003255494471286037\n",
      "step: 39, hamming: 0.0026796628068911565\n",
      "step: 40, hamming: 0.0022040121296349868\n",
      "step: 41, hamming: 0.001811665368656811\n",
      "step: 42, hamming: 0.0014883970876660596\n",
      "step: 43, hamming: 0.0012222870952029116\n",
      "step: 44, hamming: 0.0010033914641085328\n",
      "step: 45, hamming: 0.0008234432936021228\n",
      "Running panda took: 1.23 seconds!\n",
      "  Elapsed time: 1.23 sec.\n",
      "Saving LIONESS network 15 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 16:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6451357933509942\n",
      "step: 1, hamming: 0.6066460429428087\n",
      "step: 2, hamming: 0.6104672809769209\n",
      "step: 3, hamming: 0.5874429022191956\n",
      "step: 4, hamming: 0.5535053796996752\n",
      "step: 5, hamming: 0.5130102568327171\n",
      "step: 6, hamming: 0.46880788051023403\n",
      "step: 7, hamming: 0.4231820754210904\n",
      "step: 8, hamming: 0.37794630825110404\n",
      "step: 9, hamming: 0.33446172175110056\n",
      "step: 10, hamming: 0.2936769576201944\n",
      "step: 11, hamming: 0.2561810037908882\n",
      "step: 12, hamming: 0.22227406388621238\n",
      "step: 13, hamming: 0.19203490092286687\n",
      "step: 14, hamming: 0.1653828131013553\n",
      "step: 15, hamming: 0.14212890528332242\n",
      "step: 16, hamming: 0.12201583114277063\n",
      "step: 17, hamming: 0.10474707223510565\n",
      "step: 18, hamming: 0.09000753562825538\n",
      "step: 19, hamming: 0.07747728987948439\n",
      "step: 20, hamming: 0.06684154362198168\n",
      "step: 21, hamming: 0.05779884394184241\n",
      "step: 22, hamming: 0.05006952159265663\n",
      "step: 23, hamming: 0.04340519640748409\n",
      "step: 24, hamming: 0.037598110122971234\n",
      "step: 25, hamming: 0.03248747270773837\n",
      "step: 26, hamming: 0.027959425880675674\n",
      "step: 27, hamming: 0.023939839110948595\n",
      "step: 28, hamming: 0.020382085146816525\n",
      "step: 29, hamming: 0.01725399643445798\n",
      "step: 30, hamming: 0.014527676899527596\n",
      "step: 31, hamming: 0.012173735023499315\n",
      "step: 32, hamming: 0.010159486018898033\n",
      "step: 33, hamming: 0.008449638740920182\n",
      "step: 34, hamming: 0.00700801161545041\n",
      "step: 35, hamming: 0.005799298264348275\n",
      "step: 36, hamming: 0.004790418528106761\n",
      "step: 37, hamming: 0.003951344147935315\n",
      "step: 38, hamming: 0.0032554682222274765\n",
      "step: 39, hamming: 0.0026796408398129705\n",
      "step: 40, hamming: 0.002203993820812896\n",
      "step: 41, hamming: 0.0018116502262686576\n",
      "step: 42, hamming: 0.0014883845846321853\n",
      "step: 43, hamming: 0.0012222767839663946\n",
      "step: 44, hamming: 0.0010033829666874486\n",
      "step: 45, hamming: 0.0008234362966904189\n",
      "Running panda took: 1.25 seconds!\n",
      "  Elapsed time: 1.25 sec.\n",
      "Saving LIONESS network 16 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 17:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6453372188039735\n",
      "step: 1, hamming: 0.6066676287287169\n",
      "step: 2, hamming: 0.6104644764183814\n",
      "step: 3, hamming: 0.5874393760760523\n",
      "step: 4, hamming: 0.553506577030067\n",
      "step: 5, hamming: 0.5130098317112232\n",
      "step: 6, hamming: 0.46880493446531935\n",
      "step: 7, hamming: 0.42317541168503864\n",
      "step: 8, hamming: 0.3779353604570779\n",
      "step: 9, hamming: 0.33444725718243223\n",
      "step: 10, hamming: 0.2936601064819789\n",
      "step: 11, hamming: 0.2561633601925414\n",
      "step: 12, hamming: 0.2222568377191461\n",
      "step: 13, hamming: 0.19201879569173996\n",
      "step: 14, hamming: 0.16536830973066036\n",
      "step: 15, hamming: 0.1421162248277369\n",
      "step: 16, hamming: 0.12200488017569686\n",
      "step: 17, hamming: 0.10473769975276602\n",
      "step: 18, hamming: 0.08999949865507745\n",
      "step: 19, hamming: 0.077470424217288\n",
      "step: 20, hamming: 0.06683571528299424\n",
      "step: 21, hamming: 0.057793931389394995\n",
      "step: 22, hamming: 0.0500654130944078\n",
      "step: 23, hamming: 0.04340179230182477\n",
      "step: 24, hamming: 0.03759532580717451\n",
      "step: 25, hamming: 0.0324852250857994\n",
      "step: 26, hamming: 0.027957631020622514\n",
      "step: 27, hamming: 0.023938413555977024\n",
      "step: 28, hamming: 0.020380954334854536\n",
      "step: 29, hamming: 0.017253098946343814\n",
      "step: 30, hamming: 0.014526961138535554\n",
      "step: 31, hamming: 0.0121731618457686\n",
      "step: 32, hamming: 0.010159024956166728\n",
      "step: 33, hamming: 0.008449266035746403\n",
      "step: 34, hamming: 0.00700770946888819\n",
      "step: 35, hamming: 0.005799052725515068\n",
      "step: 36, hamming: 0.004790218419084313\n",
      "step: 37, hamming: 0.003951180792084351\n",
      "step: 38, hamming: 0.0032553347191006453\n",
      "step: 39, hamming: 0.0026795316594392417\n",
      "step: 40, hamming: 0.00220390448313955\n",
      "step: 41, hamming: 0.0018115770171323311\n",
      "step: 42, hamming: 0.0014883245770206178\n",
      "step: 43, hamming: 0.0012222275926846073\n",
      "step: 44, hamming: 0.0010033426427683515\n",
      "step: 45, hamming: 0.0008234032422371829\n",
      "Running panda took: 1.24 seconds!\n",
      "  Elapsed time: 1.24 sec.\n",
      "Saving LIONESS network 17 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 18:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6449357768289141\n",
      "step: 1, hamming: 0.6067478759109104\n",
      "step: 2, hamming: 0.6105187387290016\n",
      "step: 3, hamming: 0.5874835466275927\n",
      "step: 4, hamming: 0.5535417550864121\n",
      "step: 5, hamming: 0.5130402196441705\n",
      "step: 6, hamming: 0.46883185330556426\n",
      "step: 7, hamming: 0.4231991954317866\n",
      "step: 8, hamming: 0.3779561923867192\n",
      "step: 9, hamming: 0.3344661242851883\n",
      "step: 10, hamming: 0.29367727342311356\n",
      "step: 11, hamming: 0.2561788722687788\n",
      "step: 12, hamming: 0.22227085717962164\n",
      "step: 13, hamming: 0.19203122760644709\n",
      "step: 14, hamming: 0.16537907569669916\n",
      "step: 15, hamming: 0.1421253806373162\n",
      "step: 16, hamming: 0.12201265787241684\n",
      "step: 17, hamming: 0.10474431714590453\n",
      "step: 18, hamming: 0.09000515595301477\n",
      "step: 19, hamming: 0.07747531799818602\n",
      "step: 20, hamming: 0.06683996904018247\n",
      "step: 21, hamming: 0.057797597602988214\n",
      "step: 22, hamming: 0.05006852906896945\n",
      "step: 23, hamming: 0.043404392081109584\n",
      "step: 24, hamming: 0.03759745306277048\n",
      "step: 25, hamming: 0.032486925619651025\n",
      "step: 26, hamming: 0.02795896080983829\n",
      "step: 27, hamming: 0.023939439452491513\n",
      "step: 28, hamming: 0.020381738530574928\n",
      "step: 29, hamming: 0.017253695360279313\n",
      "step: 30, hamming: 0.014527415793591236\n",
      "step: 31, hamming: 0.012173510704249173\n",
      "step: 32, hamming: 0.010159294487304498\n",
      "step: 33, hamming: 0.008449476113258232\n",
      "step: 34, hamming: 0.0070078742878287355\n",
      "step: 35, hamming: 0.005799182820196557\n",
      "step: 36, hamming: 0.004790321840049444\n",
      "step: 37, hamming: 0.003951263491362447\n",
      "step: 38, hamming: 0.0032554011485745643\n",
      "step: 39, hamming: 0.002679585187798109\n",
      "step: 40, hamming: 0.0022039477307740168\n",
      "step: 41, hamming: 0.0018116120424978204\n",
      "step: 42, hamming: 0.001488352999338743\n",
      "step: 43, hamming: 0.0012222506897605569\n",
      "step: 44, hamming: 0.0010033614331443708\n",
      "step: 45, hamming: 0.0008234185581224581\n",
      "Running panda took: 1.24 seconds!\n",
      "  Elapsed time: 1.25 sec.\n",
      "Saving LIONESS network 18 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 19:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6455081786434625\n",
      "step: 1, hamming: 0.6064194187335534\n",
      "step: 2, hamming: 0.6103607543150152\n",
      "step: 3, hamming: 0.5873862777167244\n",
      "step: 4, hamming: 0.553480413292402\n",
      "step: 5, hamming: 0.5130026093014715\n",
      "step: 6, hamming: 0.4688113588544863\n",
      "step: 7, hamming: 0.42318953580922025\n",
      "step: 8, hamming: 0.37795441759448034\n",
      "step: 9, hamming: 0.33446855180911206\n",
      "step: 10, hamming: 0.29368214409431076\n",
      "step: 11, hamming: 0.2561851448411255\n",
      "step: 12, hamming: 0.22227745758160627\n",
      "step: 13, hamming: 0.192037632845968\n",
      "step: 14, hamming: 0.1653849673949693\n",
      "step: 15, hamming: 0.14213058508151208\n",
      "step: 16, hamming: 0.12201710874059979\n",
      "step: 17, hamming: 0.10474810732061202\n",
      "step: 18, hamming: 0.0900083402499391\n",
      "step: 19, hamming: 0.07747789189112711\n",
      "step: 20, hamming: 0.06684199669576117\n",
      "step: 21, hamming: 0.05779919229936601\n",
      "step: 22, hamming: 0.05006980214737102\n",
      "step: 23, hamming: 0.04340543355149784\n",
      "step: 24, hamming: 0.03759834459496707\n",
      "step: 25, hamming: 0.03248771827770938\n",
      "step: 26, hamming: 0.02795969480756446\n",
      "step: 27, hamming: 0.023940128620517382\n",
      "step: 28, hamming: 0.020382385800924933\n",
      "step: 29, hamming: 0.01725429871240458\n",
      "step: 30, hamming: 0.014527969806710518\n",
      "step: 31, hamming: 0.012174010998701938\n",
      "step: 32, hamming: 0.010159739559041083\n",
      "step: 33, hamming: 0.008449866915345711\n",
      "step: 34, hamming: 0.007008213042126161\n",
      "step: 35, hamming: 0.005799473355581619\n",
      "step: 36, hamming: 0.00479056874436645\n",
      "step: 37, hamming: 0.003951471883857408\n",
      "step: 38, hamming: 0.0032555761831291954\n",
      "step: 39, hamming: 0.0026797315837817047\n",
      "step: 40, hamming: 0.0022040697609956345\n",
      "step: 41, hamming: 0.0018117134754357912\n",
      "step: 42, hamming: 0.0014884371182850343\n",
      "step: 43, hamming: 0.0012223203233192148\n",
      "step: 44, hamming: 0.0010034189869899289\n",
      "step: 45, hamming: 0.0008234660519401823\n",
      "Running panda took: 1.24 seconds!\n",
      "  Elapsed time: 1.24 sec.\n",
      "Saving LIONESS network 19 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 20:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6439395298412643\n",
      "step: 1, hamming: 0.606444636415115\n",
      "step: 2, hamming: 0.6103874530759168\n",
      "step: 3, hamming: 0.587378224328637\n",
      "step: 4, hamming: 0.5534408069230906\n",
      "step: 5, hamming: 0.5129461968109986\n",
      "step: 6, hamming: 0.4687480609928045\n",
      "step: 7, hamming: 0.42312972606616733\n",
      "step: 8, hamming: 0.37790354425769523\n",
      "step: 9, hamming: 0.3344283359725711\n",
      "step: 10, hamming: 0.29365126515501877\n",
      "step: 11, hamming: 0.25616176939524715\n",
      "step: 12, hamming: 0.2222599888792308\n",
      "step: 13, hamming: 0.19202452498598024\n",
      "step: 14, hamming: 0.165375017595712\n",
      "step: 15, hamming: 0.14212288137746548\n",
      "step: 16, hamming: 0.12201101905345443\n",
      "step: 17, hamming: 0.10474308059192008\n",
      "step: 18, hamming: 0.09000401407761764\n",
      "step: 19, hamming: 0.07747412442844388\n",
      "step: 20, hamming: 0.06683863595804257\n",
      "step: 21, hamming: 0.057796138871261137\n",
      "step: 22, hamming: 0.05006700914121159\n",
      "step: 23, hamming: 0.043402875353600476\n",
      "step: 24, hamming: 0.03759601882243465\n",
      "step: 25, hamming: 0.032485634181046795\n",
      "step: 26, hamming: 0.027957849133881683\n",
      "step: 27, hamming: 0.023938508535798864\n",
      "step: 28, hamming: 0.020380978291102244\n",
      "step: 29, hamming: 0.01725308408299542\n",
      "step: 30, hamming: 0.014526928573715072\n",
      "step: 31, hamming: 0.012173124106190272\n",
      "step: 32, hamming: 0.01015898896686592\n",
      "step: 33, hamming: 0.008449235814467616\n",
      "step: 34, hamming: 0.007007685785454704\n",
      "step: 35, hamming: 0.005799035223298282\n",
      "step: 36, hamming: 0.0047902061802674555\n",
      "step: 37, hamming: 0.0039511729044897055\n",
      "step: 38, hamming: 0.0032553302873377593\n",
      "step: 39, hamming: 0.0026795297588183766\n",
      "step: 40, hamming: 0.0022039043608018728\n",
      "step: 41, hamming: 0.0018115780790296146\n",
      "step: 42, hamming: 0.0014883263714357737\n",
      "step: 43, hamming: 0.0012222297878657674\n",
      "step: 44, hamming: 0.0010033450031440958\n",
      "step: 45, hamming: 0.0008234056073751187\n",
      "Running panda took: 1.24 seconds!\n",
      "  Elapsed time: 1.24 sec.\n",
      "Saving LIONESS network 20 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 21:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6450848367279334\n",
      "step: 1, hamming: 0.6066233811194319\n",
      "step: 2, hamming: 0.6104738378033744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3, hamming: 0.5874590505004481\n",
      "step: 4, hamming: 0.553529311711965\n",
      "step: 5, hamming: 0.5130347272475934\n",
      "step: 6, hamming: 0.4688311364101988\n",
      "step: 7, hamming: 0.423201202950783\n",
      "step: 8, hamming: 0.3779600554810355\n",
      "step: 9, hamming: 0.33447046880848674\n",
      "step: 10, hamming: 0.29368153860449103\n",
      "step: 11, hamming: 0.256182992647495\n",
      "step: 12, hamming: 0.22227459868710234\n",
      "step: 13, hamming: 0.19203454863632288\n",
      "step: 14, hamming: 0.16538197139376914\n",
      "step: 15, hamming: 0.14212785490634228\n",
      "step: 16, hamming: 0.12201474746203654\n",
      "step: 17, hamming: 0.10474604616613903\n",
      "step: 18, hamming: 0.09000658579037367\n",
      "step: 19, hamming: 0.07747648716459518\n",
      "step: 20, hamming: 0.06684087416347566\n",
      "step: 21, hamming: 0.057798285097580865\n",
      "step: 22, hamming: 0.05006904051012411\n",
      "step: 23, hamming: 0.04340476637970672\n",
      "step: 24, hamming: 0.03759772603328491\n",
      "step: 25, hamming: 0.03248712996303483\n",
      "step: 26, hamming: 0.02795911898727303\n",
      "step: 27, hamming: 0.023939563169657063\n",
      "step: 28, hamming: 0.02038183836027533\n",
      "step: 29, hamming: 0.01725377713847119\n",
      "step: 30, hamming: 0.014527483895349053\n",
      "step: 31, hamming: 0.01217356730581402\n",
      "step: 32, hamming: 0.010159341931297585\n",
      "step: 33, hamming: 0.008449516089574675\n",
      "step: 34, hamming: 0.007007908113106246\n",
      "step: 35, hamming: 0.005799211545052688\n",
      "step: 36, hamming: 0.004790346079477174\n",
      "step: 37, hamming: 0.003951283832559894\n",
      "step: 38, hamming: 0.003255418189643129\n",
      "step: 39, hamming: 0.0026795994570336643\n",
      "step: 40, hamming: 0.0022039596712132357\n",
      "step: 41, hamming: 0.0018116220164900937\n",
      "step: 42, hamming: 0.0014883613416886991\n",
      "step: 43, hamming: 0.0012222576717636905\n",
      "step: 44, hamming: 0.0010033672638236329\n",
      "step: 45, hamming: 0.0008234234034135342\n",
      "Running panda took: 1.25 seconds!\n",
      "  Elapsed time: 1.25 sec.\n",
      "Saving LIONESS network 21 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 22:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6453859638093248\n",
      "step: 1, hamming: 0.606696579098816\n",
      "step: 2, hamming: 0.6104918352908492\n",
      "step: 3, hamming: 0.5874671302827963\n",
      "step: 4, hamming: 0.5535319083441806\n",
      "step: 5, hamming: 0.5130343438245887\n",
      "step: 6, hamming: 0.46883010704011957\n",
      "step: 7, hamming: 0.42319927931184576\n",
      "step: 8, hamming: 0.377958105463949\n",
      "step: 9, hamming: 0.3344683776483375\n",
      "step: 10, hamming: 0.29367955113712857\n",
      "step: 11, hamming: 0.2561812888003239\n",
      "step: 12, hamming: 0.22227300354666066\n",
      "step: 13, hamming: 0.1920330583902126\n",
      "step: 14, hamming: 0.16538058859573115\n",
      "step: 15, hamming: 0.14212657310449725\n",
      "step: 16, hamming: 0.12201356805469019\n",
      "step: 17, hamming: 0.10474498314173354\n",
      "step: 18, hamming: 0.09000563040511554\n",
      "step: 19, hamming: 0.07747556450073831\n",
      "step: 20, hamming: 0.06684002119191149\n",
      "step: 21, hamming: 0.05779753362320707\n",
      "step: 22, hamming: 0.05006839068356395\n",
      "step: 23, hamming: 0.0434042189139166\n",
      "step: 24, hamming: 0.03759727805610255\n",
      "step: 25, hamming: 0.03248676757264254\n",
      "step: 26, hamming: 0.027958831924822302\n",
      "step: 27, hamming: 0.02393934131099991\n",
      "step: 28, hamming: 0.0203816693839802\n",
      "step: 29, hamming: 0.017253650955294644\n",
      "step: 30, hamming: 0.014527390237321747\n",
      "step: 31, hamming: 0.01217349800996446\n",
      "step: 32, hamming: 0.010159290165315724\n",
      "step: 33, hamming: 0.008449476812725698\n",
      "step: 34, hamming: 0.00700787781935247\n",
      "step: 35, hamming: 0.005799187804456238\n",
      "step: 36, hamming: 0.004790327214202528\n",
      "step: 37, hamming: 0.003951268652192236\n",
      "step: 38, hamming: 0.003255405875696191\n",
      "step: 39, hamming: 0.002679589392770106\n",
      "step: 40, hamming: 0.0022039513978170484\n",
      "step: 41, hamming: 0.001811615192450883\n",
      "step: 42, hamming: 0.0014883556795001673\n",
      "step: 43, hamming: 0.0012222529540384143\n",
      "step: 44, hamming: 0.0010033633357891564\n",
      "step: 45, hamming: 0.0008234201342693849\n",
      "Running panda took: 1.29 seconds!\n",
      "  Elapsed time: 1.29 sec.\n",
      "Saving LIONESS network 22 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 23:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6435616555805806\n",
      "step: 1, hamming: 0.6065774661127371\n",
      "step: 2, hamming: 0.610468595413635\n",
      "step: 3, hamming: 0.5874707578579494\n",
      "step: 4, hamming: 0.5535538083324445\n",
      "step: 5, hamming: 0.5130651148399518\n",
      "step: 6, hamming: 0.4688591784891974\n",
      "step: 7, hamming: 0.4232231791435238\n",
      "step: 8, hamming: 0.37797491630976726\n",
      "step: 9, hamming: 0.33447914071665064\n",
      "step: 10, hamming: 0.29368549481398054\n",
      "step: 11, hamming: 0.25618363859574517\n",
      "step: 12, hamming: 0.22227301663758203\n",
      "step: 13, hamming: 0.19203158876314616\n",
      "step: 14, hamming: 0.1653781653066901\n",
      "step: 15, hamming: 0.14212379235777953\n",
      "step: 16, hamming: 0.12201066274790016\n",
      "step: 17, hamming: 0.10474203578042608\n",
      "step: 18, hamming: 0.09000269743578625\n",
      "step: 19, hamming: 0.07747275376080467\n",
      "step: 20, hamming: 0.06683734743392256\n",
      "step: 21, hamming: 0.0577950210516337\n",
      "step: 22, hamming: 0.05006608553256088\n",
      "step: 23, hamming: 0.04340213107403853\n",
      "step: 24, hamming: 0.03759541702968891\n",
      "step: 25, hamming: 0.03248513867836512\n",
      "step: 26, hamming: 0.02795742639550285\n",
      "step: 27, hamming: 0.02393814160249087\n",
      "step: 28, hamming: 0.020380652363289783\n",
      "step: 29, hamming: 0.017252792040855006\n",
      "step: 30, hamming: 0.014526666851716613\n",
      "step: 31, hamming: 0.012172890239012218\n",
      "step: 32, hamming: 0.010158780660191595\n",
      "step: 33, hamming: 0.008449050574685639\n",
      "step: 34, hamming: 0.007007521946824569\n",
      "step: 35, hamming: 0.0057988910697919045\n",
      "step: 36, hamming: 0.004790080090026641\n",
      "step: 37, hamming: 0.003951063180798141\n",
      "step: 38, hamming: 0.0032552352818237323\n",
      "step: 39, hamming: 0.0026794480123289452\n",
      "step: 40, hamming: 0.0022038343696844176\n",
      "step: 41, hamming: 0.0018115184280167015\n",
      "step: 42, hamming: 0.0014882757448709764\n",
      "step: 43, hamming: 0.001222186981081775\n",
      "step: 44, hamming: 0.0010033089289654196\n",
      "step: 45, hamming: 0.0008233752989717556\n",
      "Running panda took: 1.29 seconds!\n",
      "  Elapsed time: 1.29 sec.\n",
      "Saving LIONESS network 23 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 24:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6451374874258015\n",
      "step: 1, hamming: 0.6065597248100143\n",
      "step: 2, hamming: 0.6104785268315843\n",
      "step: 3, hamming: 0.5874892432769233\n",
      "step: 4, hamming: 0.553567791572826\n",
      "step: 5, hamming: 0.5130739316093189\n",
      "step: 6, hamming: 0.46886944915852674\n",
      "step: 7, hamming: 0.4232372785450944\n",
      "step: 8, hamming: 0.3779939472775404\n",
      "step: 9, hamming: 0.33450152147245443\n",
      "step: 10, hamming: 0.2937094193739473\n",
      "step: 11, hamming: 0.2562077730351675\n",
      "step: 12, hamming: 0.22229629530279874\n",
      "step: 13, hamming: 0.19205343679473316\n",
      "step: 14, hamming: 0.16539838482864916\n",
      "step: 15, hamming: 0.14214207545812796\n",
      "step: 16, hamming: 0.12202716731448059\n",
      "step: 17, hamming: 0.10475689437070312\n",
      "step: 18, hamming: 0.09001608575237814\n",
      "step: 19, hamming: 0.0774847471094978\n",
      "step: 20, hamming: 0.06684806346197887\n",
      "step: 21, hamming: 0.05780455531349574\n",
      "step: 22, hamming: 0.05007450535329472\n",
      "step: 23, hamming: 0.04340951742134202\n",
      "step: 24, hamming: 0.037601852012909276\n",
      "step: 25, hamming: 0.03249070133634119\n",
      "step: 26, hamming: 0.02796219819782178\n",
      "step: 27, hamming: 0.023942208045912385\n",
      "step: 28, hamming: 0.020384097306212396\n",
      "step: 29, hamming: 0.01725569964822445\n",
      "step: 30, hamming: 0.014529113182227082\n",
      "step: 31, hamming: 0.012174941631275542\n",
      "step: 32, hamming: 0.010160495993560505\n",
      "step: 33, hamming: 0.008450481219030376\n",
      "step: 34, hamming: 0.007008712784988989\n",
      "step: 35, hamming: 0.005799880479159794\n",
      "step: 36, hamming: 0.004790900762682554\n",
      "step: 37, hamming: 0.003951742881773002\n",
      "step: 38, hamming: 0.0032557974943049667\n",
      "step: 39, hamming: 0.0026799124669886366\n",
      "step: 40, hamming: 0.0022042177025821622\n",
      "step: 41, hamming: 0.0018118345457225498\n",
      "step: 42, hamming: 0.0014885362410434037\n",
      "step: 43, hamming: 0.0012224015057432387\n",
      "step: 44, hamming: 0.0010034854919391918\n",
      "step: 45, hamming: 0.0008235205432486282\n",
      "Running panda took: 1.24 seconds!\n",
      "  Elapsed time: 1.24 sec.\n",
      "Saving LIONESS network 24 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 25:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6457395627686134\n",
      "step: 1, hamming: 0.6064608877070142\n",
      "step: 2, hamming: 0.6103727825941945\n",
      "step: 3, hamming: 0.5873823343108445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4, hamming: 0.5534684140947953\n",
      "step: 5, hamming: 0.5129868713457605\n",
      "step: 6, hamming: 0.46879289203841173\n",
      "step: 7, hamming: 0.4231710646886092\n",
      "step: 8, hamming: 0.37793701543032326\n",
      "step: 9, hamming: 0.33445323750400024\n",
      "step: 10, hamming: 0.29366868496976617\n",
      "step: 11, hamming: 0.25617307269249323\n",
      "step: 12, hamming: 0.22226674885043102\n",
      "step: 13, hamming: 0.19202822641393982\n",
      "step: 14, hamming: 0.16537671830260126\n",
      "step: 15, hamming: 0.14212334810171476\n",
      "step: 16, hamming: 0.1220107685282419\n",
      "step: 17, hamming: 0.10474248793357537\n",
      "step: 18, hamming: 0.09000330140588078\n",
      "step: 19, hamming: 0.07747333480760728\n",
      "step: 20, hamming: 0.06683787049330008\n",
      "step: 21, hamming: 0.057795449353665554\n",
      "step: 22, hamming: 0.05006637373531308\n",
      "step: 23, hamming: 0.043402273765913546\n",
      "step: 24, hamming: 0.037595421359931186\n",
      "step: 25, hamming: 0.03248502656477798\n",
      "step: 26, hamming: 0.027957229203086794\n",
      "step: 27, hamming: 0.023937892823722058\n",
      "step: 28, hamming: 0.02038038336070406\n",
      "step: 29, hamming: 0.017252529167439\n",
      "step: 30, hamming: 0.014526424760499897\n",
      "step: 31, hamming: 0.012172676375081907\n",
      "step: 32, hamming: 0.010158597046433317\n",
      "step: 33, hamming: 0.008448896060811562\n",
      "step: 34, hamming: 0.007007393568975932\n",
      "step: 35, hamming: 0.005798785453088683\n",
      "step: 36, hamming: 0.004789993873907058\n",
      "step: 37, hamming: 0.003950993159323438\n",
      "step: 38, hamming: 0.003255178560133814\n",
      "step: 39, hamming: 0.0026794020858112956\n",
      "step: 40, hamming: 0.0022037972259520604\n",
      "step: 41, hamming: 0.001811488397401407\n",
      "step: 42, hamming: 0.001488251462841637\n",
      "step: 43, hamming: 0.0012221673419373616\n",
      "step: 44, hamming: 0.00100329303825463\n",
      "step: 45, hamming: 0.0008233624337668047\n",
      "Running panda took: 1.25 seconds!\n",
      "  Elapsed time: 1.25 sec.\n",
      "Saving LIONESS network 25 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 26:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.645496705325531\n",
      "step: 1, hamming: 0.6066220697004651\n",
      "step: 2, hamming: 0.6104569962998728\n",
      "step: 3, hamming: 0.5874394872323974\n",
      "step: 4, hamming: 0.553510740033988\n",
      "step: 5, hamming: 0.5130174988541942\n",
      "step: 6, hamming: 0.46881371156127255\n",
      "step: 7, hamming: 0.423183189727278\n",
      "step: 8, hamming: 0.3779417340382216\n",
      "step: 9, hamming: 0.33445240537930465\n",
      "step: 10, hamming: 0.29366433099533695\n",
      "step: 11, hamming: 0.25616701474636144\n",
      "step: 12, hamming: 0.22226007042599125\n",
      "step: 13, hamming: 0.19202153205178443\n",
      "step: 14, hamming: 0.1653704289664206\n",
      "step: 15, hamming: 0.1421176990914315\n",
      "step: 16, hamming: 0.12200588715609274\n",
      "step: 17, hamming: 0.10473835923993631\n",
      "step: 18, hamming: 0.08999989292935066\n",
      "step: 19, hamming: 0.07747064160809623\n",
      "step: 20, hamming: 0.0668358086697313\n",
      "step: 21, hamming: 0.05779392045356974\n",
      "step: 22, hamming: 0.050065303344148486\n",
      "step: 23, hamming: 0.04340157748923955\n",
      "step: 24, hamming: 0.03759501485574574\n",
      "step: 25, hamming: 0.03248483073655701\n",
      "step: 26, hamming: 0.027957178391357862\n",
      "step: 27, hamming: 0.023937933402786545\n",
      "step: 28, hamming: 0.020380473632123763\n",
      "step: 29, hamming: 0.01725263894437879\n",
      "step: 30, hamming: 0.01452653691101459\n",
      "step: 31, hamming: 0.012172781802429688\n",
      "step: 32, hamming: 0.010158691621547705\n",
      "step: 33, hamming: 0.008448978359003054\n",
      "step: 34, hamming: 0.007007463803651778\n",
      "step: 35, hamming: 0.005798844650285862\n",
      "step: 36, hamming: 0.004790043434414073\n",
      "step: 37, hamming: 0.003951034364586717\n",
      "step: 38, hamming: 0.003255212664761859\n",
      "step: 39, hamming: 0.0026794302159438544\n",
      "step: 40, hamming: 0.0022038203680394266\n",
      "step: 41, hamming: 0.0018115074140994405\n",
      "step: 42, hamming: 0.0014882670710365026\n",
      "step: 43, hamming: 0.0012221801429058432\n",
      "step: 44, hamming: 0.0010033035277769993\n",
      "step: 45, hamming: 0.0008233710253226138\n",
      "Running panda took: 1.25 seconds!\n",
      "  Elapsed time: 1.25 sec.\n",
      "Saving LIONESS network 26 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 27:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6453631316746891\n",
      "step: 1, hamming: 0.6065851763245884\n",
      "step: 2, hamming: 0.6104687108084998\n",
      "step: 3, hamming: 0.5874708414567841\n",
      "step: 4, hamming: 0.553550407750318\n",
      "step: 5, hamming: 0.5130590911886108\n",
      "step: 6, hamming: 0.46885507018779043\n",
      "step: 7, hamming: 0.42322248970490667\n",
      "step: 8, hamming: 0.37797766912185493\n",
      "step: 9, hamming: 0.3344846767623718\n",
      "step: 10, hamming: 0.2936929239245133\n",
      "step: 11, hamming: 0.2561922957060964\n",
      "step: 12, hamming: 0.22228219573712402\n",
      "step: 13, hamming: 0.1920407638069488\n",
      "step: 14, hamming: 0.1653870519561906\n",
      "step: 15, hamming: 0.14213208646367623\n",
      "step: 16, hamming: 0.1220182928027473\n",
      "step: 17, hamming: 0.10474902300437654\n",
      "step: 18, hamming: 0.09000903326934917\n",
      "step: 19, hamming: 0.07747844392157899\n",
      "step: 20, hamming: 0.06684245770007372\n",
      "step: 21, hamming: 0.05779957445832755\n",
      "step: 22, hamming: 0.05007009118596108\n",
      "step: 23, hamming: 0.04340563042618257\n",
      "step: 24, hamming: 0.03759844243481214\n",
      "step: 25, hamming: 0.03248772255656214\n",
      "step: 26, hamming: 0.02795960379491282\n",
      "step: 27, hamming: 0.023939953980151828\n",
      "step: 28, hamming: 0.020382151479531884\n",
      "step: 29, hamming: 0.01725402442350402\n",
      "step: 30, hamming: 0.014527676045144938\n",
      "step: 31, hamming: 0.012173715629341523\n",
      "step: 32, hamming: 0.01015945637253753\n",
      "step: 33, hamming: 0.008449604489718607\n",
      "step: 34, hamming: 0.007007976542379953\n",
      "step: 35, hamming: 0.005799264728544191\n",
      "step: 36, hamming: 0.004790387758916936\n",
      "step: 37, hamming: 0.003951316728991022\n",
      "step: 38, hamming: 0.0032554443063554026\n",
      "step: 39, hamming: 0.0026796202983168298\n",
      "step: 40, hamming: 0.002203976375195704\n",
      "step: 41, hamming: 0.0018116354595428003\n",
      "step: 42, hamming: 0.0014883721681424797\n",
      "step: 43, hamming: 0.0012222663986386558\n",
      "step: 44, hamming: 0.0010033743151023368\n",
      "step: 45, hamming: 0.0008234291107443008\n",
      "Running panda took: 1.25 seconds!\n",
      "  Elapsed time: 1.25 sec.\n",
      "Saving LIONESS network 27 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 28:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6451358310824151\n",
      "step: 1, hamming: 0.6067202812268652\n",
      "step: 2, hamming: 0.6105084509106121\n",
      "step: 3, hamming: 0.5874770766110783\n",
      "step: 4, hamming: 0.5535401297746914\n",
      "step: 5, hamming: 0.5130406798275531\n",
      "step: 6, hamming: 0.46883450589918707\n",
      "step: 7, hamming: 0.42320295085821286\n",
      "step: 8, hamming: 0.37796147500536276\n",
      "step: 9, hamming: 0.33447178250965853\n",
      "step: 10, hamming: 0.2936826915890226\n",
      "step: 11, hamming: 0.25618414233804776\n",
      "step: 12, hamming: 0.22227561563567474\n",
      "step: 13, hamming: 0.19203539669004427\n",
      "step: 14, hamming: 0.16538265059792898\n",
      "step: 15, hamming: 0.14212839414046055\n",
      "step: 16, hamming: 0.12201518520640053\n",
      "step: 17, hamming: 0.10474641185408665\n",
      "step: 18, hamming: 0.09000687786719422\n",
      "step: 19, hamming: 0.07747669705672466\n",
      "step: 20, hamming: 0.0668410413623714\n",
      "step: 21, hamming: 0.057798435063911136\n",
      "step: 22, hamming: 0.050069179429029616\n",
      "step: 23, hamming: 0.04340488739282702\n",
      "step: 24, hamming: 0.0375978199151767\n",
      "step: 25, hamming: 0.03248719460754284\n",
      "step: 26, hamming: 0.027959154377771425\n",
      "step: 27, hamming: 0.023939574661234145\n",
      "step: 28, hamming: 0.020381831962738853\n",
      "step: 29, hamming: 0.0172537603782067\n",
      "step: 30, hamming: 0.014527461133636087\n",
      "step: 31, hamming: 0.012173542661750417\n",
      "step: 32, hamming: 0.010159317402521096\n",
      "step: 33, hamming: 0.008449492965825199\n",
      "step: 34, hamming: 0.007007887028456989\n",
      "step: 35, hamming: 0.0057991926884953865\n",
      "step: 36, hamming: 0.004790329522725734\n",
      "step: 37, hamming: 0.0039512694941902776\n",
      "step: 38, hamming: 0.0032554058740101076\n",
      "step: 39, hamming: 0.0026795889452937774\n",
      "step: 40, hamming: 0.002203950746453345\n",
      "step: 41, hamming: 0.0018116144780692218\n",
      "step: 42, hamming: 0.001488354974026338\n",
      "step: 43, hamming: 0.0012222522977348139\n",
      "step: 44, hamming: 0.00100336274382203\n",
      "step: 45, hamming: 0.0008234196121337712\n",
      "Running panda took: 1.25 seconds!\n",
      "  Elapsed time: 1.25 sec.\n",
      "Saving LIONESS network 28 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 29:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6455351008769453\n",
      "step: 1, hamming: 0.6066727499218522\n",
      "step: 2, hamming: 0.610483504192098\n",
      "step: 3, hamming: 0.5874613800094756\n",
      "step: 4, hamming: 0.5535261027676674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5, hamming: 0.5130277394158711\n",
      "step: 6, hamming: 0.4688217647975806\n",
      "step: 7, hamming: 0.42319024909743624\n",
      "step: 8, hamming: 0.3779481583579115\n",
      "step: 9, hamming: 0.33445799331873677\n",
      "step: 10, hamming: 0.29366902135579254\n",
      "step: 11, hamming: 0.25617054999127764\n",
      "step: 12, hamming: 0.22226262032373942\n",
      "step: 13, hamming: 0.19202326345396328\n",
      "step: 14, hamming: 0.16537148632869753\n",
      "step: 15, hamming: 0.142118264201008\n",
      "step: 16, hamming: 0.12200609786553841\n",
      "step: 17, hamming: 0.10473832871874272\n",
      "step: 18, hamming: 0.08999971271249907\n",
      "step: 19, hamming: 0.07747039064766185\n",
      "step: 20, hamming: 0.06683555388538216\n",
      "step: 21, hamming: 0.057793686110969834\n",
      "step: 22, hamming: 0.050065094270375594\n",
      "step: 23, hamming: 0.04340140844053629\n",
      "step: 24, hamming: 0.03759489383654498\n",
      "step: 25, hamming: 0.032484760257860644\n",
      "step: 26, hamming: 0.027957153083011842\n",
      "step: 27, hamming: 0.02393794536063863\n",
      "step: 28, hamming: 0.020380515556205\n",
      "step: 29, hamming: 0.01725270067904533\n",
      "step: 30, hamming: 0.014526608601153238\n",
      "step: 31, hamming: 0.012172855437592785\n",
      "step: 32, hamming: 0.010158762007530757\n",
      "step: 33, hamming: 0.008449042800011774\n",
      "step: 34, hamming: 0.0070075209839526925\n",
      "step: 35, hamming: 0.0057988942286212805\n",
      "step: 36, hamming: 0.00479008565892259\n",
      "step: 37, hamming: 0.003951069946375388\n",
      "step: 38, hamming: 0.0032552424077837267\n",
      "step: 39, hamming: 0.002679454936658191\n",
      "step: 40, hamming: 0.0022038408329425113\n",
      "step: 41, hamming: 0.0018115242912096806\n",
      "step: 42, hamming: 0.0014882809567832752\n",
      "step: 43, hamming: 0.0012221915474429246\n",
      "step: 44, hamming: 0.0010033128856263525\n",
      "step: 45, hamming: 0.0008233786956426908\n",
      "Running panda took: 1.25 seconds!\n",
      "  Elapsed time: 1.25 sec.\n",
      "Saving LIONESS network 29 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 30:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6441925069818978\n",
      "step: 1, hamming: 0.6063136112432319\n",
      "step: 2, hamming: 0.6103932208346182\n",
      "step: 3, hamming: 0.5874474722143483\n",
      "step: 4, hamming: 0.5535649464335758\n",
      "step: 5, hamming: 0.5130928337695616\n",
      "step: 6, hamming: 0.4688939095266525\n",
      "step: 7, hamming: 0.4232588527650599\n",
      "step: 8, hamming: 0.3780087198271526\n",
      "step: 9, hamming: 0.3345100558700422\n",
      "step: 10, hamming: 0.2937142747732435\n",
      "step: 11, hamming: 0.2562107598757153\n",
      "step: 12, hamming: 0.22229857181764945\n",
      "step: 13, hamming: 0.19205531145425267\n",
      "step: 14, hamming: 0.16539978011088835\n",
      "step: 15, hamming: 0.14214287312471557\n",
      "step: 16, hamming: 0.12202708424929\n",
      "step: 17, hamming: 0.10475587118837185\n",
      "step: 18, hamming: 0.09001400820385419\n",
      "step: 19, hamming: 0.07748166638509847\n",
      "step: 20, hamming: 0.06684413555730546\n",
      "step: 21, hamming: 0.05779998950120316\n",
      "step: 22, hamming: 0.050069556133776956\n",
      "step: 23, hamming: 0.04340444199041317\n",
      "step: 24, hamming: 0.03759688450174653\n",
      "step: 25, hamming: 0.03248603443830073\n",
      "step: 26, hamming: 0.02795796675997988\n",
      "step: 27, hamming: 0.023938483652704987\n",
      "step: 28, hamming: 0.0203808962291558\n",
      "step: 29, hamming: 0.017252993901731097\n",
      "step: 30, hamming: 0.01452685247958995\n",
      "step: 31, hamming: 0.012173067882296\n",
      "step: 32, hamming: 0.010158950869745383\n",
      "step: 33, hamming: 0.008449210882153616\n",
      "step: 34, hamming: 0.007007669688350226\n",
      "step: 35, hamming: 0.005799024608140825\n",
      "step: 36, hamming: 0.0047901987375082965\n",
      "step: 37, hamming: 0.003951167141327139\n",
      "step: 38, hamming: 0.003255325350745027\n",
      "step: 39, hamming: 0.0026795253550045647\n",
      "step: 40, hamming: 0.0022039003201826408\n",
      "step: 41, hamming: 0.0018115743353996415\n",
      "step: 42, hamming: 0.0014883229143649365\n",
      "step: 43, hamming: 0.001222226622914164\n",
      "step: 44, hamming: 0.0010033421337824078\n",
      "step: 45, hamming: 0.0008234030345286966\n",
      "Running panda took: 1.31 seconds!\n",
      "  Elapsed time: 1.31 sec.\n",
      "Saving LIONESS network 30 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 31:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6453374894828802\n",
      "step: 1, hamming: 0.6066967069145408\n",
      "step: 2, hamming: 0.6104840389078137\n",
      "step: 3, hamming: 0.5874565733434927\n",
      "step: 4, hamming: 0.5535188219795216\n",
      "step: 5, hamming: 0.5130204873787859\n",
      "step: 6, hamming: 0.4688150596423362\n",
      "step: 7, hamming: 0.4231847518707811\n",
      "step: 8, hamming: 0.37794456875278243\n",
      "step: 9, hamming: 0.33445666843682814\n",
      "step: 10, hamming: 0.29366950408238246\n",
      "step: 11, hamming: 0.25617272617649023\n",
      "step: 12, hamming: 0.22226581478746255\n",
      "step: 13, hamming: 0.1920270800407892\n",
      "step: 14, hamming: 0.1653756413311506\n",
      "step: 15, hamming: 0.14212251274972568\n",
      "step: 16, hamming: 0.12201028566869025\n",
      "step: 17, hamming: 0.10474235378727433\n",
      "step: 18, hamming: 0.09000359838145931\n",
      "step: 19, hamming: 0.07747406299496648\n",
      "step: 20, hamming: 0.0668389202442914\n",
      "step: 21, hamming: 0.05779672357526197\n",
      "step: 22, hamming: 0.05006780392032585\n",
      "step: 23, hamming: 0.043403791059584765\n",
      "step: 24, hamming: 0.03759695373852516\n",
      "step: 25, hamming: 0.032486517555771505\n",
      "step: 26, hamming: 0.027958627760509558\n",
      "step: 27, hamming: 0.023939165045318303\n",
      "step: 28, hamming: 0.02038151237433263\n",
      "step: 29, hamming: 0.017253509321226976\n",
      "step: 30, hamming: 0.01452726248992398\n",
      "step: 31, hamming: 0.01217338409078206\n",
      "step: 32, hamming: 0.01015918998360408\n",
      "step: 33, hamming: 0.008449389939631926\n",
      "step: 34, hamming: 0.007007803396138961\n",
      "step: 35, hamming: 0.0057991246636717375\n",
      "step: 36, hamming: 0.004790274070575602\n",
      "step: 37, hamming: 0.003951224259266886\n",
      "step: 38, hamming: 0.0032553690960775237\n",
      "step: 39, hamming: 0.002679559015472811\n",
      "step: 40, hamming: 0.002203926367689019\n",
      "step: 41, hamming: 0.0018115946043863224\n",
      "step: 42, hamming: 0.0014883387657010924\n",
      "step: 43, hamming: 0.0012222390734737913\n",
      "step: 44, hamming: 0.0010033519517067173\n",
      "step: 45, hamming: 0.000823410802845808\n",
      "Running panda took: 1.40 seconds!\n",
      "  Elapsed time: 1.41 sec.\n",
      "Saving LIONESS network 31 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 32:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6449746310280962\n",
      "step: 1, hamming: 0.6067058332082043\n",
      "step: 2, hamming: 0.6105227585080633\n",
      "step: 3, hamming: 0.5874998685226731\n",
      "step: 4, hamming: 0.5535644118765826\n",
      "step: 5, hamming: 0.5130637409325514\n",
      "step: 6, hamming: 0.468855207197495\n",
      "step: 7, hamming: 0.4232205988814208\n",
      "step: 8, hamming: 0.37797509767336696\n",
      "step: 9, hamming: 0.33448191453778925\n",
      "step: 10, hamming: 0.2936901232265988\n",
      "step: 11, hamming: 0.2561893953353568\n",
      "step: 12, hamming: 0.2222794648610085\n",
      "step: 13, hamming: 0.19203833582770796\n",
      "step: 14, hamming: 0.16538499865483383\n",
      "step: 15, hamming: 0.14213033937025307\n",
      "step: 16, hamming: 0.1220168447185877\n",
      "step: 17, hamming: 0.10474787876788032\n",
      "step: 18, hamming: 0.09000825020978295\n",
      "step: 19, hamming: 0.0774780088393839\n",
      "step: 20, hamming: 0.06684228198432349\n",
      "step: 21, hamming: 0.057799594299135344\n",
      "step: 22, hamming: 0.05007025371136523\n",
      "step: 23, hamming: 0.04340587382574411\n",
      "step: 24, hamming: 0.03759871933174081\n",
      "step: 25, hamming: 0.03248800162100365\n",
      "step: 26, hamming: 0.027959871764145786\n",
      "step: 27, hamming: 0.023940202435957\n",
      "step: 28, hamming: 0.020382372773605797\n",
      "step: 29, hamming: 0.017254220926788584\n",
      "step: 30, hamming: 0.014527849564486101\n",
      "step: 31, hamming: 0.01217386793079806\n",
      "step: 32, hamming: 0.010159588465741654\n",
      "step: 33, hamming: 0.008449718062226337\n",
      "step: 34, hamming: 0.007008073488535545\n",
      "step: 35, hamming: 0.005799346994199014\n",
      "step: 36, hamming: 0.004790456976200905\n",
      "step: 37, hamming: 0.003951374637817457\n",
      "step: 38, hamming: 0.0032554925944706567\n",
      "step: 39, hamming: 0.0026796605246515583\n",
      "step: 40, hamming: 0.002204009784151203\n",
      "step: 41, hamming: 0.0018116631348835946\n",
      "step: 42, hamming: 0.001488395046021938\n",
      "step: 43, hamming: 0.0012222852787877193\n",
      "step: 44, hamming: 0.0010033898748033525\n",
      "step: 45, hamming: 0.0008234419197268575\n",
      "Running panda took: 1.40 seconds!\n",
      "  Elapsed time: 1.40 sec.\n",
      "Saving LIONESS network 32 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 33:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6456567808462211\n",
      "step: 1, hamming: 0.6066002678232714\n",
      "step: 2, hamming: 0.6105813100451256\n",
      "step: 3, hamming: 0.5876366440945432\n",
      "step: 4, hamming: 0.5537384939042065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5, hamming: 0.5132510551234207\n",
      "step: 6, hamming: 0.4690351665376202\n",
      "step: 7, hamming: 0.42338117383780194\n",
      "step: 8, hamming: 0.3781135868865882\n",
      "step: 9, hamming: 0.33459944752452325\n",
      "step: 10, hamming: 0.2937892636785368\n",
      "step: 11, hamming: 0.25627321887464255\n",
      "step: 12, hamming: 0.22235047832920657\n",
      "step: 13, hamming: 0.1920987613785881\n",
      "step: 14, hamming: 0.16543669817665527\n",
      "step: 15, hamming: 0.14217460617494096\n",
      "step: 16, hamming: 0.12205478208053701\n",
      "step: 17, hamming: 0.10478042130530228\n",
      "step: 18, hamming: 0.09003612775603832\n",
      "step: 19, hamming: 0.07750184049531986\n",
      "step: 20, hamming: 0.06686269734372635\n",
      "step: 21, hamming: 0.05781712126932585\n",
      "step: 22, hamming: 0.050085333681801514\n",
      "step: 23, hamming: 0.0434188542719053\n",
      "step: 24, hamming: 0.0376098970862519\n",
      "step: 25, hamming: 0.0324976193016997\n",
      "step: 26, hamming: 0.027968121638909667\n",
      "step: 27, hamming: 0.023947253186378716\n",
      "step: 28, hamming: 0.02038837093130006\n",
      "step: 29, hamming: 0.01725930031609945\n",
      "step: 30, hamming: 0.01453213057480876\n",
      "step: 31, hamming: 0.012177459602951914\n",
      "step: 32, hamming: 0.010162590210066961\n",
      "step: 33, hamming: 0.008452218615141236\n",
      "step: 34, hamming: 0.007010150880511295\n",
      "step: 35, hamming: 0.005801068940490524\n",
      "step: 36, hamming: 0.004791881667170449\n",
      "step: 37, hamming: 0.003952551627023815\n",
      "step: 38, hamming: 0.003256463710290392\n",
      "step: 39, hamming: 0.0026804608719384142\n",
      "step: 40, hamming: 0.0022046688418683176\n",
      "step: 41, hamming: 0.0018122054772229583\n",
      "step: 42, hamming: 0.0014888410848218332\n",
      "step: 43, hamming: 0.0012226519307712378\n",
      "step: 44, hamming: 0.0010036911397340632\n",
      "step: 45, hamming: 0.0008236893647987591\n",
      "Running panda took: 1.71 seconds!\n",
      "  Elapsed time: 1.71 sec.\n",
      "Saving LIONESS network 33 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 34:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6449981971449824\n",
      "step: 1, hamming: 0.6065931831332112\n",
      "step: 2, hamming: 0.6105293103087643\n",
      "step: 3, hamming: 0.5875457591281374\n",
      "step: 4, hamming: 0.5536260899397788\n",
      "step: 5, hamming: 0.5131280856850265\n",
      "step: 6, hamming: 0.4689154027720692\n",
      "step: 7, hamming: 0.4232740123691572\n",
      "step: 8, hamming: 0.37802087584233957\n",
      "step: 9, hamming: 0.33452104855520154\n",
      "step: 10, hamming: 0.29372314994988774\n",
      "step: 11, hamming: 0.25621717754897094\n",
      "step: 12, hamming: 0.2223028366559903\n",
      "step: 13, hamming: 0.19205800982166413\n",
      "step: 14, hamming: 0.16540159888503903\n",
      "step: 15, hamming: 0.14214436276067294\n",
      "step: 16, hamming: 0.12202874055933112\n",
      "step: 17, hamming: 0.10475804762546262\n",
      "step: 18, hamming: 0.0900169464745131\n",
      "step: 19, hamming: 0.07748544549209825\n",
      "step: 20, hamming: 0.06684869516951135\n",
      "step: 21, hamming: 0.05780513650530213\n",
      "step: 22, hamming: 0.05007505612580251\n",
      "step: 23, hamming: 0.04341002702171404\n",
      "step: 24, hamming: 0.03760230583906534\n",
      "step: 25, hamming: 0.03249108895337307\n",
      "step: 26, hamming: 0.027962521329952538\n",
      "step: 27, hamming: 0.023942471148388367\n",
      "step: 28, hamming: 0.020384308356420206\n",
      "step: 29, hamming: 0.01725586604933187\n",
      "step: 30, hamming: 0.0145292430032475\n",
      "step: 31, hamming: 0.012175043490551075\n",
      "step: 32, hamming: 0.010160576904046072\n",
      "step: 33, hamming: 0.008450545793374795\n",
      "step: 34, hamming: 0.007008764233183781\n",
      "step: 35, hamming: 0.005799922014886088\n",
      "step: 36, hamming: 0.0047909345756681\n",
      "step: 37, hamming: 0.0039517704601507225\n",
      "step: 38, hamming: 0.0032558200052223347\n",
      "step: 39, hamming: 0.002679930861517707\n",
      "step: 40, hamming: 0.0022042327414596506\n",
      "step: 41, hamming: 0.001811846845087572\n",
      "step: 42, hamming: 0.0014885463019372123\n",
      "step: 43, hamming: 0.0012224097309378736\n",
      "step: 44, hamming: 0.0010034922163639033\n",
      "step: 45, hamming: 0.000823526038024523\n",
      "Running panda took: 1.43 seconds!\n",
      "  Elapsed time: 1.43 sec.\n",
      "Saving LIONESS network 34 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 35:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6426000250494666\n",
      "step: 1, hamming: 0.6063071921865067\n",
      "step: 2, hamming: 0.6104809784525866\n",
      "step: 3, hamming: 0.5875787789957999\n",
      "step: 4, hamming: 0.5536924206915095\n",
      "step: 5, hamming: 0.5132239248844174\n",
      "step: 6, hamming: 0.4690279451515534\n",
      "step: 7, hamming: 0.4234005441297799\n",
      "step: 8, hamming: 0.37815866164971773\n",
      "step: 9, hamming: 0.33466411068445123\n",
      "step: 10, hamming: 0.2938652232754584\n",
      "step: 11, hamming: 0.25635291398589555\n",
      "step: 12, hamming: 0.2224278035307866\n",
      "step: 13, hamming: 0.1921699165297888\n",
      "step: 14, hamming: 0.16549975125668104\n",
      "step: 15, hamming: 0.14222892834479822\n",
      "step: 16, hamming: 0.12210051057291255\n",
      "step: 17, hamming: 0.1048181600149727\n",
      "step: 18, hamming: 0.09006670326722839\n",
      "step: 19, hamming: 0.07752617005100722\n",
      "step: 20, hamming: 0.06688172438724069\n",
      "step: 21, hamming: 0.05783176894755019\n",
      "step: 22, hamming: 0.05009647661533095\n",
      "step: 23, hamming: 0.04342729230869477\n",
      "step: 24, hamming: 0.037616314867588554\n",
      "step: 25, hamming: 0.032502556849793765\n",
      "step: 26, hamming: 0.027971999052752584\n",
      "step: 27, hamming: 0.02395035939389856\n",
      "step: 28, hamming: 0.02039090053720676\n",
      "step: 29, hamming: 0.017261384985951662\n",
      "step: 30, hamming: 0.01453386083972818\n",
      "step: 31, hamming: 0.01217890040608688\n",
      "step: 32, hamming: 0.010163791137853357\n",
      "step: 33, hamming: 0.008453219201499561\n",
      "step: 34, hamming: 0.007010983652762794\n",
      "step: 35, hamming: 0.0058017611584794\n",
      "step: 36, hamming: 0.004792456415076777\n",
      "step: 37, hamming: 0.003953028346447056\n",
      "step: 38, hamming: 0.0032568587382547803\n",
      "step: 39, hamming: 0.002680787897775716\n",
      "step: 40, hamming: 0.0022049393777273303\n",
      "step: 41, hamming: 0.0018124290872041876\n",
      "step: 42, hamming: 0.0014890257649965667\n",
      "step: 43, hamming: 0.0012228043489239595\n",
      "step: 44, hamming: 0.0010038168464440735\n",
      "step: 45, hamming: 0.000823792977083675\n",
      "Running panda took: 1.31 seconds!\n",
      "  Elapsed time: 1.31 sec.\n",
      "Saving LIONESS network 35 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 36:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6453822083232199\n",
      "step: 1, hamming: 0.6066969273795092\n",
      "step: 2, hamming: 0.6105383132265271\n",
      "step: 3, hamming: 0.5875246652799395\n",
      "step: 4, hamming: 0.5535923400428431\n",
      "step: 5, hamming: 0.5130969212335402\n",
      "step: 6, hamming: 0.4688916302793686\n",
      "step: 7, hamming: 0.4232591493736284\n",
      "step: 8, hamming: 0.3780152221141369\n",
      "step: 9, hamming: 0.33452209514331444\n",
      "step: 10, hamming: 0.2937286374246192\n",
      "step: 11, hamming: 0.25622520736811455\n",
      "step: 12, hamming: 0.22231178357995593\n",
      "step: 13, hamming: 0.19206706531225234\n",
      "step: 14, hamming: 0.16541028792801\n",
      "step: 15, hamming: 0.14215237669006922\n",
      "step: 16, hamming: 0.1220358856668059\n",
      "step: 17, hamming: 0.10476421069641416\n",
      "step: 18, hamming: 0.09002215316977837\n",
      "step: 19, hamming: 0.07748976024988688\n",
      "step: 20, hamming: 0.06685218168838662\n",
      "step: 21, hamming: 0.05780791039379919\n",
      "step: 22, hamming: 0.05007722152860851\n",
      "step: 23, hamming: 0.0434117205050181\n",
      "step: 24, hamming: 0.037603623119995024\n",
      "step: 25, hamming: 0.0324921143107474\n",
      "step: 26, hamming: 0.027963316385545108\n",
      "step: 27, hamming: 0.02394308925055814\n",
      "step: 28, hamming: 0.020384791637139875\n",
      "step: 29, hamming: 0.017256245588859704\n",
      "step: 30, hamming: 0.014529542455540407\n",
      "step: 31, hamming: 0.012175281230148287\n",
      "step: 32, hamming: 0.010160766715614725\n",
      "step: 33, hamming: 0.008450698522426142\n",
      "step: 34, hamming: 0.00700888797478153\n",
      "step: 35, hamming: 0.005800022398900317\n",
      "step: 36, hamming: 0.004791016237650569\n",
      "step: 37, hamming: 0.003951837291752229\n",
      "step: 38, hamming: 0.0032558748913572885\n",
      "step: 39, hamming: 0.0026799760260441618\n",
      "step: 40, hamming: 0.0022042699447564146\n",
      "step: 41, hamming: 0.0018118775067475117\n",
      "step: 42, hamming: 0.0014885715795620953\n",
      "step: 43, hamming: 0.0012224305718871052\n",
      "step: 44, hamming: 0.001003509395952679\n",
      "step: 45, hamming: 0.0008235401960124114\n",
      "Running panda took: 1.28 seconds!\n",
      "  Elapsed time: 1.28 sec.\n",
      "Saving LIONESS network 36 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 37:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6450937603753659\n",
      "step: 1, hamming: 0.6064436660837452\n",
      "step: 2, hamming: 0.6105412861118334\n",
      "step: 3, hamming: 0.5876066009972607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4, hamming: 0.5537181506446464\n",
      "step: 5, hamming: 0.5132401622393848\n",
      "step: 6, hamming: 0.46903005243019497\n",
      "step: 7, hamming: 0.4233816338164551\n",
      "step: 8, hamming: 0.37811730216131945\n",
      "step: 9, hamming: 0.3346042764085535\n",
      "step: 10, hamming: 0.29379370959045664\n",
      "step: 11, hamming: 0.2562765055577094\n",
      "step: 12, hamming: 0.22235228297194926\n",
      "step: 13, hamming: 0.1920990888414815\n",
      "step: 14, hamming: 0.16543565555984818\n",
      "step: 15, hamming: 0.14217249050227085\n",
      "step: 16, hamming: 0.12205187338509321\n",
      "step: 17, hamming: 0.10477697957160514\n",
      "step: 18, hamming: 0.09003236276240752\n",
      "step: 19, hamming: 0.07749793593084385\n",
      "step: 20, hamming: 0.06685876647177448\n",
      "step: 21, hamming: 0.057813243887915165\n",
      "step: 22, hamming: 0.0500815676624227\n",
      "step: 23, hamming: 0.04341524699674484\n",
      "step: 24, hamming: 0.03760647449990497\n",
      "step: 25, hamming: 0.032494402723185246\n",
      "step: 26, hamming: 0.027965137026658638\n",
      "step: 27, hamming: 0.023944516079917964\n",
      "step: 28, hamming: 0.020385898620171303\n",
      "step: 29, hamming: 0.01725709609099151\n",
      "step: 30, hamming: 0.014530191634674654\n",
      "step: 31, hamming: 0.012175775564193872\n",
      "step: 32, hamming: 0.010161143736087763\n",
      "step: 33, hamming: 0.008450987745447076\n",
      "step: 34, hamming: 0.007009111445933516\n",
      "step: 35, hamming: 0.005800196669853901\n",
      "step: 36, hamming: 0.004791153274172694\n",
      "step: 37, hamming: 0.003951945757724805\n",
      "step: 38, hamming: 0.0032559612971043766\n",
      "step: 39, hamming: 0.0026800452215014316\n",
      "step: 40, hamming: 0.002204325608741197\n",
      "step: 41, hamming: 0.001811922446478191\n",
      "step: 42, hamming: 0.0014886079619687142\n",
      "step: 43, hamming: 0.0012224600919155358\n",
      "step: 44, hamming: 0.0010035333901201532\n",
      "step: 45, hamming: 0.0008235597257708719\n",
      "Running panda took: 1.30 seconds!\n",
      "  Elapsed time: 1.30 sec.\n",
      "Saving LIONESS network 37 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 38:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6440630972479126\n",
      "step: 1, hamming: 0.6065362164769552\n",
      "step: 2, hamming: 0.6104532026394006\n",
      "step: 3, hamming: 0.5874604045897801\n",
      "step: 4, hamming: 0.5535439576972261\n",
      "step: 5, hamming: 0.5130532048778697\n",
      "step: 6, hamming: 0.46884617663248207\n",
      "step: 7, hamming: 0.4232106291765286\n",
      "step: 8, hamming: 0.37796351385560745\n",
      "step: 9, hamming: 0.3344694203478318\n",
      "step: 10, hamming: 0.29367784651746603\n",
      "step: 11, hamming: 0.2561779100594673\n",
      "step: 12, hamming: 0.2222690261377353\n",
      "step: 13, hamming: 0.19202898759027912\n",
      "step: 14, hamming: 0.16537677619988345\n",
      "step: 15, hamming: 0.1421232526413549\n",
      "step: 16, hamming: 0.12201074582554952\n",
      "step: 17, hamming: 0.10474263477818666\n",
      "step: 18, hamming: 0.09000370065023104\n",
      "step: 19, hamming: 0.0774740215271501\n",
      "step: 20, hamming: 0.06683879577708175\n",
      "step: 21, hamming: 0.0577965551811048\n",
      "step: 22, hamming: 0.050067607385594834\n",
      "step: 23, hamming: 0.043403573496942294\n",
      "step: 24, hamming: 0.03759672349751944\n",
      "step: 25, hamming: 0.03248628020522619\n",
      "step: 26, hamming: 0.02795839210769396\n",
      "step: 27, hamming: 0.023938938938551046\n",
      "step: 28, hamming: 0.020381302269394757\n",
      "step: 29, hamming: 0.017253319330902395\n",
      "step: 30, hamming: 0.014527095084336911\n",
      "step: 31, hamming: 0.012173239529471585\n",
      "step: 32, hamming: 0.010159067354651509\n",
      "step: 33, hamming: 0.008449287001086301\n",
      "step: 34, hamming: 0.007007717820632076\n",
      "step: 35, hamming: 0.0057990536240283075\n",
      "step: 36, hamming: 0.004790215055454623\n",
      "step: 37, hamming: 0.003951175336782722\n",
      "step: 38, hamming: 0.0032553284588682893\n",
      "step: 39, hamming: 0.002679525308582265\n",
      "step: 40, hamming: 0.002203898423392684\n",
      "step: 41, hamming: 0.0018115714471615195\n",
      "step: 42, hamming: 0.0014883195860679456\n",
      "step: 43, hamming: 0.0012222231985379258\n",
      "step: 44, hamming: 0.0010033388191364977\n",
      "step: 45, hamming: 0.000823399944870464\n",
      "Running panda took: 1.33 seconds!\n",
      "  Elapsed time: 1.33 sec.\n",
      "Saving LIONESS network 38 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 39:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.645008672822219\n",
      "step: 1, hamming: 0.6066279590964606\n",
      "step: 2, hamming: 0.6105189710157038\n",
      "step: 3, hamming: 0.5875129702246186\n",
      "step: 4, hamming: 0.5535881728916275\n",
      "step: 5, hamming: 0.5130942680692989\n",
      "step: 6, hamming: 0.4688837087451931\n",
      "step: 7, hamming: 0.42324469194157627\n",
      "step: 8, hamming: 0.3779945678456299\n",
      "step: 9, hamming: 0.3344972254062105\n",
      "step: 10, hamming: 0.2937027228697728\n",
      "step: 11, hamming: 0.2561999698599468\n",
      "step: 12, hamming: 0.22228841230441757\n",
      "step: 13, hamming: 0.19204594680519657\n",
      "step: 14, hamming: 0.16539145284192142\n",
      "step: 15, hamming: 0.14213579042349994\n",
      "step: 16, hamming: 0.12202150658243803\n",
      "step: 17, hamming: 0.10475182786542586\n",
      "step: 18, hamming: 0.09001151446194962\n",
      "step: 19, hamming: 0.077480629396115\n",
      "step: 20, hamming: 0.06684439090269251\n",
      "step: 21, hamming: 0.0578012977397978\n",
      "step: 22, hamming: 0.050071632819650706\n",
      "step: 23, hamming: 0.043407005970971885\n",
      "step: 24, hamming: 0.037599660950331404\n",
      "step: 25, hamming: 0.032488803075486614\n",
      "step: 26, hamming: 0.027960560323392912\n",
      "step: 27, hamming: 0.02394080074258424\n",
      "step: 28, hamming: 0.020382896359423752\n",
      "step: 29, hamming: 0.01725467844967507\n",
      "step: 30, hamming: 0.01452824788682867\n",
      "step: 31, hamming: 0.012174211893688454\n",
      "step: 32, hamming: 0.010159883384549461\n",
      "step: 33, hamming: 0.008449969131479787\n",
      "step: 34, hamming: 0.007008285718950417\n",
      "step: 35, hamming: 0.005799525161101374\n",
      "step: 36, hamming: 0.004790605850968231\n",
      "step: 37, hamming: 0.003951498594437267\n",
      "step: 38, hamming: 0.0032555954389956172\n",
      "step: 39, hamming: 0.002679745529938827\n",
      "step: 40, hamming: 0.0022040799086265896\n",
      "step: 41, hamming: 0.0018117209077095193\n",
      "step: 42, hamming: 0.0014884425909900132\n",
      "step: 43, hamming: 0.0012223243766523943\n",
      "step: 44, hamming: 0.0010034220089883334\n",
      "step: 45, hamming: 0.0008234683171750288\n",
      "Running panda took: 1.34 seconds!\n",
      "  Elapsed time: 1.34 sec.\n",
      "Saving LIONESS network 39 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 40:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6446899829950574\n",
      "step: 1, hamming: 0.6065720657385845\n",
      "step: 2, hamming: 0.6104769060458073\n",
      "step: 3, hamming: 0.587468913447344\n",
      "step: 4, hamming: 0.553541697529604\n",
      "step: 5, hamming: 0.5130476369210646\n",
      "step: 6, hamming: 0.4688442859318447\n",
      "step: 7, hamming: 0.4232147915742599\n",
      "step: 8, hamming: 0.3779745725802862\n",
      "step: 9, hamming: 0.33448614616016825\n",
      "step: 10, hamming: 0.29369793769767927\n",
      "step: 11, hamming: 0.25619940627415155\n",
      "step: 12, hamming: 0.22229019835157474\n",
      "step: 13, hamming: 0.19204895567253835\n",
      "step: 14, hamming: 0.16539501582991115\n",
      "step: 15, hamming: 0.1421394748862269\n",
      "step: 16, hamming: 0.12202499625028414\n",
      "step: 17, hamming: 0.10475511557113841\n",
      "step: 18, hamming: 0.09001460630282851\n",
      "step: 19, hamming: 0.07748348660857822\n",
      "step: 20, hamming: 0.06684699483703208\n",
      "step: 21, hamming: 0.05780364640854137\n",
      "step: 22, hamming: 0.05007375265804036\n",
      "step: 23, hamming: 0.043408911909504244\n",
      "step: 24, hamming: 0.03760138090088409\n",
      "step: 25, hamming: 0.03249034534683271\n",
      "step: 26, hamming: 0.027961940282617167\n",
      "step: 27, hamming: 0.02394202780317782\n",
      "step: 28, hamming: 0.020383977524069202\n",
      "step: 29, hamming: 0.017255623043299226\n",
      "step: 30, hamming: 0.014529064889671078\n",
      "step: 31, hamming: 0.012174912034662342\n",
      "step: 32, hamming: 0.01016047845157582\n",
      "step: 33, hamming: 0.008450471314942094\n",
      "step: 34, hamming: 0.00700870709647639\n",
      "step: 35, hamming: 0.005799877499710262\n",
      "step: 36, hamming: 0.004790899569917692\n",
      "step: 37, hamming: 0.003951742693309238\n",
      "step: 38, hamming: 0.0032557978512664306\n",
      "step: 39, hamming: 0.002679913100611094\n",
      "step: 40, hamming: 0.0022042184426658384\n",
      "step: 41, hamming: 0.0018118352905121438\n",
      "step: 42, hamming: 0.001488536939266653\n",
      "step: 43, hamming: 0.0012224021313626224\n",
      "step: 44, hamming: 0.001003486038915215\n",
      "step: 45, hamming: 0.0008235210127976309\n",
      "Running panda took: 1.34 seconds!\n",
      "  Elapsed time: 1.34 sec.\n",
      "Saving LIONESS network 40 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 41:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.643958877072217\n",
      "step: 1, hamming: 0.6066729641072985\n",
      "step: 2, hamming: 0.6105706441429531\n",
      "step: 3, hamming: 0.5875671840941765\n",
      "step: 4, hamming: 0.5536412525576695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5, hamming: 0.5131442161431156\n",
      "step: 6, hamming: 0.46893158026561005\n",
      "step: 7, hamming: 0.42328943886764275\n",
      "step: 8, hamming: 0.3780348238806755\n",
      "step: 9, hamming: 0.3345325891631173\n",
      "step: 10, hamming: 0.2937325823349309\n",
      "step: 11, hamming: 0.2562249355349886\n",
      "step: 12, hamming: 0.22230910059206208\n",
      "step: 13, hamming: 0.1920630661505349\n",
      "step: 14, hamming: 0.16540567166944584\n",
      "step: 15, hamming: 0.142147668817602\n",
      "step: 16, hamming: 0.12203143924750816\n",
      "step: 17, hamming: 0.10476020433355153\n",
      "step: 18, hamming: 0.09001866866683568\n",
      "step: 19, hamming: 0.07748681890195008\n",
      "step: 20, hamming: 0.06684977392071134\n",
      "step: 21, hamming: 0.0578060040584336\n",
      "step: 22, hamming: 0.050075759807870907\n",
      "step: 23, hamming: 0.043410587150283765\n",
      "step: 24, hamming: 0.037602741756048684\n",
      "step: 25, hamming: 0.03249142680963005\n",
      "step: 26, hamming: 0.02796277245679931\n",
      "step: 27, hamming: 0.023942649977909784\n",
      "step: 28, hamming: 0.0203844295732805\n",
      "step: 29, hamming: 0.017255942798246854\n",
      "step: 30, hamming: 0.0145292867393714\n",
      "step: 31, hamming: 0.012175064147083636\n",
      "step: 32, hamming: 0.010160581814379423\n",
      "step: 33, hamming: 0.008450540939725247\n",
      "step: 34, hamming: 0.007008754041534418\n",
      "step: 35, hamming: 0.0057999088696900685\n",
      "step: 36, hamming: 0.004790920217625292\n",
      "step: 37, hamming: 0.003951756159188347\n",
      "step: 38, hamming: 0.003255806511443424\n",
      "step: 39, hamming: 0.0026799185423229273\n",
      "step: 40, hamming: 0.0022042217409228124\n",
      "step: 41, hamming: 0.0018118371780350337\n",
      "step: 42, hamming: 0.0014885379101339323\n",
      "step: 43, hamming: 0.0012224025182755486\n",
      "step: 44, hamming: 0.0010034860661721828\n",
      "step: 45, hamming: 0.0008235208251140591\n",
      "Running panda took: 1.33 seconds!\n",
      "  Elapsed time: 1.33 sec.\n",
      "Saving LIONESS network 41 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 42:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.645090088879035\n",
      "step: 1, hamming: 0.6066516738621497\n",
      "step: 2, hamming: 0.6104985666021202\n",
      "step: 3, hamming: 0.5874788063748753\n",
      "step: 4, hamming: 0.553540130637487\n",
      "step: 5, hamming: 0.5130397114128462\n",
      "step: 6, hamming: 0.46883365127766485\n",
      "step: 7, hamming: 0.42320294317071977\n",
      "step: 8, hamming: 0.3779626905033645\n",
      "step: 9, hamming: 0.33447473860572524\n",
      "step: 10, hamming: 0.29368685975225084\n",
      "step: 11, hamming: 0.2561887985822545\n",
      "step: 12, hamming: 0.22228041328144352\n",
      "step: 13, hamming: 0.1920401220158585\n",
      "step: 14, hamming: 0.16538712296890062\n",
      "step: 15, hamming: 0.14213253273283044\n",
      "step: 16, hamming: 0.12201908520533228\n",
      "step: 17, hamming: 0.10475002765824207\n",
      "step: 18, hamming: 0.09001017467263965\n",
      "step: 19, hamming: 0.07747969955501102\n",
      "step: 20, hamming: 0.0668437502548238\n",
      "step: 21, hamming: 0.05780086764535796\n",
      "step: 22, hamming: 0.05007135445772276\n",
      "step: 23, hamming: 0.04340683431558541\n",
      "step: 24, hamming: 0.0375995690096326\n",
      "step: 25, hamming: 0.032488762864861374\n",
      "step: 26, hamming: 0.027960559636943563\n",
      "step: 27, hamming: 0.02394082711040951\n",
      "step: 28, hamming: 0.02038293768653359\n",
      "step: 29, hamming: 0.01725472869077617\n",
      "step: 30, hamming: 0.014528302543333647\n",
      "step: 31, hamming: 0.012174266467077246\n",
      "step: 32, hamming: 0.010159934842999263\n",
      "step: 33, hamming: 0.008450015826353949\n",
      "step: 34, hamming: 0.007008327003644894\n",
      "step: 35, hamming: 0.005799561059886952\n",
      "step: 36, hamming: 0.004790636812690917\n",
      "step: 37, hamming: 0.00395152498915356\n",
      "step: 38, hamming: 0.0032556177779712583\n",
      "step: 39, hamming: 0.0026797643542427977\n",
      "step: 40, hamming: 0.00220409572282963\n",
      "step: 41, hamming: 0.0018117341418863407\n",
      "step: 42, hamming: 0.0014884536410248793\n",
      "step: 43, hamming: 0.0012223335797172307\n",
      "step: 44, hamming: 0.0010034296580480623\n",
      "step: 45, hamming: 0.0008234746634137464\n",
      "Running panda took: 1.33 seconds!\n",
      "  Elapsed time: 1.33 sec.\n",
      "Saving LIONESS network 42 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 43:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.644052761896358\n",
      "step: 1, hamming: 0.606846793585411\n",
      "step: 2, hamming: 0.610728857719017\n",
      "step: 3, hamming: 0.5877376684115262\n",
      "step: 4, hamming: 0.5538142766265395\n",
      "step: 5, hamming: 0.5133115341098062\n",
      "step: 6, hamming: 0.4690898708650283\n",
      "step: 7, hamming: 0.4234332320104306\n",
      "step: 8, hamming: 0.3781623554018539\n",
      "step: 9, hamming: 0.3346437191689542\n",
      "step: 10, hamming: 0.29382891080082413\n",
      "step: 11, hamming: 0.25630804132509455\n",
      "step: 12, hamming: 0.22238058991015805\n",
      "step: 13, hamming: 0.1921244069073419\n",
      "step: 14, hamming: 0.16545817957481052\n",
      "step: 15, hamming: 0.14219255890384128\n",
      "step: 16, hamming: 0.12206981536725259\n",
      "step: 17, hamming: 0.10479308912740204\n",
      "step: 18, hamming: 0.09004682418382234\n",
      "step: 19, hamming: 0.07751094005916007\n",
      "step: 20, hamming: 0.0668704475528378\n",
      "step: 21, hamming: 0.05782371001722576\n",
      "step: 22, hamming: 0.05009090857316944\n",
      "step: 23, hamming: 0.04342352724644389\n",
      "step: 24, hamming: 0.037613742843345156\n",
      "step: 25, hamming: 0.032500717866233104\n",
      "step: 26, hamming: 0.027970567103492498\n",
      "step: 27, hamming: 0.023949145946046774\n",
      "step: 28, hamming: 0.020389814594208796\n",
      "step: 29, hamming: 0.017260389673325376\n",
      "step: 30, hamming: 0.014532948325301723\n",
      "step: 31, hamming: 0.012178073966408829\n",
      "step: 32, hamming: 0.010163053860804867\n",
      "step: 33, hamming: 0.00845257088969216\n",
      "step: 34, hamming: 0.007010420899916855\n",
      "step: 35, hamming: 0.00580127765077189\n",
      "step: 36, hamming: 0.0047920442777004526\n",
      "step: 37, hamming: 0.003952679304999424\n",
      "step: 38, hamming: 0.003256564629252693\n",
      "step: 39, hamming: 0.002680541077531178\n",
      "step: 40, hamming: 0.0022047328812271954\n",
      "step: 41, hamming: 0.0018122567957040041\n",
      "step: 42, hamming: 0.0014888823408190466\n",
      "step: 43, hamming: 0.0012226851824785946\n",
      "step: 44, hamming: 0.0010037179959729236\n",
      "step: 45, hamming: 0.0008237110939490808\n",
      "Running panda took: 1.32 seconds!\n",
      "  Elapsed time: 1.32 sec.\n",
      "Saving LIONESS network 43 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 44:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6455018706675434\n",
      "step: 1, hamming: 0.6066771889037847\n",
      "step: 2, hamming: 0.6104927458207806\n",
      "step: 3, hamming: 0.5874724098807874\n",
      "step: 4, hamming: 0.5535381852036291\n",
      "step: 5, hamming: 0.5130412181815369\n",
      "step: 6, hamming: 0.4688357436456272\n",
      "step: 7, hamming: 0.4232060516712834\n",
      "step: 8, hamming: 0.3779651681944428\n",
      "step: 9, hamming: 0.33447578182458715\n",
      "step: 10, hamming: 0.29368699908851514\n",
      "step: 11, hamming: 0.25618810836906636\n",
      "step: 12, hamming: 0.22227915239937623\n",
      "step: 13, hamming: 0.19203863188744116\n",
      "step: 14, hamming: 0.16538554120286297\n",
      "step: 15, hamming: 0.1421309400195572\n",
      "step: 16, hamming: 0.12201736565539253\n",
      "step: 17, hamming: 0.10474825814801286\n",
      "step: 18, hamming: 0.09000844987911948\n",
      "step: 19, hamming: 0.07747797083444702\n",
      "step: 20, hamming: 0.0668420504052051\n",
      "step: 21, hamming: 0.05779924629784824\n",
      "step: 22, hamming: 0.05006983470329827\n",
      "step: 23, hamming: 0.0434054377789933\n",
      "step: 24, hamming: 0.03759830798077482\n",
      "step: 25, hamming: 0.032487644623876356\n",
      "step: 26, hamming: 0.027959585227725067\n",
      "step: 27, hamming: 0.023939993281594022\n",
      "step: 28, hamming: 0.020382235303793\n",
      "step: 29, hamming: 0.017254142326846075\n",
      "step: 30, hamming: 0.0145278165923739\n",
      "step: 31, hamming: 0.012173865559254832\n",
      "step: 32, hamming: 0.010159605065697939\n",
      "step: 33, hamming: 0.008449745022944615\n",
      "step: 34, hamming: 0.00700810475512328\n",
      "step: 35, hamming: 0.005799378660948917\n",
      "step: 36, hamming: 0.0047904869216820745\n",
      "step: 37, hamming: 0.003951402165785128\n",
      "step: 38, hamming: 0.0032555171432301403\n",
      "step: 39, hamming: 0.002679681888018421\n",
      "step: 40, hamming: 0.0022040281149082117\n",
      "step: 41, hamming: 0.0018116787055630656\n",
      "step: 42, hamming: 0.001488408176557867\n",
      "step: 43, hamming: 0.0012222962914605249\n",
      "step: 44, hamming: 0.00100339907294035\n",
      "step: 45, hamming: 0.0008234495793931117\n",
      "Running panda took: 1.33 seconds!\n",
      "  Elapsed time: 1.33 sec.\n",
      "Saving LIONESS network 44 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 45:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, hamming: 0.6451609513913849\n",
      "step: 1, hamming: 0.60682301694336\n",
      "step: 2, hamming: 0.6106083346123937\n",
      "step: 3, hamming: 0.5875838988028538\n",
      "step: 4, hamming: 0.5536474918555495\n",
      "step: 5, hamming: 0.5131464111654228\n",
      "step: 6, hamming: 0.46893531465023547\n",
      "step: 7, hamming: 0.4232953952543082\n",
      "step: 8, hamming: 0.3780435892681105\n",
      "step: 9, hamming: 0.33454365128594743\n",
      "step: 10, hamming: 0.29374508748769934\n",
      "step: 11, hamming: 0.2562377909089144\n",
      "step: 12, hamming: 0.22232178821340307\n",
      "step: 13, hamming: 0.19207512806462554\n",
      "step: 14, hamming: 0.16541683124420956\n",
      "step: 15, hamming: 0.1421577806735096\n",
      "step: 16, hamming: 0.12204047685598642\n",
      "step: 17, hamming: 0.10476822857697907\n",
      "step: 18, hamming: 0.09002567471379647\n",
      "step: 19, hamming: 0.0774928879378122\n",
      "step: 20, hamming: 0.06685499297134434\n",
      "step: 21, hamming: 0.05781044528986368\n",
      "step: 22, hamming: 0.050079503532131836\n",
      "step: 23, hamming: 0.043413743732357984\n",
      "step: 24, hamming: 0.03760540071279273\n",
      "step: 25, hamming: 0.03249365564796301\n",
      "step: 26, hamming: 0.027964640608281126\n",
      "step: 27, hamming: 0.02394421340556936\n",
      "step: 28, hamming: 0.02038573503818736\n",
      "step: 29, hamming: 0.017257031000157456\n",
      "step: 30, hamming: 0.014530192108341458\n",
      "step: 31, hamming: 0.012175817632282373\n",
      "step: 32, hamming: 0.01016120800943048\n",
      "step: 33, hamming: 0.008451061098622106\n",
      "step: 34, hamming: 0.0070091856123166265\n",
      "step: 35, hamming: 0.005800266586091691\n",
      "step: 36, hamming: 0.0047912165562239845\n",
      "step: 37, hamming: 0.003952001550810153\n",
      "step: 38, hamming: 0.003256009551690285\n",
      "step: 39, hamming: 0.0026800863984453036\n",
      "step: 40, hamming: 0.002204360400847909\n",
      "step: 41, hamming: 0.0018119516331865828\n",
      "step: 42, hamming: 0.001488632316391934\n",
      "step: 43, hamming: 0.0012224803346953504\n",
      "step: 44, hamming: 0.001003550166649098\n",
      "step: 45, hamming: 0.0008235735965779465\n",
      "Running panda took: 1.32 seconds!\n",
      "  Elapsed time: 1.32 sec.\n",
      "Saving LIONESS network 45 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 46:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6444561373205833\n",
      "step: 1, hamming: 0.6065980545917278\n",
      "step: 2, hamming: 0.6105128971327527\n",
      "step: 3, hamming: 0.5875110985863032\n",
      "step: 4, hamming: 0.5535881909404929\n",
      "step: 5, hamming: 0.513091592106618\n",
      "step: 6, hamming: 0.46888363850009035\n",
      "step: 7, hamming: 0.42324816999840126\n",
      "step: 8, hamming: 0.3780016536091072\n",
      "step: 9, hamming: 0.3345068620657478\n",
      "step: 10, hamming: 0.29371300156314933\n",
      "step: 11, hamming: 0.2562101677520666\n",
      "step: 12, hamming: 0.22229782461584252\n",
      "step: 13, hamming: 0.19205431495200018\n",
      "step: 14, hamming: 0.16539873221832643\n",
      "step: 15, hamming: 0.14214201379337854\n",
      "step: 16, hamming: 0.1220266912618745\n",
      "step: 17, hamming: 0.10475621059832577\n",
      "step: 18, hamming: 0.09001529971582535\n",
      "step: 19, hamming: 0.07748388708610993\n",
      "step: 20, hamming: 0.06684718843182244\n",
      "step: 21, hamming: 0.05780369052361184\n",
      "step: 22, hamming: 0.05007369620781047\n",
      "step: 23, hamming: 0.04340879060521526\n",
      "step: 24, hamming: 0.0376012098195711\n",
      "step: 25, hamming: 0.03249013651071767\n",
      "step: 26, hamming: 0.02796170164783043\n",
      "step: 27, hamming: 0.023941767943218586\n",
      "step: 28, hamming: 0.02038370710945001\n",
      "step: 29, hamming: 0.01725535251678685\n",
      "step: 30, hamming: 0.01452880343841106\n",
      "step: 31, hamming: 0.012174667193433731\n",
      "step: 32, hamming: 0.01016025513605103\n",
      "step: 33, hamming: 0.008450271975505555\n",
      "step: 34, hamming: 0.00700853253028554\n",
      "step: 35, hamming: 0.005799726752184736\n",
      "step: 36, hamming: 0.004790770756328513\n",
      "step: 37, hamming: 0.003951633741495329\n",
      "step: 38, hamming: 0.003255706284492106\n",
      "step: 39, hamming: 0.002679836512915264\n",
      "step: 40, hamming: 0.0022041546289165016\n",
      "step: 41, hamming: 0.0018117822850880238\n",
      "step: 42, hamming: 0.001488493020323518\n",
      "step: 43, hamming: 0.0012223658117307828\n",
      "step: 44, hamming: 0.0010034560492991984\n",
      "step: 45, hamming: 0.0008234962794548378\n",
      "Running panda took: 1.32 seconds!\n",
      "  Elapsed time: 1.32 sec.\n",
      "Saving LIONESS network 46 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 47:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6451094435014055\n",
      "step: 1, hamming: 0.6067224879320416\n",
      "step: 2, hamming: 0.6104855876766591\n",
      "step: 3, hamming: 0.5874302260560214\n",
      "step: 4, hamming: 0.5534812095958547\n",
      "step: 5, hamming: 0.5129821856517683\n",
      "step: 6, hamming: 0.46878098398796686\n",
      "step: 7, hamming: 0.42315657041730387\n",
      "step: 8, hamming: 0.3779239978979505\n",
      "step: 9, hamming: 0.3344427227132656\n",
      "step: 10, hamming: 0.29366102731476706\n",
      "step: 11, hamming: 0.25616821630801284\n",
      "step: 12, hamming: 0.22226395137122448\n",
      "step: 13, hamming: 0.19202681658475126\n",
      "step: 14, hamming: 0.16537625986764548\n",
      "step: 15, hamming: 0.1421235728034832\n",
      "step: 16, hamming: 0.12201150665910697\n",
      "step: 17, hamming: 0.10474353103231468\n",
      "step: 18, hamming: 0.09000454950906786\n",
      "step: 19, hamming: 0.07747470953904387\n",
      "step: 20, hamming: 0.0668393074229943\n",
      "step: 21, hamming: 0.057796887158797904\n",
      "step: 22, hamming: 0.050067790022936765\n",
      "step: 23, hamming: 0.0434036556616216\n",
      "step: 24, hamming: 0.03759676223186337\n",
      "step: 25, hamming: 0.03248631716762014\n",
      "step: 26, hamming: 0.027958452078804107\n",
      "step: 27, hamming: 0.023939028616903377\n",
      "step: 28, hamming: 0.02038141944916723\n",
      "step: 29, hamming: 0.017253456558720872\n",
      "step: 30, hamming: 0.014527240970954449\n",
      "step: 31, hamming: 0.012173384845798182\n",
      "step: 32, hamming: 0.01015920549888837\n",
      "step: 33, hamming: 0.008449414288126908\n",
      "step: 34, hamming: 0.00700783251428514\n",
      "step: 35, hamming: 0.00579915532069411\n",
      "step: 36, hamming: 0.004790304232521376\n",
      "step: 37, hamming: 0.003951252628490434\n",
      "step: 38, hamming: 0.0032553948817023253\n",
      "step: 39, hamming: 0.002679582018442492\n",
      "step: 40, hamming: 0.0022039466024797337\n",
      "step: 41, hamming: 0.0018116122118621118\n",
      "step: 42, hamming: 0.0014883539520461812\n",
      "step: 43, hamming: 0.0012222520794900557\n",
      "step: 44, hamming: 0.0010033630290072085\n",
      "step: 45, hamming: 0.0008234201909991959\n",
      "Running panda took: 1.33 seconds!\n",
      "  Elapsed time: 1.33 sec.\n",
      "Saving LIONESS network 47 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 48:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6450729208857101\n",
      "step: 1, hamming: 0.6063501512150871\n",
      "step: 2, hamming: 0.6103715026046173\n",
      "step: 3, hamming: 0.5874284484527396\n",
      "step: 4, hamming: 0.5535259596516843\n",
      "step: 5, hamming: 0.5130547047753817\n",
      "step: 6, hamming: 0.4688708582148043\n",
      "step: 7, hamming: 0.42325992634308324\n",
      "step: 8, hamming: 0.37803721253202954\n",
      "step: 9, hamming: 0.3345609217695329\n",
      "step: 10, hamming: 0.2937790878929319\n",
      "step: 11, hamming: 0.2562812448018168\n",
      "step: 12, hamming: 0.22236874382375207\n",
      "step: 13, hamming: 0.19212159497551798\n",
      "step: 14, hamming: 0.16546046772472403\n",
      "step: 15, hamming: 0.14219729408942722\n",
      "step: 16, hamming: 0.12207523597375466\n",
      "step: 17, hamming: 0.10479810759599201\n",
      "step: 18, hamming: 0.09005092233539187\n",
      "step: 19, hamming: 0.0775138622498422\n",
      "step: 20, hamming: 0.06687216133585415\n",
      "step: 21, hamming: 0.05782432168099327\n",
      "step: 22, hamming: 0.050090599604048575\n",
      "step: 23, hamming: 0.04342254449049325\n",
      "step: 24, hamming: 0.037612354730298586\n",
      "step: 25, hamming: 0.03249915130287423\n",
      "step: 26, hamming: 0.027968991909988243\n",
      "step: 27, hamming: 0.023947673742928516\n",
      "step: 28, hamming: 0.020388507929298593\n",
      "step: 29, hamming: 0.01725926821592624\n",
      "step: 30, hamming: 0.014532009040509329\n",
      "step: 31, hamming: 0.012177299988630045\n",
      "step: 32, hamming: 0.010162422517800812\n",
      "step: 33, hamming: 0.008452059415929544\n",
      "step: 34, hamming: 0.0070100082569057925\n",
      "step: 35, hamming: 0.005800945322716096\n",
      "step: 36, hamming: 0.004791776880461754\n",
      "step: 37, hamming: 0.003952464204685849\n",
      "step: 38, hamming: 0.0032563915570548445\n",
      "step: 39, hamming: 0.0026804017615245797\n",
      "step: 40, hamming: 0.002204620661194443\n",
      "step: 41, hamming: 0.0018121663562190129\n",
      "step: 42, hamming: 0.0014888093898348781\n",
      "step: 43, hamming: 0.001222626284457484\n",
      "step: 44, hamming: 0.0010036704002436717\n",
      "step: 45, hamming: 0.0008236725986009675\n",
      "Running panda took: 1.33 seconds!\n",
      "  Elapsed time: 1.33 sec.\n",
      "Saving LIONESS network 48 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 49:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6450184972229367\n",
      "step: 1, hamming: 0.6068240235253973\n",
      "step: 2, hamming: 0.6105965948530253\n",
      "step: 3, hamming: 0.5875567675582667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4, hamming: 0.5536089018867548\n",
      "step: 5, hamming: 0.5130991941025524\n",
      "step: 6, hamming: 0.46888336965000044\n",
      "step: 7, hamming: 0.42324358671229345\n",
      "step: 8, hamming: 0.37799368552636897\n",
      "step: 9, hamming: 0.33449716705491805\n",
      "step: 10, hamming: 0.2937026837940896\n",
      "step: 11, hamming: 0.25619963891549247\n",
      "step: 12, hamming: 0.22228779180603883\n",
      "step: 13, hamming: 0.1920451228053333\n",
      "step: 14, hamming: 0.165390605047534\n",
      "step: 15, hamming: 0.14213514394319682\n",
      "step: 16, hamming: 0.12202099792941383\n",
      "step: 17, hamming: 0.1047515095155968\n",
      "step: 18, hamming: 0.09001138629352733\n",
      "step: 19, hamming: 0.0774807387620337\n",
      "step: 20, hamming: 0.06684470509459199\n",
      "step: 21, hamming: 0.05780176344875446\n",
      "step: 22, hamming: 0.05007220261612099\n",
      "step: 23, hamming: 0.0434076228109129\n",
      "step: 24, hamming: 0.037600282707592456\n",
      "step: 25, hamming: 0.03248939173711592\n",
      "step: 26, hamming: 0.027961098540398802\n",
      "step: 27, hamming: 0.023941274385556592\n",
      "step: 28, hamming: 0.0203833012069568\n",
      "step: 29, hamming: 0.017255017889970633\n",
      "step: 30, hamming: 0.014528528671629803\n",
      "step: 31, hamming: 0.01217444290051738\n",
      "step: 32, hamming: 0.010160072735999441\n",
      "step: 33, hamming: 0.008450124179654739\n",
      "step: 34, hamming: 0.007008412715800018\n",
      "step: 35, hamming: 0.005799629423267301\n",
      "step: 36, hamming: 0.00479069148047581\n",
      "step: 37, hamming: 0.003951568960336358\n",
      "step: 38, hamming: 0.0032556533190094515\n",
      "step: 39, hamming: 0.0026797931825938656\n",
      "step: 40, hamming: 0.0022041191628977\n",
      "step: 41, hamming: 0.0018117532364711282\n",
      "step: 42, hamming: 0.0014884692131398442\n",
      "step: 43, hamming: 0.0012223462917056505\n",
      "step: 44, hamming: 0.001003440041109761\n",
      "step: 45, hamming: 0.0008234831485799504\n",
      "Running panda took: 1.33 seconds!\n",
      "  Elapsed time: 1.33 sec.\n",
      "Saving LIONESS network 49 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 50:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6452671619076494\n",
      "step: 1, hamming: 0.6066848794849176\n",
      "step: 2, hamming: 0.6105275873120232\n",
      "step: 3, hamming: 0.5875201390790638\n",
      "step: 4, hamming: 0.5535905216591732\n",
      "step: 5, hamming: 0.5130937685955772\n",
      "step: 6, hamming: 0.4688858948251511\n",
      "step: 7, hamming: 0.42325035103333314\n",
      "step: 8, hamming: 0.3780020754008558\n",
      "step: 9, hamming: 0.33450587862635855\n",
      "step: 10, hamming: 0.29371071766895035\n",
      "step: 11, hamming: 0.25620695600438853\n",
      "step: 12, hamming: 0.22229411692149165\n",
      "step: 13, hamming: 0.1920505169749237\n",
      "step: 14, hamming: 0.16539508199998468\n",
      "step: 15, hamming: 0.14213872037052774\n",
      "step: 16, hamming: 0.12202392388371454\n",
      "step: 17, hamming: 0.10475390584465226\n",
      "step: 18, hamming: 0.09001338394595733\n",
      "step: 19, hamming: 0.07748237123447736\n",
      "step: 20, hamming: 0.06684601761691708\n",
      "step: 21, hamming: 0.05780280883485421\n",
      "step: 22, hamming: 0.05007303285774467\n",
      "step: 23, hamming: 0.043408286930817476\n",
      "step: 24, hamming: 0.03760082097488881\n",
      "step: 25, hamming: 0.03248982737352425\n",
      "step: 26, hamming: 0.02796144715861602\n",
      "step: 27, hamming: 0.023941554838090757\n",
      "step: 28, hamming: 0.020383526490059086\n",
      "step: 29, hamming: 0.017255198122503472\n",
      "step: 30, hamming: 0.014528673059252926\n",
      "step: 31, hamming: 0.012174559156125178\n",
      "step: 32, hamming: 0.010160166553288554\n",
      "step: 33, hamming: 0.008450200171028089\n",
      "step: 34, hamming: 0.007008474696064771\n",
      "step: 35, hamming: 0.005799679944846654\n",
      "step: 36, hamming: 0.0047907327210686135\n",
      "step: 37, hamming: 0.003951602686458482\n",
      "step: 38, hamming: 0.003255680919601847\n",
      "step: 39, hamming: 0.002679815808328275\n",
      "step: 40, hamming: 0.002204137721913641\n",
      "step: 41, hamming: 0.001811768467971286\n",
      "step: 42, hamming: 0.0014884817149678226\n",
      "step: 43, hamming: 0.0012223565551249145\n",
      "step: 44, hamming: 0.0010034484651433202\n",
      "step: 45, hamming: 0.0008234900629489583\n",
      "Running panda took: 1.32 seconds!\n",
      "  Elapsed time: 1.33 sec.\n",
      "Saving LIONESS network 50 to lioness_output using npy format:\n",
      "  Elapsed time: 0.00 sec.\n"
     ]
    }
   ],
   "source": [
    "lioness_obj = Lioness(panda_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Lioness with co-expression matrix\n",
    "Lioness can work with co-expression matrix. To compute Lioness with coexpression matrix, we can set motif data to `None`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading expression data ...\n",
      "Expression matrix: (1000, 50)\n",
      "  Elapsed time: 0.02 sec.\n",
      "Loading PPI data ...\n",
      "Number of PPIs: 238\n",
      "  Elapsed time: 0.01 sec.\n",
      "Calculating coexpression network ...\n",
      "  Elapsed time: 0.03 sec.\n",
      "Returning the correlation matrix of expression data in <Panda_obj>.correlation_matrix\n",
      "Loading input data ...\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 1:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 1 to lioness_output using npy format:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Running LIONESS for sample 2:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.18 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 2 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 3:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 3 to lioness_output using npy format:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Running LIONESS for sample 4:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.07 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 4 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 5:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 5 to lioness_output using npy format:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Running LIONESS for sample 6:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 6 to lioness_output using npy format:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Running LIONESS for sample 7:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.09 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 7 to lioness_output using npy format:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Running LIONESS for sample 8:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 8 to lioness_output using npy format:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Running LIONESS for sample 9:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 9 to lioness_output using npy format:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Running LIONESS for sample 10:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 10 to lioness_output using npy format:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Running LIONESS for sample 11:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 11 to lioness_output using npy format:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Running LIONESS for sample 12:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 12 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 13:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 13 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 14:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 14 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 15:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 15 to lioness_output using npy format:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Running LIONESS for sample 16:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 16 to lioness_output using npy format:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Running LIONESS for sample 17:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 17 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 18:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 18 to lioness_output using npy format:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Running LIONESS for sample 19:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 19 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 20:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 20 to lioness_output using npy format:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Running LIONESS for sample 21:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 21 to lioness_output using npy format:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Running LIONESS for sample 22:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 22 to lioness_output using npy format:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Running LIONESS for sample 23:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 23 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 24:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 24 to lioness_output using npy format:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Running LIONESS for sample 25:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 25 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 26:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 26 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 27:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 27 to lioness_output using npy format:\n",
      "  Elapsed time: 0.07 sec.\n",
      "Running LIONESS for sample 28:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 28 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 29:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 29 to lioness_output using npy format:\n",
      "  Elapsed time: 0.08 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LIONESS for sample 30:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 30 to lioness_output using npy format:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Running LIONESS for sample 31:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 31 to lioness_output using npy format:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Running LIONESS for sample 32:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 32 to lioness_output using npy format:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Running LIONESS for sample 33:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 33 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 34:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 34 to lioness_output using npy format:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Running LIONESS for sample 35:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 35 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 36:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 36 to lioness_output using npy format:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Running LIONESS for sample 37:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 37 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 38:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 38 to lioness_output using npy format:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Running LIONESS for sample 39:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 39 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 40:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.08 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 40 to lioness_output using npy format:\n",
      "  Elapsed time: 0.08 sec.\n",
      "Running LIONESS for sample 41:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 41 to lioness_output using npy format:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Running LIONESS for sample 42:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.07 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 42 to lioness_output using npy format:\n",
      "  Elapsed time: 0.09 sec.\n",
      "Running LIONESS for sample 43:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.07 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 43 to lioness_output using npy format:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Running LIONESS for sample 44:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 44 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 45:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.08 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 45 to lioness_output using npy format:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Running LIONESS for sample 46:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 46 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 47:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.08 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 47 to lioness_output using npy format:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Running LIONESS for sample 48:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.08 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.08 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 48 to lioness_output using npy format:\n",
      "  Elapsed time: 0.08 sec.\n",
      "Running LIONESS for sample 49:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 49 to lioness_output using npy format:\n",
      "  Elapsed time: 0.08 sec.\n",
      "Running LIONESS for sample 50:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 50 to lioness_output using npy format:\n",
      "  Elapsed time: 0.08 sec.\n"
     ]
    }
   ],
   "source": [
    "motif = None\n",
    "\n",
    "# Make sure to keep epxression matrix for next step\n",
    "panda_obj = Panda('netZooPy/tests/ToyData/ToyExpressionData.txt',\n",
    "                  None,\n",
    "                  'netZooPy/tests/ToyData/ToyPPIData.txt',\n",
    "                  save_tmp=True,\n",
    "                  remove_missing=False,\n",
    "                  keep_expression_matrix=True, modeProcess='legacy')\n",
    "lioness_obj = Lioness(panda_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Lioness results\n",
    "AnalyzeLioness() can be used to visualize lioness network. You may select only the `top` genes to be visualized in the graph. In current version of Lioness. Only the network of the first sample will be visualized using `.top_network_plot()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analyze_lioness_obj = AnalyzeLioness(lioness_obj)\n",
    "analyze_lioness_obj.top_network_plot(top = 10, file = \"lioness_top_10.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Lioness results\n",
    "We can save Lioness results by using `save_lioness_results()` method of the `Lioness` object. The edge weights of Lioness predictions will be saved into output file. We can get TF and target IDs from the `.export_panda_results` property of `Panda` object. Each row correspond to a row in the Lioness output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf</th>\n",
       "      <th>gene</th>\n",
       "      <th>motif</th>\n",
       "      <th>force</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AHR</td>\n",
       "      <td>AACSL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-53.984356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AR</td>\n",
       "      <td>AACSL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.276521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ARID3A</td>\n",
       "      <td>AACSL</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-64.531519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ARNT</td>\n",
       "      <td>AACSL</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-70.183704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BRCA1</td>\n",
       "      <td>AACSL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-57.854191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86995</th>\n",
       "      <td>TLX1</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.673701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86996</th>\n",
       "      <td>TP53</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.789647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86997</th>\n",
       "      <td>USF1</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.855873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86998</th>\n",
       "      <td>VDR</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.885728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86999</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-80.408914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           tf    gene  motif      force\n",
       "0         AHR   AACSL    0.0 -53.984356\n",
       "1          AR   AACSL    0.0  27.276521\n",
       "2      ARID3A   AACSL    1.0 -64.531519\n",
       "3        ARNT   AACSL    1.0 -70.183704\n",
       "4       BRCA1   AACSL    0.0 -57.854191\n",
       "...       ...     ...    ...        ...\n",
       "86995    TLX1  ZWILCH    0.0  15.673701\n",
       "86996    TP53  ZWILCH    0.0  23.789647\n",
       "86997    USF1  ZWILCH    0.0  -6.855873\n",
       "86998     VDR  ZWILCH    0.0  20.885728\n",
       "86999     YY1  ZWILCH    1.0 -80.408914\n",
       "\n",
       "[87000 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panda_obj.export_panda_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lioness_obj.save_lioness_results(file = 'lioness.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "Kuijjer ML, Tung MG, Yuan GC, Quackenbush J, Glass K: Estimating Sample-Specific Regulatory Networks. iScience 2019."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
